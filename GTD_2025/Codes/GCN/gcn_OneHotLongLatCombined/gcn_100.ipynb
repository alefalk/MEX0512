{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8b7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from build_graph_data import *\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a56371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../data/top30groups/engineered_dfs/df_top30_100.csv'\n",
    "data = pd.read_csv(path, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e017e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before dropping long/lat duplicates:  3000\n",
      "Entries after dropping long/lat duplicates (#Nodes):  1790\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to only contain unique coordinates\n",
    "print(\"Entries before dropping long/lat duplicates: \", len(data))\n",
    "df_unique_geo = create_unique_geo_data(data)\n",
    "print(\"Entries after dropping long/lat duplicates (#Nodes): \", len(df_unique_geo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1fb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates train and test data, first 70% of each group is added to train and remaining 30% to test\n",
    "def handle_leakage(df):\n",
    "    train_frames = []\n",
    "    test_frames = []\n",
    "\n",
    "    #first 70% of each groups attacks to training set, remainin 30% to testing set\n",
    "    for _, group_data in df.groupby('gname'):\n",
    "        split_point = int(len(group_data) * 0.7)  # 70% for training\n",
    "        train_frames.append(group_data.iloc[:split_point])\n",
    "        test_frames.append(group_data.iloc[split_point:])           \n",
    "\n",
    "\n",
    "    # Concatenate all the group-specific splits into final train and test DataFrames\n",
    "    train_df = pd.concat(train_frames)\n",
    "    test_df = pd.concat(test_frames)\n",
    "\n",
    "    # Shuffle each DataFrame separately\n",
    "    train_df = shuffle(train_df)\n",
    "    test_df = shuffle(test_df)\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(len(test_df))\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc1aa2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1239\n",
      "551\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = handle_leakage(df_unique_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c96dc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total nodes (unique coordinates): 1790\n",
      "Number of unique labels in this set: 30\n"
     ]
    }
   ],
   "source": [
    "# 1. Build coord_to_index from the full dataset (unique coordinate to node index mapping)\n",
    "full_coords = df_unique_geo[['longitude', 'latitude']]\n",
    "coord_to_index = {(row['longitude'], row['latitude']): i for i, row in full_coords.iterrows()}\n",
    "\n",
    "# 2. Build the global graph from the full dataset (used for both train and test)\n",
    "adj_matrix, feature_matrix, label_index = build_graph_data(df_unique_geo, coord_to_index)\n",
    "\n",
    "train_nodes = []\n",
    "train_labels = []\n",
    "for _, row in train_df.iterrows():\n",
    "    train_nodes.append(coord_to_index[(row['longitude'], row['latitude'])])\n",
    "    train_labels.append(label_index[row['gname']])\n",
    "\n",
    "test_nodes = []\n",
    "test_labels = []\n",
    "for _, row in test_df.iterrows():\n",
    "    test_nodes.append(coord_to_index[(row['longitude'], row['latitude'])])\n",
    "    test_labels.append(label_index[row['gname']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc7c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1790, 1790)\n"
     ]
    }
   ],
   "source": [
    "print(adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12314fb",
   "metadata": {},
   "source": [
    "# A simple GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a847fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class PyTorchGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f030b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def run_epoch(model, data, labels, mask, optimizer=None):\n",
    "    is_training = optimizer is not None\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    loss = loss_fn(out[mask], labels[mask])\n",
    "\n",
    "    if is_training:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Accuracy\n",
    "    pred = out[mask].argmax(dim=1)\n",
    "    acc = (pred == labels[mask]).float().mean().item()\n",
    "    return acc, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfcfa5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Acc: 0.0000 | Test Acc: 0.0218 | Train Loss: 30.4567\n",
      "Epoch 02 | Train Acc: 0.0282 | Test Acc: 0.0236 | Train Loss: 26.7460\n",
      "Epoch 03 | Train Acc: 0.0282 | Test Acc: 0.0236 | Train Loss: 23.2836\n",
      "Epoch 04 | Train Acc: 0.0379 | Test Acc: 0.0436 | Train Loss: 20.0129\n",
      "Epoch 05 | Train Acc: 0.0492 | Test Acc: 0.0508 | Train Loss: 17.3345\n",
      "Epoch 06 | Train Acc: 0.0541 | Test Acc: 0.0944 | Train Loss: 15.0752\n",
      "Epoch 07 | Train Acc: 0.1073 | Test Acc: 0.0998 | Train Loss: 13.5144\n",
      "Epoch 08 | Train Acc: 0.1146 | Test Acc: 0.0871 | Train Loss: 12.0137\n",
      "Epoch 09 | Train Acc: 0.1170 | Test Acc: 0.1107 | Train Loss: 10.5843\n",
      "Epoch 10 | Train Acc: 0.1574 | Test Acc: 0.1053 | Train Loss: 9.2248\n",
      "Epoch 11 | Train Acc: 0.1542 | Test Acc: 0.1325 | Train Loss: 8.4432\n",
      "Epoch 12 | Train Acc: 0.1582 | Test Acc: 0.1670 | Train Loss: 7.7888\n",
      "Epoch 13 | Train Acc: 0.1889 | Test Acc: 0.1579 | Train Loss: 7.1436\n",
      "Epoch 14 | Train Acc: 0.1703 | Test Acc: 0.1434 | Train Loss: 6.4974\n",
      "Epoch 15 | Train Acc: 0.1630 | Test Acc: 0.1561 | Train Loss: 5.8429\n",
      "Epoch 16 | Train Acc: 0.1396 | Test Acc: 0.1434 | Train Loss: 5.2964\n",
      "Epoch 17 | Train Acc: 0.1372 | Test Acc: 0.1887 | Train Loss: 5.1751\n",
      "Epoch 18 | Train Acc: 0.1703 | Test Acc: 0.1851 | Train Loss: 5.0481\n",
      "Epoch 19 | Train Acc: 0.1945 | Test Acc: 0.1851 | Train Loss: 4.7354\n",
      "Epoch 20 | Train Acc: 0.1953 | Test Acc: 0.1869 | Train Loss: 4.2948\n",
      "Epoch 21 | Train Acc: 0.1969 | Test Acc: 0.2105 | Train Loss: 3.8892\n",
      "Epoch 22 | Train Acc: 0.2187 | Test Acc: 0.2142 | Train Loss: 3.6140\n",
      "Epoch 23 | Train Acc: 0.2163 | Test Acc: 0.2069 | Train Loss: 3.3831\n",
      "Epoch 24 | Train Acc: 0.2066 | Test Acc: 0.1760 | Train Loss: 3.2509\n",
      "Epoch 25 | Train Acc: 0.1800 | Test Acc: 0.1706 | Train Loss: 3.1931\n",
      "Epoch 26 | Train Acc: 0.1711 | Test Acc: 0.1978 | Train Loss: 3.1385\n",
      "Epoch 27 | Train Acc: 0.1994 | Test Acc: 0.1978 | Train Loss: 3.0685\n",
      "Epoch 28 | Train Acc: 0.1994 | Test Acc: 0.1978 | Train Loss: 2.9605\n",
      "Epoch 29 | Train Acc: 0.1994 | Test Acc: 0.2196 | Train Loss: 2.8313\n",
      "Epoch 30 | Train Acc: 0.2115 | Test Acc: 0.2341 | Train Loss: 2.6944\n",
      "Epoch 31 | Train Acc: 0.2462 | Test Acc: 0.2668 | Train Loss: 2.5470\n",
      "Epoch 32 | Train Acc: 0.2680 | Test Acc: 0.2777 | Train Loss: 2.4099\n",
      "Epoch 33 | Train Acc: 0.2962 | Test Acc: 0.2904 | Train Loss: 2.3181\n",
      "Epoch 34 | Train Acc: 0.3107 | Test Acc: 0.3176 | Train Loss: 2.2686\n",
      "Epoch 35 | Train Acc: 0.3640 | Test Acc: 0.3158 | Train Loss: 2.2357\n",
      "Epoch 36 | Train Acc: 0.3600 | Test Acc: 0.3249 | Train Loss: 2.2176\n",
      "Epoch 37 | Train Acc: 0.3559 | Test Acc: 0.3103 | Train Loss: 2.1951\n",
      "Epoch 38 | Train Acc: 0.3293 | Test Acc: 0.3140 | Train Loss: 2.1385\n",
      "Epoch 39 | Train Acc: 0.3148 | Test Acc: 0.3249 | Train Loss: 2.0558\n",
      "Epoch 40 | Train Acc: 0.3713 | Test Acc: 0.3067 | Train Loss: 1.9953\n",
      "Epoch 41 | Train Acc: 0.3390 | Test Acc: 0.3031 | Train Loss: 1.9801\n",
      "Epoch 42 | Train Acc: 0.3406 | Test Acc: 0.3067 | Train Loss: 1.9736\n",
      "Epoch 43 | Train Acc: 0.3519 | Test Acc: 0.3339 | Train Loss: 1.9489\n",
      "Epoch 44 | Train Acc: 0.3818 | Test Acc: 0.3575 | Train Loss: 1.9039\n",
      "Epoch 45 | Train Acc: 0.4027 | Test Acc: 0.3702 | Train Loss: 1.8543\n",
      "Epoch 46 | Train Acc: 0.4157 | Test Acc: 0.3684 | Train Loss: 1.8184\n",
      "Epoch 47 | Train Acc: 0.4229 | Test Acc: 0.3648 | Train Loss: 1.7975\n",
      "Epoch 48 | Train Acc: 0.4140 | Test Acc: 0.3702 | Train Loss: 1.7802\n",
      "Epoch 49 | Train Acc: 0.4165 | Test Acc: 0.3684 | Train Loss: 1.7597\n",
      "Epoch 50 | Train Acc: 0.4140 | Test Acc: 0.3466 | Train Loss: 1.7375\n",
      "-----------------------\n",
      "Best test acc in epoch 45, accuracy: 0.3702359199523926\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# 1. Convert adjacency matrix to edge_index\n",
    "A_coo = coo_matrix(adj_matrix)\n",
    "edge_index = torch.tensor(np.vstack((A_coo.row, A_coo.col)), dtype=torch.long)\n",
    "\n",
    "# Feature Matrix Tensor\n",
    "coords = np.array(list(coord_to_index.keys()), dtype=np.float32)\n",
    "feature_matrix = coords  # shape: (N, 2), with [longitude, latitude]\n",
    "x = torch.tensor(feature_matrix, dtype=torch.float32)\n",
    "\n",
    "num_nodes = x.shape[0]\n",
    "\n",
    "# Label Tensor\n",
    "y = torch.full((num_nodes,), -1, dtype=torch.long)  # -1 for unlabeled\n",
    "\n",
    "# Create Masks, indicates which ndoes are used in training and testing\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "# Assign group labels for train, set nodes as part of training set\n",
    "for _, row in train_df.iterrows():\n",
    "    coord = (row['longitude'], row['latitude'])\n",
    "    idx = coord_to_index[coord]\n",
    "    y[idx] = label_index[row['gname']]\n",
    "    train_mask[idx] = True\n",
    "\n",
    "# Assign group labels for test, set nodes as part of testing set\n",
    "for _, row in test_df.iterrows():\n",
    "    coord = (row['longitude'], row['latitude'])\n",
    "    idx = coord_to_index[coord]\n",
    "    y[idx] = label_index[row['gname']]\n",
    "    test_mask[idx] = True\n",
    "\n",
    "# Create PyG Data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = PyTorchGCN(in_channels=x.shape[1], hidden_channels=16, num_classes=len(label_index))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "max_test_acc = 0\n",
    "for epoch in range(50):\n",
    "    train_acc, train_loss = run_epoch(model, data, y, train_mask, optimizer)\n",
    "    test_acc, test_loss = run_epoch(model, data, y, test_mask)\n",
    "    if test_acc > max_test_acc:\n",
    "        max_test_acc = test_acc\n",
    "        max_test_acc_epoch = epoch + 1\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "print('-----------------------')\n",
    "print(f'Best test acc in epoch {max_test_acc_epoch}, accuracy: {max_test_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
