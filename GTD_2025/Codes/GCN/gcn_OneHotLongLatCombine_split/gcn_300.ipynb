{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8b7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from build_graph_data import *\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a56371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../data/top30groups/engineered_dfs/df_top30_300.csv'\n",
    "data = pd.read_csv(path, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e017e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before dropping long/lat duplicates:  9000\n",
      "Entries after dropping long/lat duplicates (#Nodes):  4379\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to only contain unique coordinates\n",
    "print(\"Entries before dropping long/lat duplicates: \", len(data))\n",
    "df_unique_geo = create_unique_geo_data(data)\n",
    "print(\"Entries after dropping long/lat duplicates (#Nodes): \", len(df_unique_geo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1fb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates train and test data, first 70% of each group is added to train and remaining 30% to test\n",
    "def handle_leakage(df):\n",
    "    train_frames = []\n",
    "    test_frames = []\n",
    "\n",
    "    #first 70% of each groups attacks to training set, remainin 30% to testing set\n",
    "    for _, group_data in df.groupby('gname'):\n",
    "        split_point = int(len(group_data) * 0.7)  # 70% for training\n",
    "        train_frames.append(group_data.iloc[:split_point])\n",
    "        test_frames.append(group_data.iloc[split_point:])           \n",
    "\n",
    "\n",
    "    # Concatenate all the group-specific splits into final train and test DataFrames\n",
    "    train_df = pd.concat(train_frames)\n",
    "    test_df = pd.concat(test_frames)\n",
    "\n",
    "    # Shuffle each DataFrame separately\n",
    "    train_df = shuffle(train_df)\n",
    "    test_df = shuffle(test_df)\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(len(test_df))\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc1aa2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3049\n",
      "1330\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = handle_leakage(df_unique_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c96dc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total nodes (unique coordinates): 4379\n",
      "Number of unique labels in this set: 30\n"
     ]
    }
   ],
   "source": [
    "# 1. Build coord_to_index from the full dataset (unique coordinate to node index mapping)\n",
    "full_coords = df_unique_geo[['longitude', 'latitude']]\n",
    "coord_to_index = {(row['longitude'], row['latitude']): i for i, row in full_coords.iterrows()}\n",
    "\n",
    "# 2. Build the global graph from the full dataset (used for both train and test)\n",
    "adj_matrix, feature_matrix, label_index = build_graph_data(df_unique_geo, coord_to_index)\n",
    "\n",
    "train_nodes = []\n",
    "train_labels = []\n",
    "for _, row in train_df.iterrows():\n",
    "    train_nodes.append(coord_to_index[(row['longitude'], row['latitude'])])\n",
    "    train_labels.append(label_index[row['gname']])\n",
    "\n",
    "test_nodes = []\n",
    "test_labels = []\n",
    "for _, row in test_df.iterrows():\n",
    "    test_nodes.append(coord_to_index[(row['longitude'], row['latitude'])])\n",
    "    test_labels.append(label_index[row['gname']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc7c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4379, 4379)\n"
     ]
    }
   ],
   "source": [
    "print(adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12314fb",
   "metadata": {},
   "source": [
    "# A simple GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a847fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class PyTorchGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f030b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def run_epoch(model, data, labels, mask, optimizer=None):\n",
    "    is_training = optimizer is not None\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    loss = loss_fn(out[mask], labels[mask])\n",
    "\n",
    "    if is_training:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Accuracy\n",
    "    pred = out[mask].argmax(dim=1)\n",
    "    acc = (pred == labels[mask]).float().mean().item()\n",
    "    return acc, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfcfa5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Acc: 0.0272 | Test Acc: 0.0263 | Train Loss: 23.1428\n",
      "Epoch 02 | Train Acc: 0.0269 | Test Acc: 0.0722 | Train Loss: 19.3205\n",
      "Epoch 03 | Train Acc: 0.0610 | Test Acc: 0.0556 | Train Loss: 15.9042\n",
      "Epoch 04 | Train Acc: 0.0489 | Test Acc: 0.0947 | Train Loss: 13.4023\n",
      "Epoch 05 | Train Acc: 0.1315 | Test Acc: 0.0970 | Train Loss: 11.4181\n",
      "Epoch 06 | Train Acc: 0.0879 | Test Acc: 0.1293 | Train Loss: 9.9545\n",
      "Epoch 07 | Train Acc: 0.1279 | Test Acc: 0.1226 | Train Loss: 8.8048\n",
      "Epoch 08 | Train Acc: 0.1230 | Test Acc: 0.1226 | Train Loss: 7.8461\n",
      "Epoch 09 | Train Acc: 0.1220 | Test Acc: 0.0910 | Train Loss: 7.0990\n",
      "Epoch 10 | Train Acc: 0.0872 | Test Acc: 0.1120 | Train Loss: 6.3451\n",
      "Epoch 11 | Train Acc: 0.1332 | Test Acc: 0.0662 | Train Loss: 5.7556\n",
      "Epoch 12 | Train Acc: 0.1089 | Test Acc: 0.0135 | Train Loss: 5.3496\n",
      "Epoch 13 | Train Acc: 0.0538 | Test Acc: 0.0391 | Train Loss: 4.8766\n",
      "Epoch 14 | Train Acc: 0.0630 | Test Acc: 0.0353 | Train Loss: 4.3737\n",
      "Epoch 15 | Train Acc: 0.0794 | Test Acc: 0.1316 | Train Loss: 3.9778\n",
      "Epoch 16 | Train Acc: 0.1564 | Test Acc: 0.1955 | Train Loss: 3.7562\n",
      "Epoch 17 | Train Acc: 0.2106 | Test Acc: 0.2135 | Train Loss: 3.6693\n",
      "Epoch 18 | Train Acc: 0.2234 | Test Acc: 0.1992 | Train Loss: 3.5440\n",
      "Epoch 19 | Train Acc: 0.2158 | Test Acc: 0.2030 | Train Loss: 3.3646\n",
      "Epoch 20 | Train Acc: 0.2506 | Test Acc: 0.1970 | Train Loss: 3.1551\n",
      "Epoch 21 | Train Acc: 0.2230 | Test Acc: 0.2722 | Train Loss: 2.9400\n",
      "Epoch 22 | Train Acc: 0.2945 | Test Acc: 0.2624 | Train Loss: 2.7322\n",
      "Epoch 23 | Train Acc: 0.2876 | Test Acc: 0.2962 | Train Loss: 2.5334\n",
      "Epoch 24 | Train Acc: 0.2670 | Test Acc: 0.2504 | Train Loss: 2.3515\n",
      "Epoch 25 | Train Acc: 0.2483 | Test Acc: 0.2714 | Train Loss: 2.2253\n",
      "Epoch 26 | Train Acc: 0.2857 | Test Acc: 0.2541 | Train Loss: 2.1434\n",
      "Epoch 27 | Train Acc: 0.2401 | Test Acc: 0.2293 | Train Loss: 2.0893\n",
      "Epoch 28 | Train Acc: 0.2430 | Test Acc: 0.3436 | Train Loss: 2.0564\n",
      "Epoch 29 | Train Acc: 0.3165 | Test Acc: 0.3782 | Train Loss: 2.0164\n",
      "Epoch 30 | Train Acc: 0.3732 | Test Acc: 0.3835 | Train Loss: 1.9739\n",
      "Epoch 31 | Train Acc: 0.3824 | Test Acc: 0.3699 | Train Loss: 1.9385\n",
      "Epoch 32 | Train Acc: 0.3847 | Test Acc: 0.3556 | Train Loss: 1.8969\n",
      "Epoch 33 | Train Acc: 0.3916 | Test Acc: 0.3331 | Train Loss: 1.8324\n",
      "Epoch 34 | Train Acc: 0.3529 | Test Acc: 0.3737 | Train Loss: 1.7557\n",
      "Epoch 35 | Train Acc: 0.4060 | Test Acc: 0.3534 | Train Loss: 1.6924\n",
      "Epoch 36 | Train Acc: 0.4001 | Test Acc: 0.3406 | Train Loss: 1.6604\n",
      "Epoch 37 | Train Acc: 0.4103 | Test Acc: 0.3684 | Train Loss: 1.6538\n",
      "Epoch 38 | Train Acc: 0.3870 | Test Acc: 0.3895 | Train Loss: 1.6541\n",
      "Epoch 39 | Train Acc: 0.3844 | Test Acc: 0.3827 | Train Loss: 1.6460\n",
      "Epoch 40 | Train Acc: 0.3952 | Test Acc: 0.3865 | Train Loss: 1.6258\n",
      "Epoch 41 | Train Acc: 0.4106 | Test Acc: 0.3662 | Train Loss: 1.5957\n",
      "Epoch 42 | Train Acc: 0.3837 | Test Acc: 0.3624 | Train Loss: 1.5570\n",
      "Epoch 43 | Train Acc: 0.3700 | Test Acc: 0.3895 | Train Loss: 1.5133\n",
      "Epoch 44 | Train Acc: 0.4762 | Test Acc: 0.4323 | Train Loss: 1.4733\n",
      "Epoch 45 | Train Acc: 0.4434 | Test Acc: 0.4368 | Train Loss: 1.4450\n",
      "Epoch 46 | Train Acc: 0.4647 | Test Acc: 0.4226 | Train Loss: 1.4314\n",
      "Epoch 47 | Train Acc: 0.4592 | Test Acc: 0.3985 | Train Loss: 1.4296\n",
      "Epoch 48 | Train Acc: 0.4306 | Test Acc: 0.4023 | Train Loss: 1.4332\n",
      "Epoch 49 | Train Acc: 0.4208 | Test Acc: 0.4128 | Train Loss: 1.4330\n",
      "Epoch 50 | Train Acc: 0.4549 | Test Acc: 0.4143 | Train Loss: 1.4185\n",
      "-----------------------\n",
      "Best test acc in epoch 45, accuracy: 0.4368421137332916\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# 1. Convert adjacency matrix to edge_index\n",
    "A_coo = coo_matrix(adj_matrix)\n",
    "edge_index = torch.tensor(np.vstack((A_coo.row, A_coo.col)), dtype=torch.long)\n",
    "\n",
    "# Feature Matrix Tensor\n",
    "coords = np.array(list(coord_to_index.keys()), dtype=np.float32)\n",
    "feature_matrix = coords  # shape: (N, 2), with [longitude, latitude]\n",
    "x = torch.tensor(feature_matrix, dtype=torch.float32)\n",
    "\n",
    "num_nodes = x.shape[0]\n",
    "\n",
    "# Label Tensor\n",
    "y = torch.full((num_nodes,), -1, dtype=torch.long)  # -1 for unlabeled\n",
    "\n",
    "# Create Masks, indicates which ndoes are used in training and testing\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "# Assign group labels for train, set nodes as part of training set\n",
    "for _, row in train_df.iterrows():\n",
    "    coord = (row['longitude'], row['latitude'])\n",
    "    idx = coord_to_index[coord]\n",
    "    y[idx] = label_index[row['gname']]\n",
    "    train_mask[idx] = True\n",
    "\n",
    "# Assign group labels for test, set nodes as part of testing set\n",
    "for _, row in test_df.iterrows():\n",
    "    coord = (row['longitude'], row['latitude'])\n",
    "    idx = coord_to_index[coord]\n",
    "    y[idx] = label_index[row['gname']]\n",
    "    test_mask[idx] = True\n",
    "\n",
    "# Create PyG Data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = PyTorchGCN(in_channels=x.shape[1], hidden_channels=16, num_classes=len(label_index))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "max_test_acc = 0\n",
    "for epoch in range(50):\n",
    "    train_acc, train_loss = run_epoch(model, data, y, train_mask, optimizer)\n",
    "    test_acc, test_loss = run_epoch(model, data, y, test_mask)\n",
    "    if test_acc > max_test_acc:\n",
    "        max_test_acc = test_acc\n",
    "        max_test_acc_epoch = epoch + 1\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "print('-----------------------')\n",
    "print(f'Best test acc in epoch {max_test_acc_epoch}, accuracy: {max_test_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
