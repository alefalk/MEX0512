{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8b7c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from build_graph_data import *\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a56371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../data/top30groups/engineered_dfs/df_top30_478.csv'\n",
    "data = pd.read_csv(path, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e017e0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before dropping long/lat duplicates:  14340\n",
      "Entries after dropping long/lat duplicates (#Nodes):  6296\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to only contain unique coordinates\n",
    "print(\"Entries before dropping long/lat duplicates: \", len(data))\n",
    "df_unique_geo = create_unique_geo_data(data)\n",
    "print(\"Entries after dropping long/lat duplicates (#Nodes): \", len(df_unique_geo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1fb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates train and test data, first 70% of each group is added to train and remaining 30% to test\n",
    "def handle_leakage(df):\n",
    "    train_frames = []\n",
    "    test_frames = []\n",
    "\n",
    "    #first 70% of each groups attacks to training set, remainin 30% to testing set\n",
    "    for _, group_data in df.groupby('gname'):\n",
    "        split_point = int(len(group_data) * 0.7)  # 70% for training\n",
    "        train_frames.append(group_data.iloc[:split_point])\n",
    "        test_frames.append(group_data.iloc[split_point:])           \n",
    "\n",
    "\n",
    "    # Concatenate all the group-specific splits into final train and test DataFrames\n",
    "    train_df = pd.concat(train_frames)\n",
    "    test_df = pd.concat(test_frames)\n",
    "\n",
    "    # Shuffle each DataFrame separately\n",
    "    train_df = shuffle(train_df)\n",
    "    test_df = shuffle(test_df)\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(len(test_df))\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc1aa2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4392\n",
      "1904\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = handle_leakage(df_unique_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c96dc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total nodes (unique coordinates): 6296\n",
      "Number of unique labels in this set: 30\n"
     ]
    }
   ],
   "source": [
    "# 1. Build coord_to_index from the full dataset (unique coordinate to node index mapping)\n",
    "full_coords = df_unique_geo[['longitude', 'latitude']]\n",
    "coord_to_index = {(row['longitude'], row['latitude']): i for i, row in full_coords.iterrows()}\n",
    "\n",
    "# 2. Build the global graph from the full dataset (used for both train and test)\n",
    "adj_matrix, feature_matrix, label_index = build_graph_data(df_unique_geo, coord_to_index)\n",
    "\n",
    "train_nodes = []\n",
    "train_labels = []\n",
    "for _, row in train_df.iterrows():\n",
    "    train_nodes.append(coord_to_index[(row['longitude'], row['latitude'])])\n",
    "    train_labels.append(label_index[row['gname']])\n",
    "\n",
    "test_nodes = []\n",
    "test_labels = []\n",
    "for _, row in test_df.iterrows():\n",
    "    test_nodes.append(coord_to_index[(row['longitude'], row['latitude'])])\n",
    "    test_labels.append(label_index[row['gname']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc7c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6296, 6296)\n"
     ]
    }
   ],
   "source": [
    "print(adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12314fb",
   "metadata": {},
   "source": [
    "# A simple GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a847fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class PyTorchGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f030b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def run_epoch(model, data, labels, mask, optimizer=None):\n",
    "    is_training = optimizer is not None\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    loss = loss_fn(out[mask], labels[mask])\n",
    "\n",
    "    if is_training:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Accuracy\n",
    "    pred = out[mask].argmax(dim=1)\n",
    "    acc = (pred == labels[mask]).float().mean().item()\n",
    "    return acc, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfcfa5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Acc: 0.0146 | Test Acc: 0.0147 | Train Loss: 23.3899\n",
      "Epoch 02 | Train Acc: 0.0150 | Test Acc: 0.0804 | Train Loss: 20.0060\n",
      "Epoch 03 | Train Acc: 0.0806 | Test Acc: 0.0804 | Train Loss: 17.4098\n",
      "Epoch 04 | Train Acc: 0.0806 | Test Acc: 0.0557 | Train Loss: 14.9580\n",
      "Epoch 05 | Train Acc: 0.0770 | Test Acc: 0.0814 | Train Loss: 12.8267\n",
      "Epoch 06 | Train Acc: 0.0806 | Test Acc: 0.1696 | Train Loss: 11.4418\n",
      "Epoch 07 | Train Acc: 0.1680 | Test Acc: 0.1318 | Train Loss: 10.4444\n",
      "Epoch 08 | Train Acc: 0.1323 | Test Acc: 0.1318 | Train Loss: 9.8422\n",
      "Epoch 09 | Train Acc: 0.1321 | Test Acc: 0.1591 | Train Loss: 9.0653\n",
      "Epoch 10 | Train Acc: 0.1541 | Test Acc: 0.1922 | Train Loss: 8.0827\n",
      "Epoch 11 | Train Acc: 0.2024 | Test Acc: 0.1828 | Train Loss: 7.1565\n",
      "Epoch 12 | Train Acc: 0.2033 | Test Acc: 0.1728 | Train Loss: 6.4345\n",
      "Epoch 13 | Train Acc: 0.1605 | Test Acc: 0.1691 | Train Loss: 5.7955\n",
      "Epoch 14 | Train Acc: 0.1680 | Test Acc: 0.1938 | Train Loss: 5.1911\n",
      "Epoch 15 | Train Acc: 0.1944 | Test Acc: 0.2232 | Train Loss: 4.6158\n",
      "Epoch 16 | Train Acc: 0.2288 | Test Acc: 0.1828 | Train Loss: 4.1365\n",
      "Epoch 17 | Train Acc: 0.1983 | Test Acc: 0.2080 | Train Loss: 3.8503\n",
      "Epoch 18 | Train Acc: 0.2222 | Test Acc: 0.1670 | Train Loss: 3.6502\n",
      "Epoch 19 | Train Acc: 0.1944 | Test Acc: 0.1891 | Train Loss: 3.4280\n",
      "Epoch 20 | Train Acc: 0.2297 | Test Acc: 0.1628 | Train Loss: 3.1985\n",
      "Epoch 21 | Train Acc: 0.2261 | Test Acc: 0.1886 | Train Loss: 3.0225\n",
      "Epoch 22 | Train Acc: 0.2329 | Test Acc: 0.1964 | Train Loss: 2.8004\n",
      "Epoch 23 | Train Acc: 0.2443 | Test Acc: 0.2027 | Train Loss: 2.5203\n",
      "Epoch 24 | Train Acc: 0.2302 | Test Acc: 0.2300 | Train Loss: 2.2891\n",
      "Epoch 25 | Train Acc: 0.2398 | Test Acc: 0.2064 | Train Loss: 2.2398\n",
      "Epoch 26 | Train Acc: 0.1983 | Test Acc: 0.2516 | Train Loss: 2.2972\n",
      "Epoch 27 | Train Acc: 0.2402 | Test Acc: 0.2505 | Train Loss: 2.3036\n",
      "Epoch 28 | Train Acc: 0.2780 | Test Acc: 0.2904 | Train Loss: 2.2418\n",
      "Epoch 29 | Train Acc: 0.2921 | Test Acc: 0.2915 | Train Loss: 2.1631\n",
      "Epoch 30 | Train Acc: 0.3144 | Test Acc: 0.3057 | Train Loss: 2.1178\n",
      "Epoch 31 | Train Acc: 0.3219 | Test Acc: 0.3057 | Train Loss: 2.0898\n",
      "Epoch 32 | Train Acc: 0.3270 | Test Acc: 0.2689 | Train Loss: 2.0524\n",
      "Epoch 33 | Train Acc: 0.3151 | Test Acc: 0.3099 | Train Loss: 2.0005\n",
      "Epoch 34 | Train Acc: 0.3657 | Test Acc: 0.3277 | Train Loss: 1.9449\n",
      "Epoch 35 | Train Acc: 0.3550 | Test Acc: 0.3241 | Train Loss: 1.8997\n",
      "Epoch 36 | Train Acc: 0.3593 | Test Acc: 0.3776 | Train Loss: 1.8617\n",
      "Epoch 37 | Train Acc: 0.4169 | Test Acc: 0.4322 | Train Loss: 1.8250\n",
      "Epoch 38 | Train Acc: 0.4706 | Test Acc: 0.4538 | Train Loss: 1.7896\n",
      "Epoch 39 | Train Acc: 0.4693 | Test Acc: 0.4286 | Train Loss: 1.7534\n",
      "Epoch 40 | Train Acc: 0.4643 | Test Acc: 0.3866 | Train Loss: 1.7097\n",
      "Epoch 41 | Train Acc: 0.4677 | Test Acc: 0.3535 | Train Loss: 1.6627\n",
      "Epoch 42 | Train Acc: 0.4426 | Test Acc: 0.3051 | Train Loss: 1.6312\n",
      "Epoch 43 | Train Acc: 0.4028 | Test Acc: 0.3293 | Train Loss: 1.6220\n",
      "Epoch 44 | Train Acc: 0.3493 | Test Acc: 0.3209 | Train Loss: 1.6229\n",
      "Epoch 45 | Train Acc: 0.3775 | Test Acc: 0.3424 | Train Loss: 1.6188\n",
      "Epoch 46 | Train Acc: 0.4078 | Test Acc: 0.3703 | Train Loss: 1.6031\n",
      "Epoch 47 | Train Acc: 0.4176 | Test Acc: 0.4039 | Train Loss: 1.5788\n",
      "Epoch 48 | Train Acc: 0.4551 | Test Acc: 0.4349 | Train Loss: 1.5534\n",
      "Epoch 49 | Train Acc: 0.4743 | Test Acc: 0.4611 | Train Loss: 1.5331\n",
      "Epoch 50 | Train Acc: 0.5184 | Test Acc: 0.4695 | Train Loss: 1.5204\n",
      "-----------------------\n",
      "Best test acc in epoch 50, accuracy: 0.4695378243923187\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# 1. Convert adjacency matrix to edge_index\n",
    "A_coo = coo_matrix(adj_matrix)\n",
    "edge_index = torch.tensor(np.vstack((A_coo.row, A_coo.col)), dtype=torch.long)\n",
    "\n",
    "# Feature Matrix Tensor\n",
    "coords = np.array(list(coord_to_index.keys()), dtype=np.float32)\n",
    "feature_matrix = coords  # shape: (N, 2), with [longitude, latitude]\n",
    "x = torch.tensor(feature_matrix, dtype=torch.float32)\n",
    "\n",
    "num_nodes = x.shape[0]\n",
    "\n",
    "# Label Tensor\n",
    "y = torch.full((num_nodes,), -1, dtype=torch.long)  # -1 for unlabeled\n",
    "\n",
    "# Create Masks, indicates which ndoes are used in training and testing\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "# Assign group labels for train, set nodes as part of training set\n",
    "for _, row in train_df.iterrows():\n",
    "    coord = (row['longitude'], row['latitude'])\n",
    "    idx = coord_to_index[coord]\n",
    "    y[idx] = label_index[row['gname']]\n",
    "    train_mask[idx] = True\n",
    "\n",
    "# Assign group labels for test, set nodes as part of testing set\n",
    "for _, row in test_df.iterrows():\n",
    "    coord = (row['longitude'], row['latitude'])\n",
    "    idx = coord_to_index[coord]\n",
    "    y[idx] = label_index[row['gname']]\n",
    "    test_mask[idx] = True\n",
    "\n",
    "# Create PyG Data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = PyTorchGCN(in_channels=x.shape[1], hidden_channels=16, num_classes=len(label_index))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "max_test_acc = 0\n",
    "for epoch in range(50):\n",
    "    train_acc, train_loss = run_epoch(model, data, y, train_mask, optimizer)\n",
    "    test_acc, test_loss = run_epoch(model, data, y, test_mask)\n",
    "    if test_acc > max_test_acc:\n",
    "        max_test_acc = test_acc\n",
    "        max_test_acc_epoch = epoch + 1\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "print('-----------------------')\n",
    "print(f'Best test acc in epoch {max_test_acc_epoch}, accuracy: {max_test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
