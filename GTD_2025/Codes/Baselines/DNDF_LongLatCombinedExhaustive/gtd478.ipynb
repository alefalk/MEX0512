{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from train import main\n",
    "from itertools import product  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: n_tree=5, t_depth=1, hd=1024, batch_size=256 tree_feature_rates=0.1, feat_dropouts=0.0, hd=1024, lrs=0.0001\n",
      "combination: 1 / 51840\n",
      "Use gtd478 dataset\n",
      "Patience: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 300/300 [00:40<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy: 0.216068\n",
      "complete\n",
      "\n",
      "Running: n_tree=5, t_depth=1, hd=1024, batch_size=256 tree_feature_rates=0.1, feat_dropouts=0.0, hd=1024, lrs=0.001\n",
      "combination: 2 / 51840\n",
      "Use gtd478 dataset\n",
      "Patience: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  66%|██████▋   | 199/300 [00:26<00:13,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 200\n",
      "\n",
      "Best Accuracy: 0.341816\n",
      "complete\n",
      "\n",
      "Running: n_tree=5, t_depth=1, hd=1024, batch_size=256 tree_feature_rates=0.1, feat_dropouts=0.0, hd=1024, lrs=0.01\n",
      "combination: 3 / 51840\n",
      "Use gtd478 dataset\n",
      "Patience: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  38%|███▊      | 113/300 [00:15<00:25,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 114\n",
      "\n",
      "Best Accuracy: 0.271457\n",
      "complete\n",
      "\n",
      "Running: n_tree=5, t_depth=1, hd=1024, batch_size=256 tree_feature_rates=0.1, feat_dropouts=0.1, hd=1024, lrs=0.0001\n",
      "combination: 4 / 51840\n",
      "Use gtd478 dataset\n",
      "Patience: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████| 300/300 [00:41<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy: 0.194611\n",
      "complete\n",
      "\n",
      "Running: n_tree=5, t_depth=1, hd=1024, batch_size=256 tree_feature_rates=0.1, feat_dropouts=0.1, hd=1024, lrs=0.001\n",
      "combination: 5 / 51840\n",
      "Use gtd478 dataset\n",
      "Patience: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  76%|███████▌  | 227/300 [00:30<00:09,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 228\n",
      "\n",
      "Best Accuracy: 0.306886\n",
      "complete\n",
      "\n",
      "Running: n_tree=5, t_depth=1, hd=1024, batch_size=256 tree_feature_rates=0.1, feat_dropouts=0.1, hd=1024, lrs=0.01\n",
      "combination: 6 / 51840\n",
      "Use gtd478 dataset\n",
      "Patience: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  34%|███▍      | 103/300 [00:14<00:27,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 104\n",
      "\n",
      "Best Accuracy: 0.300399\n",
      "complete\n",
      "\n",
      "Running: n_tree=5, t_depth=1, hd=1024, batch_size=256 tree_feature_rates=0.1, feat_dropouts=0.2, hd=1024, lrs=0.0001\n",
      "combination: 7 / 51840\n",
      "Use gtd478 dataset\n",
      "Patience: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   7%|▋         | 22/300 [00:03<00:39,  6.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombination: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumber_of_partitions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.py\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-dataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgtd\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-searching\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m ]\n\u001b[0;32m---> 43\u001b[0m complete \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(complete)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Read best score from file (assumes one run per file)\u001b[39;00m\n",
      "File \u001b[0;32m~/MEX0512/GTD_2025/Codes/Baselines/DNDF_LongLatCombinedExhaustive/train.py:278\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model, best_preds, best_targets, best_labels, epoch_logs\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m     complete \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complete\n",
      "File \u001b[0;32m~/MEX0512/GTD_2025/Codes/Baselines/DNDF_LongLatCombinedExhaustive/train.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optim, db, opt)\u001b[0m\n\u001b[1;32m    182\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(db[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    184\u001b[0m train_loss_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 185\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_tree_values = [5, 10, 20, 50, 100, 150, 200, 300]\n",
    "tree_depth_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "hidden_dim = [1024, 768]\n",
    "batch_size_values = [256, 512, 1000]\n",
    "\n",
    "#default 0.5\n",
    "tree_feature_rates = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "#default 0.3\n",
    "feat_dropouts = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "#default 0.0001\n",
    "lrs = [0.0001, 0.001, 0.01]\n",
    "\n",
    "best_score = 0\n",
    "best_config = {}\n",
    "\n",
    "number_of_partitions = len(n_tree_values) * len(tree_depth_values) * len(hidden_dim) * len(batch_size_values) * len(tree_feature_rates) * len(feat_dropouts) * len(lrs)\n",
    "\n",
    "i = 0\n",
    "for n_tree, t_depth, hd, batch_size, tfr, do, lr in product(n_tree_values, tree_depth_values, hidden_dim, batch_size_values, tree_feature_rates, feat_dropouts, lrs):\n",
    "    i = i + 1\n",
    "    print(f\"\\nRunning: n_tree={n_tree}, t_depth={t_depth}, hd={hd}, batch_size={batch_size} tree_feature_rates={tfr}, feat_dropouts={do}, hd={hd}, lrs={lr}\")\n",
    "    print(f\"combination: {i} / {number_of_partitions}\")\n",
    "    sys.argv = [\n",
    "        'train.py',\n",
    "        '-dataset', f'gtd{partition}',\n",
    "        '-n_class', '30',\n",
    "        '-gpuid', '0',\n",
    "        '-n_tree', str(n_tree),\n",
    "        '-tree_depth', str(t_depth),\n",
    "        '-batch_size', str(batch_size),\n",
    "        '-hidden_dim', str(hd),\n",
    "        '-epochs', '300',\n",
    "        '-verbose', '0',\n",
    "        '-tree_feature_rate', str(tfr),\n",
    "        '-feat_dropout', str(do),\n",
    "        '-lr', str(lr),\n",
    "        '-jointly_training',\n",
    "        '-searching', '1'\n",
    "    ]\n",
    "    \n",
    "    complete = main()\n",
    "    print(complete)\n",
    "\n",
    "    # Read best score from file (assumes one run per file)\n",
    "    result_file = f\"results/result_gtd{partition}\"\n",
    "    with open(result_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if \"Best Accuracy\" in line:\n",
    "                acc = float(line.split()[2])\n",
    "                if acc > best_score:\n",
    "                    best_score = acc\n",
    "                    best_config = {\n",
    "                        'n_tree': n_tree,\n",
    "                        'tree_depth': t_depth,\n",
    "                        'batch_size': batch_size,\n",
    "                        'hidden_dim': hd,\n",
    "                        'tree_feature_rate': tfr,\n",
    "                        'feat_dropout': do,\n",
    "                        'lr': lr\n",
    "                    }\n",
    "print(\"\\nBest hyperparameter configuration:\")\n",
    "print(best_config)\n",
    "print(f\"Best accuracy: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"sys.argv = [\n",
    "    'train.py',\n",
    "    '-dataset', f'gtd{partition}',\n",
    "    '-n_class', '30',\n",
    "    '-gpuid', '0',\n",
    "    '-n_tree', str(best_config['n_tree']),\n",
    "    '-tree_depth', str(best_config['tree_depth']),\n",
    "    '-batch_size', str(best_config['batch_size']),\n",
    "    '-epochs', '1000',\n",
    "    '-verbose', '1',\n",
    "    '-jointly_training'\n",
    "]\"\"\"\n",
    "\n",
    "sys.argv = [\n",
    "        'train.py',\n",
    "        '-dataset', f'gtd{partition}',\n",
    "        '-n_class', '30',\n",
    "        '-gpuid', '0',\n",
    "        '-n_tree', str(best_config['n_tree']),\n",
    "        '-tree_depth', str(best_config['tree_depth']),\n",
    "        '-batch_size', str(best_config['batch_size']),\n",
    "        '-hidden_dim', str(best_config['hidden_dim']),\n",
    "        '-epochs', '1500',\n",
    "        '-verbose', '0',\n",
    "        '-tree_feature_rate', str(best_config['tree_feature_rate']),\n",
    "        '-feat_dropout', str(best_config['feat_dropout']),\n",
    "        '-lr', str(best_config['lr']),\n",
    "        '-jointly_training',\n",
    "        '-searching', '0'\n",
    "    ]\n",
    "\n",
    "best_model, preds, targets, labels, epoch_logs = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(targets, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels, partition):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(labels)))\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(cm_normalized,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels,\n",
    "                cmap=\"viridis\",\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "    plt.title(f\"Normalized Confusion Matrix (Partition gtd{partition})\", fontsize=18)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "    plt.ylabel(\"True Label\", fontsize=14)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = f\"results/confusion_matrix_partition_gtd{partition}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved confusion matrix for partition gtd{partition} to {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(targets, preds, labels, partition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
