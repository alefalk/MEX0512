{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = f'../../../../../data/top30groups/noGeographic/scaledtrain1/train{partition}.csv'\n",
    "testpath = f'../../../../../data/top30groups/noGeographic/scaledtest1/test{partition}.csv'\n",
    "\n",
    "traindata = pd.read_csv(trainpath, encoding='ISO-8859-1')\n",
    "testdata = pd.read_csv(testpath, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4320, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10020, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(dftrain, dftest):\n",
    "    Xtrain = dftrain.drop(columns=['gname']).values\n",
    "    Ytrain = dftrain['gname'].values\n",
    "    Xtest = dftest.drop(columns=['gname']).values\n",
    "    Ytest = dftest['gname'].values\n",
    "\n",
    "    # Encode labels as integers\n",
    "    le = LabelEncoder()\n",
    "    Ytrain = le.fit_transform(Ytrain)\n",
    "    Ytest = le.transform(Ytest)\n",
    "\n",
    "    Xtrain = Xtrain.astype(float)\n",
    "    Xtest = Xtest.astype(float)\n",
    "\n",
    "    # Convert to torch tensors and move to GPU\n",
    "    Xtrain = torch.tensor(Xtrain, dtype=torch.float32).to(\"cuda\")\n",
    "    Ytrain = torch.tensor(Ytrain, dtype=torch.long).to(\"cuda\")\n",
    "    Xtest = torch.tensor(Xtest, dtype=torch.float32).to(\"cuda\")\n",
    "    Ytest = torch.tensor(Ytest, dtype=torch.long).to(\"cuda\")\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "class MLP3Layer(nn.Module):\n",
    "    def __init__(self, input_dim, h1, h2, h3, output_dim, activation='relu'):\n",
    "        super().__init__()\n",
    "        act_fn = nn.ReLU() if activation == 'relu' else nn.Tanh()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, h1),\n",
    "            act_fn,\n",
    "            nn.Linear(h1, h2),\n",
    "            act_fn,\n",
    "            nn.Linear(h2, h3),\n",
    "            act_fn,\n",
    "            nn.Linear(h3, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_model(model, Xtrain, Ytrain, lr, alpha, searching=False, max_epochs=1000):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=alpha)\n",
    "\n",
    "    epoch_times = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    best_acc = -1\n",
    "    best_epoch = -1\n",
    "    best_state_dict = None\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Training step\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(Xtrain)\n",
    "        loss = criterion(output, Ytrain)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy on full training set\n",
    "        if not searching:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = output.argmax(dim=1)\n",
    "                acc = (pred == Ytrain).float().mean().item()\n",
    "                train_accuracies.append(acc)\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_epoch = epoch\n",
    "                    best_state_dict = model.state_dict()\n",
    "\n",
    "            end_time = time.time()\n",
    "            epoch_times.append(end_time - start_time)\n",
    "\n",
    "            print(f\"Epoch {epoch+1:03d}: loss = {loss.item():.4f}, acc = {acc:.4f}, time = {end_time - start_time:.3f}s\")\n",
    "\n",
    "    # Restore best model weights\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    if not searching:\n",
    "        print(f\"best epoch: {best_epoch} Best acc: {best_acc}\")\n",
    "\n",
    "    return model, epoch_times, train_accuracies, best_epoch, best_acc\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def evaluate_model(model, Xval, Yval):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(Xval).argmax(dim=1)\n",
    "        acc = (pred == Yval).float().mean().item()\n",
    "    return acc\n",
    "\n",
    "def find_best_mlp_3layer(Xtrain, Ytrain, num_classes, max_epochs=1000):\n",
    "    input_dim = Xtrain.shape[1]\n",
    "\n",
    "    param_grid = {\n",
    "        'h1': [100, 150],\n",
    "        'h2': [50, 100],\n",
    "        'h3': [25, 50],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'lr': [0.001, 0.01],\n",
    "        'alpha': [1e-4, 1e-3]\n",
    "    }\n",
    "\n",
    "    best_acc = -1\n",
    "    best_params = None\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        indices = torch.randperm(Xtrain.size(0))\n",
    "        split = int(0.8 * len(indices))\n",
    "        train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "        model = MLP3Layer(\n",
    "            input_dim=input_dim,\n",
    "            h1=params['h1'],\n",
    "            h2=params['h2'],\n",
    "            h3=params['h3'],\n",
    "            output_dim=num_classes,\n",
    "            activation=params['activation']\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        _ = train_model(model, Xtrain[train_idx], Ytrain[train_idx],\n",
    "                    lr=params['lr'], alpha=params['alpha'], searching=True, max_epochs=max_epochs)\n",
    "\n",
    "        acc = evaluate_model(model, Xtrain[val_idx], Ytrain[val_idx])\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_params = params\n",
    "\n",
    "   \n",
    "    final_model = MLP3Layer(\n",
    "        input_dim=input_dim,\n",
    "        h1=best_params['h1'],\n",
    "        h2=best_params['h2'],\n",
    "        h3=best_params['h3'],\n",
    "        output_dim=num_classes,\n",
    "        activation=best_params['activation']\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    _, epoch_times, train_accuracies, best_epoch, best_acc = train_model(final_model, Xtrain, Ytrain,\n",
    "                lr=best_params['lr'], alpha=best_params['alpha'], searching=False, max_epochs=max_epochs)\n",
    "\n",
    "    print(f\"Best accuracy on validation split: {best_acc * 100:.2f}%\")\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "    return final_model, epoch_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: loss = 3.4108, acc = 0.0173, time = 0.000s\n",
      "Epoch 002: loss = 3.2947, acc = 0.1359, time = 0.006s\n",
      "Epoch 003: loss = 3.1761, acc = 0.1594, time = 0.001s\n",
      "Epoch 004: loss = 3.0499, acc = 0.1622, time = 0.000s\n",
      "Epoch 005: loss = 2.9540, acc = 0.1822, time = 0.000s\n",
      "Epoch 006: loss = 2.8982, acc = 0.2081, time = 0.006s\n",
      "Epoch 007: loss = 2.8574, acc = 0.2001, time = 0.000s\n",
      "Epoch 008: loss = 2.8233, acc = 0.2040, time = 0.000s\n",
      "Epoch 009: loss = 2.7953, acc = 0.2155, time = 0.000s\n",
      "Epoch 010: loss = 2.7660, acc = 0.2354, time = 0.007s\n",
      "Epoch 011: loss = 2.7356, acc = 0.2462, time = 0.000s\n",
      "Epoch 012: loss = 2.7094, acc = 0.2535, time = 0.003s\n",
      "Epoch 013: loss = 2.6854, acc = 0.2573, time = 0.000s\n",
      "Epoch 014: loss = 2.6606, acc = 0.2759, time = 0.005s\n",
      "Epoch 015: loss = 2.6344, acc = 0.2815, time = 0.002s\n",
      "Epoch 016: loss = 2.6115, acc = 0.2826, time = 0.000s\n",
      "Epoch 017: loss = 2.5891, acc = 0.2843, time = 0.003s\n",
      "Epoch 018: loss = 2.5623, acc = 0.2958, time = 0.001s\n",
      "Epoch 019: loss = 2.5365, acc = 0.3003, time = 0.000s\n",
      "Epoch 020: loss = 2.5123, acc = 0.3079, time = 0.000s\n",
      "Epoch 021: loss = 2.4892, acc = 0.3151, time = 0.006s\n",
      "Epoch 022: loss = 2.4646, acc = 0.3215, time = 0.001s\n",
      "Epoch 023: loss = 2.4403, acc = 0.3215, time = 0.000s\n",
      "Epoch 024: loss = 2.4152, acc = 0.3251, time = 0.000s\n",
      "Epoch 025: loss = 2.3898, acc = 0.3332, time = 0.006s\n",
      "Epoch 026: loss = 2.3640, acc = 0.3399, time = 0.001s\n",
      "Epoch 027: loss = 2.3387, acc = 0.3426, time = 0.000s\n",
      "Epoch 028: loss = 2.3130, acc = 0.3484, time = 0.000s\n",
      "Epoch 029: loss = 2.2878, acc = 0.3553, time = 0.005s\n",
      "Epoch 030: loss = 2.2629, acc = 0.3591, time = 0.001s\n",
      "Epoch 031: loss = 2.2382, acc = 0.3667, time = 0.000s\n",
      "Epoch 032: loss = 2.2134, acc = 0.3746, time = 0.000s\n",
      "Epoch 033: loss = 2.1891, acc = 0.3794, time = 0.007s\n",
      "Epoch 034: loss = 2.1664, acc = 0.3854, time = 0.000s\n",
      "Epoch 035: loss = 2.1430, acc = 0.3888, time = 0.000s\n",
      "Epoch 036: loss = 2.1203, acc = 0.3967, time = 0.000s\n",
      "Epoch 037: loss = 2.0976, acc = 0.4036, time = 0.006s\n",
      "Epoch 038: loss = 2.0760, acc = 0.4116, time = 0.001s\n",
      "Epoch 039: loss = 2.0561, acc = 0.4195, time = 0.002s\n",
      "Epoch 040: loss = 2.0394, acc = 0.4215, time = 0.000s\n",
      "Epoch 041: loss = 2.0271, acc = 0.4215, time = 0.004s\n",
      "Epoch 042: loss = 2.0012, acc = 0.4317, time = 0.000s\n",
      "Epoch 043: loss = 1.9792, acc = 0.4343, time = 0.000s\n",
      "Epoch 044: loss = 1.9682, acc = 0.4314, time = 0.000s\n",
      "Epoch 045: loss = 1.9470, acc = 0.4364, time = 0.007s\n",
      "Epoch 046: loss = 1.9317, acc = 0.4438, time = 0.001s\n",
      "Epoch 047: loss = 1.9248, acc = 0.4376, time = 0.000s\n",
      "Epoch 048: loss = 1.9128, acc = 0.4469, time = 0.000s\n",
      "Epoch 049: loss = 1.8928, acc = 0.4507, time = 0.006s\n",
      "Epoch 050: loss = 1.8761, acc = 0.4533, time = 0.000s\n",
      "Epoch 051: loss = 1.8700, acc = 0.4591, time = 0.000s\n",
      "Epoch 052: loss = 1.8481, acc = 0.4642, time = 0.000s\n",
      "Epoch 053: loss = 1.8436, acc = 0.4599, time = 0.006s\n",
      "Epoch 054: loss = 1.8271, acc = 0.4682, time = 0.001s\n",
      "Epoch 055: loss = 1.8147, acc = 0.4716, time = 0.000s\n",
      "Epoch 056: loss = 1.8085, acc = 0.4682, time = 0.000s\n",
      "Epoch 057: loss = 1.7904, acc = 0.4754, time = 0.001s\n",
      "Epoch 058: loss = 1.7857, acc = 0.4774, time = 0.000s\n",
      "Epoch 059: loss = 1.7727, acc = 0.4805, time = 0.000s\n",
      "Epoch 060: loss = 1.7612, acc = 0.4865, time = 0.000s\n",
      "Epoch 061: loss = 1.7548, acc = 0.4830, time = 0.007s\n",
      "Epoch 062: loss = 1.7417, acc = 0.4898, time = 0.000s\n",
      "Epoch 063: loss = 1.7338, acc = 0.4967, time = 0.000s\n",
      "Epoch 064: loss = 1.7244, acc = 0.4928, time = 0.005s\n",
      "Epoch 065: loss = 1.7148, acc = 0.4954, time = 0.002s\n",
      "Epoch 066: loss = 1.7058, acc = 0.5015, time = 0.000s\n",
      "Epoch 067: loss = 1.6966, acc = 0.5019, time = 0.000s\n",
      "Epoch 068: loss = 1.6892, acc = 0.5018, time = 0.000s\n",
      "Epoch 069: loss = 1.6797, acc = 0.5101, time = 0.007s\n",
      "Epoch 070: loss = 1.6709, acc = 0.5084, time = 0.000s\n",
      "Epoch 071: loss = 1.6639, acc = 0.5123, time = 0.000s\n",
      "Epoch 072: loss = 1.6556, acc = 0.5144, time = 0.005s\n",
      "Epoch 073: loss = 1.6471, acc = 0.5161, time = 0.002s\n",
      "Epoch 074: loss = 1.6412, acc = 0.5161, time = 0.000s\n",
      "Epoch 075: loss = 1.6343, acc = 0.5206, time = 0.000s\n",
      "Epoch 076: loss = 1.6276, acc = 0.5196, time = 0.000s\n",
      "Epoch 077: loss = 1.6234, acc = 0.5223, time = 0.000s\n",
      "Epoch 078: loss = 1.6241, acc = 0.5195, time = 0.000s\n",
      "Epoch 079: loss = 1.6274, acc = 0.5192, time = 0.000s\n",
      "Epoch 080: loss = 1.6258, acc = 0.5135, time = 0.012s\n",
      "Epoch 081: loss = 1.6111, acc = 0.5286, time = 0.002s\n",
      "Epoch 082: loss = 1.6001, acc = 0.5273, time = 0.001s\n",
      "Epoch 083: loss = 1.5900, acc = 0.5269, time = 0.000s\n",
      "Epoch 084: loss = 1.5882, acc = 0.5366, time = 0.004s\n",
      "Epoch 085: loss = 1.5802, acc = 0.5355, time = 0.002s\n",
      "Epoch 086: loss = 1.5661, acc = 0.5369, time = 0.000s\n",
      "Epoch 087: loss = 1.5712, acc = 0.5385, time = 0.000s\n",
      "Epoch 088: loss = 1.5572, acc = 0.5425, time = 0.005s\n",
      "Epoch 089: loss = 1.5508, acc = 0.5460, time = 0.002s\n",
      "Epoch 090: loss = 1.5520, acc = 0.5466, time = 0.000s\n",
      "Epoch 091: loss = 1.5396, acc = 0.5516, time = 0.000s\n",
      "Epoch 092: loss = 1.5380, acc = 0.5527, time = 0.000s\n",
      "Epoch 093: loss = 1.5309, acc = 0.5550, time = 0.000s\n",
      "Epoch 094: loss = 1.5280, acc = 0.5536, time = 0.000s\n",
      "Epoch 095: loss = 1.5204, acc = 0.5609, time = 0.000s\n",
      "Epoch 096: loss = 1.5155, acc = 0.5623, time = 0.013s\n",
      "Epoch 097: loss = 1.5129, acc = 0.5560, time = 0.001s\n",
      "Epoch 098: loss = 1.5067, acc = 0.5667, time = 0.000s\n",
      "Epoch 099: loss = 1.5014, acc = 0.5635, time = 0.000s\n",
      "Epoch 100: loss = 1.4971, acc = 0.5657, time = 0.000s\n",
      "Epoch 101: loss = 1.4929, acc = 0.5720, time = 0.000s\n",
      "Epoch 102: loss = 1.4895, acc = 0.5652, time = 0.000s\n",
      "Epoch 103: loss = 1.4840, acc = 0.5715, time = 0.000s\n",
      "Epoch 104: loss = 1.4807, acc = 0.5736, time = 0.013s\n",
      "Epoch 105: loss = 1.4793, acc = 0.5698, time = 0.001s\n",
      "Epoch 106: loss = 1.4803, acc = 0.5766, time = 0.002s\n",
      "Epoch 107: loss = 1.4834, acc = 0.5636, time = 0.000s\n",
      "Epoch 108: loss = 1.4731, acc = 0.5824, time = 0.004s\n",
      "Epoch 109: loss = 1.4609, acc = 0.5775, time = 0.001s\n",
      "Epoch 110: loss = 1.4578, acc = 0.5791, time = 0.000s\n",
      "Epoch 111: loss = 1.4588, acc = 0.5839, time = 0.000s\n",
      "Epoch 112: loss = 1.4535, acc = 0.5814, time = 0.006s\n",
      "Epoch 113: loss = 1.4451, acc = 0.5873, time = 0.001s\n",
      "Epoch 114: loss = 1.4446, acc = 0.5843, time = 0.000s\n",
      "Epoch 115: loss = 1.4449, acc = 0.5868, time = 0.000s\n",
      "Epoch 116: loss = 1.4359, acc = 0.5904, time = 0.000s\n",
      "Epoch 117: loss = 1.4306, acc = 0.5942, time = 0.000s\n",
      "Epoch 118: loss = 1.4308, acc = 0.5899, time = 0.000s\n",
      "Epoch 119: loss = 1.4263, acc = 0.5982, time = 0.000s\n",
      "Epoch 120: loss = 1.4200, acc = 0.5968, time = 0.000s\n",
      "Epoch 121: loss = 1.4171, acc = 0.5961, time = 0.000s\n",
      "Epoch 122: loss = 1.4164, acc = 0.6038, time = 0.014s\n",
      "Epoch 123: loss = 1.4123, acc = 0.5953, time = 0.000s\n",
      "Epoch 124: loss = 1.4070, acc = 0.6038, time = 0.000s\n",
      "Epoch 125: loss = 1.4060, acc = 0.6044, time = 0.006s\n",
      "Epoch 126: loss = 1.4070, acc = 0.5970, time = 0.000s\n",
      "Epoch 127: loss = 1.4070, acc = 0.5939, time = 0.000s\n",
      "Epoch 128: loss = 1.4103, acc = 0.6005, time = 0.000s\n",
      "Epoch 129: loss = 1.4187, acc = 0.5752, time = 0.000s\n",
      "Epoch 130: loss = 1.4129, acc = 0.6024, time = 0.000s\n",
      "Epoch 131: loss = 1.3923, acc = 0.5984, time = 0.000s\n",
      "Epoch 132: loss = 1.3839, acc = 0.6096, time = 0.000s\n",
      "Epoch 133: loss = 1.3929, acc = 0.6081, time = 0.000s\n",
      "Epoch 134: loss = 1.3927, acc = 0.5917, time = 0.000s\n",
      "Epoch 135: loss = 1.3776, acc = 0.6139, time = 0.016s\n",
      "Epoch 136: loss = 1.3754, acc = 0.6136, time = 0.000s\n",
      "Epoch 137: loss = 1.3807, acc = 0.5966, time = 0.000s\n",
      "Epoch 138: loss = 1.3738, acc = 0.6155, time = 0.005s\n",
      "Epoch 139: loss = 1.3656, acc = 0.6173, time = 0.000s\n",
      "Epoch 140: loss = 1.3672, acc = 0.6112, time = 0.000s\n",
      "Epoch 141: loss = 1.3659, acc = 0.6205, time = 0.000s\n",
      "Epoch 142: loss = 1.3588, acc = 0.6167, time = 0.006s\n",
      "Epoch 143: loss = 1.3565, acc = 0.6183, time = 0.001s\n",
      "Epoch 144: loss = 1.3570, acc = 0.6213, time = 0.000s\n",
      "Epoch 145: loss = 1.3530, acc = 0.6164, time = 0.000s\n",
      "Epoch 146: loss = 1.3478, acc = 0.6259, time = 0.006s\n",
      "Epoch 147: loss = 1.3482, acc = 0.6242, time = 0.001s\n",
      "Epoch 148: loss = 1.3471, acc = 0.6184, time = 0.000s\n",
      "Epoch 149: loss = 1.3413, acc = 0.6298, time = 0.000s\n",
      "Epoch 150: loss = 1.3392, acc = 0.6286, time = 0.006s\n",
      "Epoch 151: loss = 1.3394, acc = 0.6222, time = 0.001s\n",
      "Epoch 152: loss = 1.3360, acc = 0.6291, time = 0.000s\n",
      "Epoch 153: loss = 1.3321, acc = 0.6272, time = 0.000s\n",
      "Epoch 154: loss = 1.3304, acc = 0.6290, time = 0.000s\n",
      "Epoch 155: loss = 1.3296, acc = 0.6331, time = 0.007s\n",
      "Epoch 156: loss = 1.3273, acc = 0.6311, time = 0.000s\n",
      "Epoch 157: loss = 1.3247, acc = 0.6319, time = 0.000s\n",
      "Epoch 158: loss = 1.3238, acc = 0.6357, time = 0.000s\n",
      "Epoch 159: loss = 1.3261, acc = 0.6261, time = 0.006s\n",
      "Epoch 160: loss = 1.3274, acc = 0.6379, time = 0.002s\n",
      "Epoch 161: loss = 1.3275, acc = 0.6244, time = 0.001s\n",
      "Epoch 162: loss = 1.3206, acc = 0.6371, time = 0.002s\n",
      "Epoch 163: loss = 1.3140, acc = 0.6335, time = 0.002s\n",
      "Epoch 164: loss = 1.3111, acc = 0.6298, time = 0.001s\n",
      "Epoch 165: loss = 1.3122, acc = 0.6414, time = 0.000s\n",
      "Epoch 166: loss = 1.3135, acc = 0.6290, time = 0.000s\n",
      "Epoch 167: loss = 1.3084, acc = 0.6395, time = 0.000s\n",
      "Epoch 168: loss = 1.3033, acc = 0.6381, time = 0.000s\n",
      "Epoch 169: loss = 1.3022, acc = 0.6382, time = 0.000s\n",
      "Epoch 170: loss = 1.3031, acc = 0.6426, time = 0.000s\n",
      "Epoch 171: loss = 1.3019, acc = 0.6351, time = 0.000s\n",
      "Epoch 172: loss = 1.2967, acc = 0.6425, time = 0.000s\n",
      "Epoch 173: loss = 1.2930, acc = 0.6456, time = 0.015s\n",
      "Epoch 174: loss = 1.2930, acc = 0.6382, time = 0.000s\n",
      "Epoch 175: loss = 1.2933, acc = 0.6464, time = 0.000s\n",
      "Epoch 176: loss = 1.2912, acc = 0.6393, time = 0.000s\n",
      "Epoch 177: loss = 1.2873, acc = 0.6453, time = 0.000s\n",
      "Epoch 178: loss = 1.2852, acc = 0.6456, time = 0.000s\n",
      "Epoch 179: loss = 1.2853, acc = 0.6413, time = 0.000s\n",
      "Epoch 180: loss = 1.2845, acc = 0.6475, time = 0.000s\n",
      "Epoch 181: loss = 1.2823, acc = 0.6413, time = 0.000s\n",
      "Epoch 182: loss = 1.2791, acc = 0.6492, time = 0.000s\n",
      "Epoch 183: loss = 1.2778, acc = 0.6492, time = 0.016s\n",
      "Epoch 184: loss = 1.2779, acc = 0.6435, time = 0.000s\n",
      "Epoch 185: loss = 1.2777, acc = 0.6505, time = 0.000s\n",
      "Epoch 186: loss = 1.2770, acc = 0.6399, time = 0.000s\n",
      "Epoch 187: loss = 1.2762, acc = 0.6484, time = 0.000s\n",
      "Epoch 188: loss = 1.2773, acc = 0.6399, time = 0.000s\n",
      "Epoch 189: loss = 1.2790, acc = 0.6442, time = 0.000s\n",
      "Epoch 190: loss = 1.2801, acc = 0.6395, time = 0.000s\n",
      "Epoch 191: loss = 1.2767, acc = 0.6440, time = 0.000s\n",
      "Epoch 192: loss = 1.2697, acc = 0.6456, time = 0.000s\n",
      "Epoch 193: loss = 1.2636, acc = 0.6510, time = 0.017s\n",
      "Epoch 194: loss = 1.2615, acc = 0.6548, time = 0.000s\n",
      "Epoch 195: loss = 1.2624, acc = 0.6511, time = 0.000s\n",
      "Epoch 196: loss = 1.2632, acc = 0.6511, time = 0.000s\n",
      "Epoch 197: loss = 1.2613, acc = 0.6491, time = 0.000s\n",
      "Epoch 198: loss = 1.2576, acc = 0.6534, time = 0.000s\n",
      "Epoch 199: loss = 1.2547, acc = 0.6557, time = 0.000s\n",
      "Epoch 200: loss = 1.2536, acc = 0.6541, time = 0.000s\n",
      "Epoch 201: loss = 1.2536, acc = 0.6570, time = 0.000s\n",
      "Epoch 202: loss = 1.2524, acc = 0.6533, time = 0.000s\n",
      "Epoch 203: loss = 1.2501, acc = 0.6565, time = 0.016s\n",
      "Epoch 204: loss = 1.2477, acc = 0.6557, time = 0.000s\n",
      "Epoch 205: loss = 1.2462, acc = 0.6582, time = 0.000s\n",
      "Epoch 206: loss = 1.2454, acc = 0.6569, time = 0.000s\n",
      "Epoch 207: loss = 1.2446, acc = 0.6577, time = 0.000s\n",
      "Epoch 208: loss = 1.2432, acc = 0.6581, time = 0.000s\n",
      "Epoch 209: loss = 1.2410, acc = 0.6585, time = 0.000s\n",
      "Epoch 210: loss = 1.2390, acc = 0.6603, time = 0.000s\n",
      "Epoch 211: loss = 1.2376, acc = 0.6591, time = 0.000s\n",
      "Epoch 212: loss = 1.2368, acc = 0.6616, time = 0.000s\n",
      "Epoch 213: loss = 1.2361, acc = 0.6553, time = 0.000s\n",
      "Epoch 214: loss = 1.2349, acc = 0.6636, time = 0.000s\n",
      "Epoch 215: loss = 1.2333, acc = 0.6568, time = 0.016s\n",
      "Epoch 216: loss = 1.2313, acc = 0.6637, time = 0.000s\n",
      "Epoch 217: loss = 1.2296, acc = 0.6613, time = 0.000s\n",
      "Epoch 218: loss = 1.2285, acc = 0.6622, time = 0.000s\n",
      "Epoch 219: loss = 1.2276, acc = 0.6634, time = 0.000s\n",
      "Epoch 220: loss = 1.2271, acc = 0.6625, time = 0.010s\n",
      "Epoch 221: loss = 1.2263, acc = 0.6647, time = 0.002s\n",
      "Epoch 222: loss = 1.2254, acc = 0.6639, time = 0.002s\n",
      "Epoch 223: loss = 1.2242, acc = 0.6639, time = 0.000s\n",
      "Epoch 224: loss = 1.2231, acc = 0.6644, time = 0.000s\n",
      "Epoch 225: loss = 1.2223, acc = 0.6649, time = 0.000s\n",
      "Epoch 226: loss = 1.2224, acc = 0.6648, time = 0.000s\n",
      "Epoch 227: loss = 1.2241, acc = 0.6548, time = 0.000s\n",
      "Epoch 228: loss = 1.2284, acc = 0.6620, time = 0.000s\n",
      "Epoch 229: loss = 1.2371, acc = 0.6432, time = 0.010s\n",
      "Epoch 230: loss = 1.2465, acc = 0.6471, time = 0.000s\n",
      "Epoch 231: loss = 1.2542, acc = 0.6330, time = 0.000s\n",
      "Epoch 232: loss = 1.2386, acc = 0.6500, time = 0.000s\n",
      "Epoch 233: loss = 1.2172, acc = 0.6593, time = 0.000s\n",
      "Epoch 234: loss = 1.2120, acc = 0.6676, time = 0.000s\n",
      "Epoch 235: loss = 1.2242, acc = 0.6587, time = 0.000s\n",
      "Epoch 236: loss = 1.2288, acc = 0.6444, time = 0.000s\n",
      "Epoch 237: loss = 1.2142, acc = 0.6668, time = 0.000s\n",
      "Epoch 238: loss = 1.2084, acc = 0.6671, time = 0.000s\n",
      "Epoch 239: loss = 1.2158, acc = 0.6559, time = 0.016s\n",
      "Epoch 240: loss = 1.2135, acc = 0.6650, time = 0.000s\n",
      "Epoch 241: loss = 1.2048, acc = 0.6666, time = 0.000s\n",
      "Epoch 242: loss = 1.2044, acc = 0.6670, time = 0.000s\n",
      "Epoch 243: loss = 1.2088, acc = 0.6646, time = 0.000s\n",
      "Epoch 244: loss = 1.2056, acc = 0.6621, time = 0.000s\n",
      "Epoch 245: loss = 1.1987, acc = 0.6738, time = 0.012s\n",
      "Epoch 246: loss = 1.2004, acc = 0.6710, time = 0.001s\n",
      "Epoch 247: loss = 1.2034, acc = 0.6633, time = 0.000s\n",
      "Epoch 248: loss = 1.1985, acc = 0.6704, time = 0.000s\n",
      "Epoch 249: loss = 1.1955, acc = 0.6735, time = 0.007s\n",
      "Epoch 250: loss = 1.1971, acc = 0.6675, time = 0.001s\n",
      "Epoch 251: loss = 1.1961, acc = 0.6734, time = 0.001s\n",
      "Epoch 252: loss = 1.1927, acc = 0.6727, time = 0.001s\n",
      "Epoch 253: loss = 1.1917, acc = 0.6727, time = 0.001s\n",
      "Epoch 254: loss = 1.1926, acc = 0.6726, time = 0.003s\n",
      "Epoch 255: loss = 1.1916, acc = 0.6691, time = 0.001s\n",
      "Epoch 256: loss = 1.1889, acc = 0.6733, time = 0.001s\n",
      "Epoch 257: loss = 1.1880, acc = 0.6752, time = 0.003s\n",
      "Epoch 258: loss = 1.1885, acc = 0.6718, time = 0.001s\n",
      "Epoch 259: loss = 1.1873, acc = 0.6761, time = 0.002s\n",
      "Epoch 260: loss = 1.1849, acc = 0.6741, time = 0.001s\n",
      "Epoch 261: loss = 1.1842, acc = 0.6770, time = 0.003s\n",
      "Epoch 262: loss = 1.1843, acc = 0.6744, time = 0.001s\n",
      "Epoch 263: loss = 1.1827, acc = 0.6769, time = 0.001s\n",
      "Epoch 264: loss = 1.1808, acc = 0.6761, time = 0.002s\n",
      "Epoch 265: loss = 1.1801, acc = 0.6764, time = 0.002s\n",
      "Epoch 266: loss = 1.1798, acc = 0.6716, time = 0.001s\n",
      "Epoch 267: loss = 1.1788, acc = 0.6766, time = 0.002s\n",
      "Epoch 268: loss = 1.1776, acc = 0.6750, time = 0.001s\n",
      "Epoch 269: loss = 1.1768, acc = 0.6764, time = 0.001s\n",
      "Epoch 270: loss = 1.1764, acc = 0.6769, time = 0.001s\n",
      "Epoch 271: loss = 1.1757, acc = 0.6750, time = 0.002s\n",
      "Epoch 272: loss = 1.1746, acc = 0.6774, time = 0.001s\n",
      "Epoch 273: loss = 1.1734, acc = 0.6769, time = 0.000s\n",
      "Epoch 274: loss = 1.1727, acc = 0.6759, time = 0.000s\n",
      "Epoch 275: loss = 1.1724, acc = 0.6782, time = 0.007s\n",
      "Epoch 276: loss = 1.1722, acc = 0.6750, time = 0.001s\n",
      "Epoch 277: loss = 1.1718, acc = 0.6795, time = 0.000s\n",
      "Epoch 278: loss = 1.1719, acc = 0.6750, time = 0.004s\n",
      "Epoch 279: loss = 1.1710, acc = 0.6785, time = 0.002s\n",
      "Epoch 280: loss = 1.1699, acc = 0.6762, time = 0.001s\n",
      "Epoch 281: loss = 1.1680, acc = 0.6804, time = 0.001s\n",
      "Epoch 282: loss = 1.1673, acc = 0.6781, time = 0.000s\n",
      "Epoch 283: loss = 1.1671, acc = 0.6778, time = 0.005s\n",
      "Epoch 284: loss = 1.1675, acc = 0.6735, time = 0.002s\n",
      "Epoch 285: loss = 1.1663, acc = 0.6784, time = 0.001s\n",
      "Epoch 286: loss = 1.1648, acc = 0.6734, time = 0.003s\n",
      "Epoch 287: loss = 1.1629, acc = 0.6807, time = 0.003s\n",
      "Epoch 288: loss = 1.1617, acc = 0.6773, time = 0.001s\n",
      "Epoch 289: loss = 1.1606, acc = 0.6809, time = 0.002s\n",
      "Epoch 290: loss = 1.1594, acc = 0.6800, time = 0.001s\n",
      "Epoch 291: loss = 1.1582, acc = 0.6809, time = 0.001s\n",
      "Epoch 292: loss = 1.1572, acc = 0.6808, time = 0.001s\n",
      "Epoch 293: loss = 1.1564, acc = 0.6805, time = 0.001s\n",
      "Epoch 294: loss = 1.1558, acc = 0.6830, time = 0.003s\n",
      "Epoch 295: loss = 1.1554, acc = 0.6796, time = 0.002s\n",
      "Epoch 296: loss = 1.1549, acc = 0.6819, time = 0.002s\n",
      "Epoch 297: loss = 1.1543, acc = 0.6806, time = 0.001s\n",
      "Epoch 298: loss = 1.1536, acc = 0.6820, time = 0.003s\n",
      "Epoch 299: loss = 1.1531, acc = 0.6814, time = 0.002s\n",
      "Epoch 300: loss = 1.1527, acc = 0.6824, time = 0.002s\n",
      "Epoch 301: loss = 1.1524, acc = 0.6806, time = 0.001s\n",
      "Epoch 302: loss = 1.1518, acc = 0.6816, time = 0.002s\n",
      "Epoch 303: loss = 1.1512, acc = 0.6815, time = 0.002s\n",
      "Epoch 304: loss = 1.1503, acc = 0.6826, time = 0.002s\n",
      "Epoch 305: loss = 1.1493, acc = 0.6811, time = 0.002s\n",
      "Epoch 306: loss = 1.1480, acc = 0.6831, time = 0.001s\n",
      "Epoch 307: loss = 1.1470, acc = 0.6834, time = 0.002s\n",
      "Epoch 308: loss = 1.1459, acc = 0.6837, time = 0.001s\n",
      "Epoch 309: loss = 1.1449, acc = 0.6837, time = 0.003s\n",
      "Epoch 310: loss = 1.1439, acc = 0.6849, time = 0.002s\n",
      "Epoch 311: loss = 1.1429, acc = 0.6838, time = 0.001s\n",
      "Epoch 312: loss = 1.1421, acc = 0.6841, time = 0.002s\n",
      "Epoch 313: loss = 1.1415, acc = 0.6838, time = 0.002s\n",
      "Epoch 314: loss = 1.1410, acc = 0.6848, time = 0.002s\n",
      "Epoch 315: loss = 1.1405, acc = 0.6855, time = 0.003s\n",
      "Epoch 316: loss = 1.1400, acc = 0.6853, time = 0.002s\n",
      "Epoch 317: loss = 1.1397, acc = 0.6869, time = 0.002s\n",
      "Epoch 318: loss = 1.1397, acc = 0.6848, time = 0.002s\n",
      "Epoch 319: loss = 1.1398, acc = 0.6856, time = 0.001s\n",
      "Epoch 320: loss = 1.1405, acc = 0.6847, time = 0.002s\n",
      "Epoch 321: loss = 1.1413, acc = 0.6849, time = 0.002s\n",
      "Epoch 322: loss = 1.1431, acc = 0.6784, time = 0.000s\n",
      "Epoch 323: loss = 1.1442, acc = 0.6835, time = 0.000s\n",
      "Epoch 324: loss = 1.1462, acc = 0.6774, time = 0.005s\n",
      "Epoch 325: loss = 1.1444, acc = 0.6830, time = 0.002s\n",
      "Epoch 326: loss = 1.1415, acc = 0.6842, time = 0.002s\n",
      "Epoch 327: loss = 1.1371, acc = 0.6829, time = 0.002s\n",
      "Epoch 328: loss = 1.1355, acc = 0.6861, time = 0.001s\n",
      "Epoch 329: loss = 1.1379, acc = 0.6796, time = 0.002s\n",
      "Epoch 330: loss = 1.1416, acc = 0.6857, time = 0.002s\n",
      "Epoch 331: loss = 1.1446, acc = 0.6744, time = 0.002s\n",
      "Epoch 332: loss = 1.1462, acc = 0.6827, time = 0.002s\n",
      "Epoch 333: loss = 1.1529, acc = 0.6649, time = 0.002s\n",
      "Epoch 334: loss = 1.1661, acc = 0.6674, time = 0.000s\n",
      "Epoch 335: loss = 1.1840, acc = 0.6458, time = 0.002s\n",
      "Epoch 336: loss = 1.1780, acc = 0.6566, time = 0.002s\n",
      "Epoch 337: loss = 1.1500, acc = 0.6689, time = 0.000s\n",
      "Epoch 338: loss = 1.1313, acc = 0.6861, time = 0.002s\n",
      "Epoch 339: loss = 1.1456, acc = 0.6803, time = 0.002s\n",
      "Epoch 340: loss = 1.1553, acc = 0.6597, time = 0.000s\n",
      "Epoch 341: loss = 1.1355, acc = 0.6840, time = 0.000s\n",
      "Epoch 342: loss = 1.1300, acc = 0.6870, time = 0.000s\n",
      "Epoch 343: loss = 1.1445, acc = 0.6692, time = 0.007s\n",
      "Epoch 344: loss = 1.1357, acc = 0.6837, time = 0.001s\n",
      "Epoch 345: loss = 1.1248, acc = 0.6880, time = 0.000s\n",
      "Epoch 346: loss = 1.1330, acc = 0.6795, time = 0.000s\n",
      "Epoch 347: loss = 1.1324, acc = 0.6839, time = 0.007s\n",
      "Epoch 348: loss = 1.1234, acc = 0.6891, time = 0.002s\n",
      "Epoch 349: loss = 1.1250, acc = 0.6857, time = 0.000s\n",
      "Epoch 350: loss = 1.1286, acc = 0.6861, time = 0.004s\n",
      "Epoch 351: loss = 1.1231, acc = 0.6879, time = 0.002s\n",
      "Epoch 352: loss = 1.1205, acc = 0.6897, time = 0.000s\n",
      "Epoch 353: loss = 1.1247, acc = 0.6887, time = 0.005s\n",
      "Epoch 354: loss = 1.1224, acc = 0.6875, time = 0.003s\n",
      "Epoch 355: loss = 1.1183, acc = 0.6897, time = 0.002s\n",
      "Epoch 356: loss = 1.1212, acc = 0.6906, time = 0.002s\n",
      "Epoch 357: loss = 1.1208, acc = 0.6888, time = 0.002s\n",
      "Epoch 358: loss = 1.1168, acc = 0.6928, time = 0.002s\n",
      "Epoch 359: loss = 1.1182, acc = 0.6922, time = 0.002s\n",
      "Epoch 360: loss = 1.1189, acc = 0.6906, time = 0.001s\n",
      "Epoch 361: loss = 1.1159, acc = 0.6938, time = 0.000s\n",
      "Epoch 362: loss = 1.1156, acc = 0.6922, time = 0.000s\n",
      "Epoch 363: loss = 1.1164, acc = 0.6904, time = 0.000s\n",
      "Epoch 364: loss = 1.1149, acc = 0.6930, time = 0.008s\n",
      "Epoch 365: loss = 1.1136, acc = 0.6919, time = 0.001s\n",
      "Epoch 366: loss = 1.1141, acc = 0.6921, time = 0.000s\n",
      "Epoch 367: loss = 1.1136, acc = 0.6940, time = 0.005s\n",
      "Epoch 368: loss = 1.1121, acc = 0.6915, time = 0.002s\n",
      "Epoch 369: loss = 1.1119, acc = 0.6919, time = 0.001s\n",
      "Epoch 370: loss = 1.1120, acc = 0.6926, time = 0.000s\n",
      "Epoch 371: loss = 1.1108, acc = 0.6921, time = 0.004s\n",
      "Epoch 372: loss = 1.1100, acc = 0.6931, time = 0.002s\n",
      "Epoch 373: loss = 1.1102, acc = 0.6941, time = 0.002s\n",
      "Epoch 374: loss = 1.1096, acc = 0.6921, time = 0.000s\n",
      "Epoch 375: loss = 1.1086, acc = 0.6952, time = 0.004s\n",
      "Epoch 376: loss = 1.1083, acc = 0.6945, time = 0.001s\n",
      "Epoch 377: loss = 1.1080, acc = 0.6934, time = 0.002s\n",
      "Epoch 378: loss = 1.1073, acc = 0.6955, time = 0.003s\n",
      "Epoch 379: loss = 1.1066, acc = 0.6943, time = 0.002s\n",
      "Epoch 380: loss = 1.1063, acc = 0.6942, time = 0.001s\n",
      "Epoch 381: loss = 1.1059, acc = 0.6960, time = 0.001s\n",
      "Epoch 382: loss = 1.1052, acc = 0.6948, time = 0.002s\n",
      "Epoch 383: loss = 1.1046, acc = 0.6949, time = 0.001s\n",
      "Epoch 384: loss = 1.1043, acc = 0.6948, time = 0.002s\n",
      "Epoch 385: loss = 1.1039, acc = 0.6946, time = 0.001s\n",
      "Epoch 386: loss = 1.1032, acc = 0.6953, time = 0.002s\n",
      "Epoch 387: loss = 1.1027, acc = 0.6970, time = 0.001s\n",
      "Epoch 388: loss = 1.1023, acc = 0.6954, time = 0.000s\n",
      "Epoch 389: loss = 1.1019, acc = 0.6956, time = 0.000s\n",
      "Epoch 390: loss = 1.1013, acc = 0.6953, time = 0.007s\n",
      "Epoch 391: loss = 1.1008, acc = 0.6952, time = 0.000s\n",
      "Epoch 392: loss = 1.1005, acc = 0.6956, time = 0.000s\n",
      "Epoch 393: loss = 1.1001, acc = 0.6953, time = 0.000s\n",
      "Epoch 394: loss = 1.0999, acc = 0.6969, time = 0.006s\n",
      "Epoch 395: loss = 1.1001, acc = 0.6949, time = 0.002s\n",
      "Epoch 396: loss = 1.1011, acc = 0.6946, time = 0.000s\n",
      "Epoch 397: loss = 1.1037, acc = 0.6946, time = 0.000s\n",
      "Epoch 398: loss = 1.1079, acc = 0.6932, time = 0.004s\n",
      "Epoch 399: loss = 1.1114, acc = 0.6898, time = 0.002s\n",
      "Epoch 400: loss = 1.1090, acc = 0.6922, time = 0.002s\n",
      "Epoch 401: loss = 1.1030, acc = 0.6926, time = 0.001s\n",
      "Epoch 402: loss = 1.1011, acc = 0.6968, time = 0.003s\n",
      "Epoch 403: loss = 1.1049, acc = 0.6934, time = 0.001s\n",
      "Epoch 404: loss = 1.1072, acc = 0.6932, time = 0.002s\n",
      "Epoch 405: loss = 1.1065, acc = 0.6860, time = 0.001s\n",
      "Epoch 406: loss = 1.1032, acc = 0.6941, time = 0.003s\n",
      "Epoch 407: loss = 1.1004, acc = 0.6918, time = 0.001s\n",
      "Epoch 408: loss = 1.0972, acc = 0.6962, time = 0.002s\n",
      "Epoch 409: loss = 1.0963, acc = 0.6952, time = 0.001s\n",
      "Epoch 410: loss = 1.0972, acc = 0.6968, time = 0.002s\n",
      "Epoch 411: loss = 1.0966, acc = 0.6981, time = 0.001s\n",
      "Epoch 412: loss = 1.0957, acc = 0.6937, time = 0.003s\n",
      "Epoch 413: loss = 1.0959, acc = 0.6984, time = 0.000s\n",
      "Epoch 414: loss = 1.0951, acc = 0.6952, time = 0.004s\n",
      "Epoch 415: loss = 1.0921, acc = 0.6976, time = 0.000s\n",
      "Epoch 416: loss = 1.0914, acc = 0.6972, time = 0.003s\n",
      "Epoch 417: loss = 1.0923, acc = 0.6958, time = 0.000s\n",
      "Epoch 418: loss = 1.0919, acc = 0.6983, time = 0.002s\n",
      "Epoch 419: loss = 1.0912, acc = 0.6967, time = 0.003s\n",
      "Epoch 420: loss = 1.0905, acc = 0.6976, time = 0.001s\n",
      "Epoch 421: loss = 1.0892, acc = 0.6995, time = 0.001s\n",
      "Epoch 422: loss = 1.0883, acc = 0.6973, time = 0.001s\n",
      "Epoch 423: loss = 1.0886, acc = 0.6991, time = 0.003s\n",
      "Epoch 424: loss = 1.0883, acc = 0.6989, time = 0.002s\n",
      "Epoch 425: loss = 1.0873, acc = 0.6996, time = 0.002s\n",
      "Epoch 426: loss = 1.0873, acc = 0.6982, time = 0.002s\n",
      "Epoch 427: loss = 1.0872, acc = 0.7000, time = 0.002s\n",
      "Epoch 428: loss = 1.0860, acc = 0.6981, time = 0.001s\n",
      "Epoch 429: loss = 1.0851, acc = 0.6984, time = 0.003s\n",
      "Epoch 430: loss = 1.0851, acc = 0.6996, time = 0.002s\n",
      "Epoch 431: loss = 1.0849, acc = 0.6985, time = 0.002s\n",
      "Epoch 432: loss = 1.0843, acc = 0.7000, time = 0.001s\n",
      "Epoch 433: loss = 1.0840, acc = 0.6994, time = 0.001s\n",
      "Epoch 434: loss = 1.0838, acc = 0.7003, time = 0.002s\n",
      "Epoch 435: loss = 1.0831, acc = 0.6995, time = 0.001s\n",
      "Epoch 436: loss = 1.0823, acc = 0.7010, time = 0.002s\n",
      "Epoch 437: loss = 1.0821, acc = 0.6988, time = 0.001s\n",
      "Epoch 438: loss = 1.0819, acc = 0.6992, time = 0.001s\n",
      "Epoch 439: loss = 1.0813, acc = 0.7002, time = 0.000s\n",
      "Epoch 440: loss = 1.0808, acc = 0.6994, time = 0.000s\n",
      "Epoch 441: loss = 1.0806, acc = 0.7019, time = 0.006s\n",
      "Epoch 442: loss = 1.0803, acc = 0.6991, time = 0.002s\n",
      "Epoch 443: loss = 1.0797, acc = 0.7019, time = 0.001s\n",
      "Epoch 444: loss = 1.0793, acc = 0.6998, time = 0.000s\n",
      "Epoch 445: loss = 1.0789, acc = 0.7019, time = 0.005s\n",
      "Epoch 446: loss = 1.0785, acc = 0.7004, time = 0.001s\n",
      "Epoch 447: loss = 1.0780, acc = 0.7017, time = 0.001s\n",
      "Epoch 448: loss = 1.0775, acc = 0.7011, time = 0.000s\n",
      "Epoch 449: loss = 1.0772, acc = 0.7008, time = 0.004s\n",
      "Epoch 450: loss = 1.0768, acc = 0.7012, time = 0.001s\n",
      "Epoch 451: loss = 1.0764, acc = 0.7013, time = 0.001s\n",
      "Epoch 452: loss = 1.0760, acc = 0.7009, time = 0.008s\n",
      "Epoch 453: loss = 1.0758, acc = 0.7025, time = 0.003s\n",
      "Epoch 454: loss = 1.0755, acc = 0.7030, time = 0.003s\n",
      "Epoch 455: loss = 1.0754, acc = 0.7032, time = 0.002s\n",
      "Epoch 456: loss = 1.0755, acc = 0.7020, time = 0.003s\n",
      "Epoch 457: loss = 1.0763, acc = 0.7049, time = 0.003s\n",
      "Epoch 458: loss = 1.0780, acc = 0.7008, time = 0.002s\n",
      "Epoch 459: loss = 1.0811, acc = 0.7026, time = 0.003s\n",
      "Epoch 460: loss = 1.0834, acc = 0.6996, time = 0.004s\n",
      "Epoch 461: loss = 1.0828, acc = 0.7005, time = 0.003s\n",
      "Epoch 462: loss = 1.0772, acc = 0.7024, time = 0.001s\n",
      "Epoch 463: loss = 1.0734, acc = 0.7006, time = 0.003s\n",
      "Epoch 464: loss = 1.0747, acc = 0.7071, time = 0.002s\n",
      "Epoch 465: loss = 1.0777, acc = 0.6987, time = 0.001s\n",
      "Epoch 466: loss = 1.0774, acc = 0.7044, time = 0.002s\n",
      "Epoch 467: loss = 1.0744, acc = 0.7024, time = 0.002s\n",
      "Epoch 468: loss = 1.0736, acc = 0.7023, time = 0.001s\n",
      "Epoch 469: loss = 1.0756, acc = 0.7037, time = 0.000s\n",
      "Epoch 470: loss = 1.0752, acc = 0.7030, time = 0.000s\n",
      "Epoch 471: loss = 1.0727, acc = 0.7014, time = 0.007s\n",
      "Epoch 472: loss = 1.0712, acc = 0.7060, time = 0.000s\n",
      "Epoch 473: loss = 1.0716, acc = 0.7015, time = 0.000s\n",
      "Epoch 474: loss = 1.0710, acc = 0.7072, time = 0.000s\n",
      "Epoch 475: loss = 1.0692, acc = 0.7040, time = 0.006s\n",
      "Epoch 476: loss = 1.0684, acc = 0.7042, time = 0.001s\n",
      "Epoch 477: loss = 1.0687, acc = 0.7066, time = 0.000s\n",
      "Epoch 478: loss = 1.0684, acc = 0.7032, time = 0.000s\n",
      "Epoch 479: loss = 1.0674, acc = 0.7058, time = 0.007s\n",
      "Epoch 480: loss = 1.0671, acc = 0.7044, time = 0.002s\n",
      "Epoch 481: loss = 1.0671, acc = 0.7043, time = 0.001s\n",
      "Epoch 482: loss = 1.0670, acc = 0.7053, time = 0.001s\n",
      "Epoch 483: loss = 1.0664, acc = 0.7071, time = 0.001s\n",
      "Epoch 484: loss = 1.0661, acc = 0.7020, time = 0.002s\n",
      "Epoch 485: loss = 1.0659, acc = 0.7090, time = 0.001s\n",
      "Epoch 486: loss = 1.0655, acc = 0.7046, time = 0.002s\n",
      "Epoch 487: loss = 1.0649, acc = 0.7063, time = 0.001s\n",
      "Epoch 488: loss = 1.0645, acc = 0.7062, time = 0.000s\n",
      "Epoch 489: loss = 1.0641, acc = 0.7054, time = 0.000s\n",
      "Epoch 490: loss = 1.0636, acc = 0.7056, time = 0.005s\n",
      "Epoch 491: loss = 1.0632, acc = 0.7062, time = 0.005s\n",
      "Epoch 492: loss = 1.0630, acc = 0.7050, time = 0.002s\n",
      "Epoch 493: loss = 1.0629, acc = 0.7080, time = 0.002s\n",
      "Epoch 494: loss = 1.0628, acc = 0.7054, time = 0.002s\n",
      "Epoch 495: loss = 1.0629, acc = 0.7062, time = 0.001s\n",
      "Epoch 496: loss = 1.0635, acc = 0.7044, time = 0.003s\n",
      "Epoch 497: loss = 1.0647, acc = 0.7061, time = 0.002s\n",
      "Epoch 498: loss = 1.0661, acc = 0.7039, time = 0.003s\n",
      "Epoch 499: loss = 1.0676, acc = 0.7056, time = 0.003s\n",
      "Epoch 500: loss = 1.0685, acc = 0.7011, time = 0.002s\n",
      "Epoch 501: loss = 1.0679, acc = 0.7049, time = 0.003s\n",
      "Epoch 502: loss = 1.0657, acc = 0.7004, time = 0.003s\n",
      "Epoch 503: loss = 1.0632, acc = 0.7057, time = 0.003s\n",
      "Epoch 504: loss = 1.0623, acc = 0.7042, time = 0.003s\n",
      "Epoch 505: loss = 1.0634, acc = 0.7048, time = 0.002s\n",
      "Epoch 506: loss = 1.0653, acc = 0.6976, time = 0.004s\n",
      "Epoch 507: loss = 1.0660, acc = 0.7032, time = 0.002s\n",
      "Epoch 508: loss = 1.0647, acc = 0.7029, time = 0.003s\n",
      "Epoch 509: loss = 1.0622, acc = 0.7066, time = 0.003s\n",
      "Epoch 510: loss = 1.0609, acc = 0.7046, time = 0.002s\n",
      "Epoch 511: loss = 1.0608, acc = 0.7088, time = 0.004s\n",
      "Epoch 512: loss = 1.0617, acc = 0.7041, time = 0.003s\n",
      "Epoch 513: loss = 1.0610, acc = 0.7075, time = 0.003s\n",
      "Epoch 514: loss = 1.0591, acc = 0.7065, time = 0.000s\n",
      "Epoch 515: loss = 1.0563, acc = 0.7084, time = 0.006s\n",
      "Epoch 516: loss = 1.0548, acc = 0.7092, time = 0.003s\n",
      "Epoch 517: loss = 1.0553, acc = 0.7076, time = 0.000s\n",
      "Epoch 518: loss = 1.0565, acc = 0.7080, time = 0.003s\n",
      "Epoch 519: loss = 1.0571, acc = 0.7077, time = 0.001s\n",
      "Epoch 520: loss = 1.0564, acc = 0.7094, time = 0.001s\n",
      "Epoch 521: loss = 1.0555, acc = 0.7064, time = 0.002s\n",
      "Epoch 522: loss = 1.0546, acc = 0.7113, time = 0.000s\n",
      "Epoch 523: loss = 1.0543, acc = 0.7066, time = 0.003s\n",
      "Epoch 524: loss = 1.0539, acc = 0.7090, time = 0.002s\n",
      "Epoch 525: loss = 1.0532, acc = 0.7082, time = 0.000s\n",
      "Epoch 526: loss = 1.0523, acc = 0.7091, time = 0.000s\n",
      "Epoch 527: loss = 1.0516, acc = 0.7109, time = 0.004s\n",
      "Epoch 528: loss = 1.0513, acc = 0.7097, time = 0.001s\n",
      "Epoch 529: loss = 1.0514, acc = 0.7109, time = 0.001s\n",
      "Epoch 530: loss = 1.0515, acc = 0.7075, time = 0.002s\n",
      "Epoch 531: loss = 1.0513, acc = 0.7107, time = 0.001s\n",
      "Epoch 532: loss = 1.0509, acc = 0.7082, time = 0.001s\n",
      "Epoch 533: loss = 1.0503, acc = 0.7101, time = 0.002s\n",
      "Epoch 534: loss = 1.0498, acc = 0.7095, time = 0.002s\n",
      "Epoch 535: loss = 1.0494, acc = 0.7100, time = 0.002s\n",
      "Epoch 536: loss = 1.0492, acc = 0.7091, time = 0.002s\n",
      "Epoch 537: loss = 1.0490, acc = 0.7105, time = 0.001s\n",
      "Epoch 538: loss = 1.0488, acc = 0.7092, time = 0.002s\n",
      "Epoch 539: loss = 1.0486, acc = 0.7104, time = 0.002s\n",
      "Epoch 540: loss = 1.0484, acc = 0.7100, time = 0.002s\n",
      "Epoch 541: loss = 1.0484, acc = 0.7113, time = 0.002s\n",
      "Epoch 542: loss = 1.0485, acc = 0.7088, time = 0.001s\n",
      "Epoch 543: loss = 1.0488, acc = 0.7120, time = 0.000s\n",
      "Epoch 544: loss = 1.0493, acc = 0.7066, time = 0.005s\n",
      "Epoch 545: loss = 1.0499, acc = 0.7114, time = 0.002s\n",
      "Epoch 546: loss = 1.0509, acc = 0.7054, time = 0.000s\n",
      "Epoch 547: loss = 1.0520, acc = 0.7109, time = 0.003s\n",
      "Epoch 548: loss = 1.0539, acc = 0.7028, time = 0.001s\n",
      "Epoch 549: loss = 1.0554, acc = 0.7072, time = 0.002s\n",
      "Epoch 550: loss = 1.0577, acc = 0.6967, time = 0.000s\n",
      "Epoch 551: loss = 1.0584, acc = 0.7055, time = 0.000s\n",
      "Epoch 552: loss = 1.0590, acc = 0.6957, time = 0.004s\n",
      "Epoch 553: loss = 1.0566, acc = 0.7086, time = 0.001s\n",
      "Epoch 554: loss = 1.0533, acc = 0.7030, time = 0.000s\n",
      "Epoch 555: loss = 1.0491, acc = 0.7119, time = 0.000s\n",
      "Epoch 556: loss = 1.0472, acc = 0.7082, time = 0.000s\n",
      "Epoch 557: loss = 1.0466, acc = 0.7101, time = 0.006s\n",
      "Epoch 558: loss = 1.0470, acc = 0.7122, time = 0.000s\n",
      "Epoch 559: loss = 1.0472, acc = 0.7062, time = 0.000s\n",
      "Epoch 560: loss = 1.0464, acc = 0.7133, time = 0.000s\n",
      "Epoch 561: loss = 1.0456, acc = 0.7090, time = 0.000s\n",
      "Epoch 562: loss = 1.0448, acc = 0.7129, time = 0.007s\n",
      "Epoch 563: loss = 1.0446, acc = 0.7120, time = 0.001s\n",
      "Epoch 564: loss = 1.0437, acc = 0.7110, time = 0.001s\n",
      "Epoch 565: loss = 1.0425, acc = 0.7120, time = 0.001s\n",
      "Epoch 566: loss = 1.0414, acc = 0.7096, time = 0.001s\n",
      "Epoch 567: loss = 1.0411, acc = 0.7120, time = 0.001s\n",
      "Epoch 568: loss = 1.0414, acc = 0.7125, time = 0.002s\n",
      "Epoch 569: loss = 1.0415, acc = 0.7133, time = 0.001s\n",
      "Epoch 570: loss = 1.0413, acc = 0.7125, time = 0.001s\n",
      "Epoch 571: loss = 1.0401, acc = 0.7129, time = 0.000s\n",
      "Epoch 572: loss = 1.0389, acc = 0.7132, time = 0.003s\n",
      "Epoch 573: loss = 1.0383, acc = 0.7125, time = 0.001s\n",
      "Epoch 574: loss = 1.0383, acc = 0.7147, time = 0.001s\n",
      "Epoch 575: loss = 1.0387, acc = 0.7128, time = 0.000s\n",
      "Epoch 576: loss = 1.0388, acc = 0.7141, time = 0.003s\n",
      "Epoch 577: loss = 1.0384, acc = 0.7134, time = 0.001s\n",
      "Epoch 578: loss = 1.0375, acc = 0.7136, time = 0.001s\n",
      "Epoch 579: loss = 1.0366, acc = 0.7134, time = 0.000s\n",
      "Epoch 580: loss = 1.0361, acc = 0.7137, time = 0.000s\n",
      "Epoch 581: loss = 1.0360, acc = 0.7148, time = 0.006s\n",
      "Epoch 582: loss = 1.0362, acc = 0.7126, time = 0.000s\n",
      "Epoch 583: loss = 1.0362, acc = 0.7139, time = 0.000s\n",
      "Epoch 584: loss = 1.0361, acc = 0.7150, time = 0.000s\n",
      "Epoch 585: loss = 1.0357, acc = 0.7141, time = 0.000s\n",
      "Epoch 586: loss = 1.0354, acc = 0.7165, time = 0.007s\n",
      "Epoch 587: loss = 1.0356, acc = 0.7138, time = 0.001s\n",
      "Epoch 588: loss = 1.0367, acc = 0.7149, time = 0.000s\n",
      "Epoch 589: loss = 1.0399, acc = 0.7112, time = 0.000s\n",
      "Epoch 590: loss = 1.0463, acc = 0.7117, time = 0.005s\n",
      "Epoch 591: loss = 1.0546, acc = 0.7052, time = 0.001s\n",
      "Epoch 592: loss = 1.0519, acc = 0.7093, time = 0.001s\n",
      "Epoch 593: loss = 1.0393, acc = 0.7103, time = 0.001s\n",
      "Epoch 594: loss = 1.0362, acc = 0.7143, time = 0.001s\n",
      "Epoch 595: loss = 1.0446, acc = 0.7124, time = 0.003s\n",
      "Epoch 596: loss = 1.0459, acc = 0.7101, time = 0.001s\n",
      "Epoch 597: loss = 1.0366, acc = 0.7136, time = 0.001s\n",
      "Epoch 598: loss = 1.0363, acc = 0.7147, time = 0.002s\n",
      "Epoch 599: loss = 1.0401, acc = 0.7113, time = 0.000s\n",
      "Epoch 600: loss = 1.0358, acc = 0.7163, time = 0.004s\n",
      "Epoch 601: loss = 1.0332, acc = 0.7136, time = 0.001s\n",
      "Epoch 602: loss = 1.0358, acc = 0.7120, time = 0.001s\n",
      "Epoch 603: loss = 1.0347, acc = 0.7152, time = 0.000s\n",
      "Epoch 604: loss = 1.0329, acc = 0.7132, time = 0.000s\n",
      "Epoch 605: loss = 1.0332, acc = 0.7156, time = 0.004s\n",
      "Epoch 606: loss = 1.0332, acc = 0.7150, time = 0.000s\n",
      "Epoch 607: loss = 1.0324, acc = 0.7115, time = 0.000s\n",
      "Epoch 608: loss = 1.0317, acc = 0.7139, time = 0.000s\n",
      "Epoch 609: loss = 1.0314, acc = 0.7171, time = 0.000s\n",
      "Epoch 610: loss = 1.0315, acc = 0.7129, time = 0.006s\n",
      "Epoch 611: loss = 1.0302, acc = 0.7155, time = 0.000s\n",
      "Epoch 612: loss = 1.0300, acc = 0.7161, time = 0.000s\n",
      "Epoch 613: loss = 1.0303, acc = 0.7145, time = 0.007s\n",
      "Epoch 614: loss = 1.0290, acc = 0.7154, time = 0.002s\n",
      "Epoch 615: loss = 1.0287, acc = 0.7190, time = 0.002s\n",
      "Epoch 616: loss = 1.0291, acc = 0.7133, time = 0.000s\n",
      "Epoch 617: loss = 1.0278, acc = 0.7168, time = 0.004s\n",
      "Epoch 618: loss = 1.0273, acc = 0.7170, time = 0.002s\n",
      "Epoch 619: loss = 1.0279, acc = 0.7147, time = 0.000s\n",
      "Epoch 620: loss = 1.0269, acc = 0.7164, time = 0.005s\n",
      "Epoch 621: loss = 1.0262, acc = 0.7174, time = 0.001s\n",
      "Epoch 622: loss = 1.0267, acc = 0.7142, time = 0.000s\n",
      "Epoch 623: loss = 1.0261, acc = 0.7164, time = 0.005s\n",
      "Epoch 624: loss = 1.0254, acc = 0.7161, time = 0.000s\n",
      "Epoch 625: loss = 1.0257, acc = 0.7162, time = 0.000s\n",
      "Epoch 626: loss = 1.0255, acc = 0.7165, time = 0.000s\n",
      "Epoch 627: loss = 1.0251, acc = 0.7168, time = 0.007s\n",
      "Epoch 628: loss = 1.0253, acc = 0.7159, time = 0.000s\n",
      "Epoch 629: loss = 1.0257, acc = 0.7171, time = 0.000s\n",
      "Epoch 630: loss = 1.0263, acc = 0.7146, time = 0.007s\n",
      "Epoch 631: loss = 1.0278, acc = 0.7161, time = 0.002s\n",
      "Epoch 632: loss = 1.0303, acc = 0.7095, time = 0.001s\n",
      "Epoch 633: loss = 1.0335, acc = 0.7143, time = 0.002s\n",
      "Epoch 634: loss = 1.0365, acc = 0.7056, time = 0.002s\n",
      "Epoch 635: loss = 1.0373, acc = 0.7118, time = 0.004s\n",
      "Epoch 636: loss = 1.0377, acc = 0.7016, time = 0.001s\n",
      "Epoch 637: loss = 1.0387, acc = 0.7090, time = 0.002s\n",
      "Epoch 638: loss = 1.0448, acc = 0.7000, time = 0.002s\n",
      "Epoch 639: loss = 1.0511, acc = 0.6988, time = 0.001s\n",
      "Epoch 640: loss = 1.0560, acc = 0.6937, time = 0.002s\n",
      "Epoch 641: loss = 1.0522, acc = 0.6999, time = 0.001s\n",
      "Epoch 642: loss = 1.0440, acc = 0.7000, time = 0.001s\n",
      "Epoch 643: loss = 1.0325, acc = 0.7098, time = 0.001s\n",
      "Epoch 644: loss = 1.0285, acc = 0.7134, time = 0.001s\n",
      "Epoch 645: loss = 1.0328, acc = 0.7099, time = 0.001s\n",
      "Epoch 646: loss = 1.0344, acc = 0.7096, time = 0.002s\n",
      "Epoch 647: loss = 1.0294, acc = 0.7097, time = 0.000s\n",
      "Epoch 648: loss = 1.0245, acc = 0.7155, time = 0.000s\n",
      "Epoch 649: loss = 1.0258, acc = 0.7152, time = 0.000s\n",
      "Epoch 650: loss = 1.0289, acc = 0.7073, time = 0.001s\n",
      "Epoch 651: loss = 1.0266, acc = 0.7152, time = 0.000s\n",
      "Epoch 652: loss = 1.0222, acc = 0.7168, time = 0.000s\n",
      "Epoch 653: loss = 1.0226, acc = 0.7142, time = 0.006s\n",
      "Epoch 654: loss = 1.0242, acc = 0.7154, time = 0.002s\n",
      "Epoch 655: loss = 1.0227, acc = 0.7106, time = 0.000s\n",
      "Epoch 656: loss = 1.0203, acc = 0.7193, time = 0.000s\n",
      "Epoch 657: loss = 1.0208, acc = 0.7185, time = 0.006s\n",
      "Epoch 658: loss = 1.0214, acc = 0.7136, time = 0.001s\n",
      "Epoch 659: loss = 1.0200, acc = 0.7187, time = 0.002s\n",
      "Epoch 660: loss = 1.0194, acc = 0.7179, time = 0.001s\n",
      "Epoch 661: loss = 1.0196, acc = 0.7178, time = 0.002s\n",
      "Epoch 662: loss = 1.0186, acc = 0.7181, time = 0.001s\n",
      "Epoch 663: loss = 1.0177, acc = 0.7187, time = 0.000s\n",
      "Epoch 664: loss = 1.0181, acc = 0.7199, time = 0.000s\n",
      "Epoch 665: loss = 1.0180, acc = 0.7182, time = 0.005s\n",
      "Epoch 666: loss = 1.0168, acc = 0.7190, time = 0.001s\n",
      "Epoch 667: loss = 1.0166, acc = 0.7200, time = 0.000s\n",
      "Epoch 668: loss = 1.0170, acc = 0.7186, time = 0.000s\n",
      "Epoch 669: loss = 1.0164, acc = 0.7207, time = 0.006s\n",
      "Epoch 670: loss = 1.0155, acc = 0.7183, time = 0.002s\n",
      "Epoch 671: loss = 1.0154, acc = 0.7185, time = 0.002s\n",
      "Epoch 672: loss = 1.0155, acc = 0.7198, time = 0.002s\n",
      "Epoch 673: loss = 1.0151, acc = 0.7182, time = 0.001s\n",
      "Epoch 674: loss = 1.0144, acc = 0.7207, time = 0.000s\n",
      "Epoch 675: loss = 1.0143, acc = 0.7198, time = 0.000s\n",
      "Epoch 676: loss = 1.0143, acc = 0.7186, time = 0.006s\n",
      "Epoch 677: loss = 1.0140, acc = 0.7204, time = 0.001s\n",
      "Epoch 678: loss = 1.0134, acc = 0.7188, time = 0.000s\n",
      "Epoch 679: loss = 1.0132, acc = 0.7200, time = 0.000s\n",
      "Epoch 680: loss = 1.0132, acc = 0.7198, time = 0.006s\n",
      "Epoch 681: loss = 1.0130, acc = 0.7194, time = 0.001s\n",
      "Epoch 682: loss = 1.0124, acc = 0.7210, time = 0.002s\n",
      "Epoch 683: loss = 1.0121, acc = 0.7201, time = 0.001s\n",
      "Epoch 684: loss = 1.0121, acc = 0.7197, time = 0.002s\n",
      "Epoch 685: loss = 1.0120, acc = 0.7212, time = 0.003s\n",
      "Epoch 686: loss = 1.0116, acc = 0.7203, time = 0.002s\n",
      "Epoch 687: loss = 1.0112, acc = 0.7201, time = 0.001s\n",
      "Epoch 688: loss = 1.0111, acc = 0.7210, time = 0.001s\n",
      "Epoch 689: loss = 1.0111, acc = 0.7212, time = 0.001s\n",
      "Epoch 690: loss = 1.0111, acc = 0.7207, time = 0.001s\n",
      "Epoch 691: loss = 1.0112, acc = 0.7219, time = 0.002s\n",
      "Epoch 692: loss = 1.0118, acc = 0.7208, time = 0.001s\n",
      "Epoch 693: loss = 1.0129, acc = 0.7224, time = 0.001s\n",
      "Epoch 694: loss = 1.0150, acc = 0.7196, time = 0.002s\n",
      "Epoch 695: loss = 1.0172, acc = 0.7218, time = 0.001s\n",
      "Epoch 696: loss = 1.0192, acc = 0.7172, time = 0.000s\n",
      "Epoch 697: loss = 1.0174, acc = 0.7202, time = 0.004s\n",
      "Epoch 698: loss = 1.0138, acc = 0.7182, time = 0.002s\n",
      "Epoch 699: loss = 1.0105, acc = 0.7206, time = 0.001s\n",
      "Epoch 700: loss = 1.0106, acc = 0.7165, time = 0.001s\n",
      "Epoch 701: loss = 1.0127, acc = 0.7201, time = 0.003s\n",
      "Epoch 702: loss = 1.0138, acc = 0.7179, time = 0.001s\n",
      "Epoch 703: loss = 1.0117, acc = 0.7196, time = 0.001s\n",
      "Epoch 704: loss = 1.0094, acc = 0.7217, time = 0.000s\n",
      "Epoch 705: loss = 1.0094, acc = 0.7207, time = 0.004s\n",
      "Epoch 706: loss = 1.0110, acc = 0.7205, time = 0.001s\n",
      "Epoch 707: loss = 1.0110, acc = 0.7202, time = 0.002s\n",
      "Epoch 708: loss = 1.0096, acc = 0.7201, time = 0.000s\n",
      "Epoch 709: loss = 1.0086, acc = 0.7220, time = 0.004s\n",
      "Epoch 710: loss = 1.0091, acc = 0.7219, time = 0.001s\n",
      "Epoch 711: loss = 1.0094, acc = 0.7222, time = 0.001s\n",
      "Epoch 712: loss = 1.0085, acc = 0.7180, time = 0.001s\n",
      "Epoch 713: loss = 1.0071, acc = 0.7211, time = 0.002s\n",
      "Epoch 714: loss = 1.0068, acc = 0.7169, time = 0.002s\n",
      "Epoch 715: loss = 1.0069, acc = 0.7218, time = 0.002s\n",
      "Epoch 716: loss = 1.0067, acc = 0.7219, time = 0.001s\n",
      "Epoch 717: loss = 1.0056, acc = 0.7216, time = 0.002s\n",
      "Epoch 718: loss = 1.0047, acc = 0.7219, time = 0.002s\n",
      "Epoch 719: loss = 1.0044, acc = 0.7226, time = 0.000s\n",
      "Epoch 720: loss = 1.0044, acc = 0.7222, time = 0.000s\n",
      "Epoch 721: loss = 1.0042, acc = 0.7220, time = 0.000s\n",
      "Epoch 722: loss = 1.0039, acc = 0.7225, time = 0.000s\n",
      "Epoch 723: loss = 1.0036, acc = 0.7218, time = 0.000s\n",
      "Epoch 724: loss = 1.0033, acc = 0.7226, time = 0.000s\n",
      "Epoch 725: loss = 1.0031, acc = 0.7228, time = 0.000s\n",
      "Epoch 726: loss = 1.0028, acc = 0.7229, time = 0.000s\n",
      "Epoch 727: loss = 1.0026, acc = 0.7234, time = 0.014s\n",
      "Epoch 728: loss = 1.0024, acc = 0.7224, time = 0.000s\n",
      "Epoch 729: loss = 1.0022, acc = 0.7228, time = 0.000s\n",
      "Epoch 730: loss = 1.0021, acc = 0.7243, time = 0.000s\n",
      "Epoch 731: loss = 1.0022, acc = 0.7220, time = 0.000s\n",
      "Epoch 732: loss = 1.0024, acc = 0.7246, time = 0.000s\n",
      "Epoch 733: loss = 1.0028, acc = 0.7217, time = 0.000s\n",
      "Epoch 734: loss = 1.0032, acc = 0.7249, time = 0.000s\n",
      "Epoch 735: loss = 1.0044, acc = 0.7177, time = 0.000s\n",
      "Epoch 736: loss = 1.0057, acc = 0.7236, time = 0.016s\n",
      "Epoch 737: loss = 1.0081, acc = 0.7171, time = 0.000s\n",
      "Epoch 738: loss = 1.0089, acc = 0.7217, time = 0.000s\n",
      "Epoch 739: loss = 1.0099, acc = 0.7134, time = 0.000s\n",
      "Epoch 740: loss = 1.0081, acc = 0.7209, time = 0.000s\n",
      "Epoch 741: loss = 1.0064, acc = 0.7192, time = 0.000s\n",
      "Epoch 742: loss = 1.0038, acc = 0.7215, time = 0.000s\n",
      "Epoch 743: loss = 1.0024, acc = 0.7226, time = 0.000s\n",
      "Epoch 744: loss = 1.0025, acc = 0.7213, time = 0.000s\n",
      "Epoch 745: loss = 1.0036, acc = 0.7231, time = 0.000s\n",
      "Epoch 746: loss = 1.0050, acc = 0.7160, time = 0.000s\n",
      "Epoch 747: loss = 1.0037, acc = 0.7235, time = 0.016s\n",
      "Epoch 748: loss = 1.0014, acc = 0.7232, time = 0.000s\n",
      "Epoch 749: loss = 0.9986, acc = 0.7248, time = 0.003s\n",
      "Epoch 750: loss = 0.9975, acc = 0.7250, time = 0.002s\n",
      "Epoch 751: loss = 0.9981, acc = 0.7239, time = 0.002s\n",
      "Epoch 752: loss = 0.9993, acc = 0.7234, time = 0.000s\n",
      "Epoch 753: loss = 1.0004, acc = 0.7194, time = 0.000s\n",
      "Epoch 754: loss = 0.9998, acc = 0.7230, time = 0.000s\n",
      "Epoch 755: loss = 0.9987, acc = 0.7206, time = 0.000s\n",
      "Epoch 756: loss = 0.9976, acc = 0.7240, time = 0.000s\n",
      "Epoch 757: loss = 0.9977, acc = 0.7242, time = 0.000s\n",
      "Epoch 758: loss = 0.9989, acc = 0.7241, time = 0.000s\n",
      "Epoch 759: loss = 1.0000, acc = 0.7213, time = 0.000s\n",
      "Epoch 760: loss = 1.0004, acc = 0.7237, time = 0.000s\n",
      "Epoch 761: loss = 0.9997, acc = 0.7221, time = 0.016s\n",
      "Epoch 762: loss = 0.9986, acc = 0.7236, time = 0.000s\n",
      "Epoch 763: loss = 0.9975, acc = 0.7239, time = 0.000s\n",
      "Epoch 764: loss = 0.9969, acc = 0.7245, time = 0.000s\n",
      "Epoch 765: loss = 0.9963, acc = 0.7187, time = 0.000s\n",
      "Epoch 766: loss = 0.9956, acc = 0.7254, time = 0.000s\n",
      "Epoch 767: loss = 0.9950, acc = 0.7203, time = 0.000s\n",
      "Epoch 768: loss = 0.9942, acc = 0.7253, time = 0.000s\n",
      "Epoch 769: loss = 0.9937, acc = 0.7246, time = 0.000s\n",
      "Epoch 770: loss = 0.9933, acc = 0.7263, time = 0.016s\n",
      "Epoch 771: loss = 0.9933, acc = 0.7255, time = 0.000s\n",
      "Epoch 772: loss = 0.9936, acc = 0.7254, time = 0.002s\n",
      "Epoch 773: loss = 0.9938, acc = 0.7259, time = 0.002s\n",
      "Epoch 774: loss = 0.9939, acc = 0.7206, time = 0.000s\n",
      "Epoch 775: loss = 0.9933, acc = 0.7260, time = 0.000s\n",
      "Epoch 776: loss = 0.9926, acc = 0.7244, time = 0.000s\n",
      "Epoch 777: loss = 0.9917, acc = 0.7259, time = 0.000s\n",
      "Epoch 778: loss = 0.9911, acc = 0.7266, time = 0.000s\n",
      "Epoch 779: loss = 0.9909, acc = 0.7251, time = 0.000s\n",
      "Epoch 780: loss = 0.9908, acc = 0.7264, time = 0.000s\n",
      "Epoch 781: loss = 0.9910, acc = 0.7256, time = 0.012s\n",
      "Epoch 782: loss = 0.9911, acc = 0.7258, time = 0.000s\n",
      "Epoch 783: loss = 0.9913, acc = 0.7261, time = 0.000s\n",
      "Epoch 784: loss = 0.9914, acc = 0.7267, time = 0.000s\n",
      "Epoch 785: loss = 0.9919, acc = 0.7271, time = 0.000s\n",
      "Epoch 786: loss = 0.9925, acc = 0.7252, time = 0.000s\n",
      "Epoch 787: loss = 0.9935, acc = 0.7257, time = 0.000s\n",
      "Epoch 788: loss = 0.9944, acc = 0.7241, time = 0.000s\n",
      "Epoch 789: loss = 0.9953, acc = 0.7224, time = 0.000s\n",
      "Epoch 790: loss = 0.9960, acc = 0.7227, time = 0.000s\n",
      "Epoch 791: loss = 0.9967, acc = 0.7206, time = 0.016s\n",
      "Epoch 792: loss = 0.9988, acc = 0.7161, time = 0.000s\n",
      "Epoch 793: loss = 1.0015, acc = 0.7194, time = 0.003s\n",
      "Epoch 794: loss = 1.0056, acc = 0.7097, time = 0.002s\n",
      "Epoch 795: loss = 1.0089, acc = 0.7154, time = 0.001s\n",
      "Epoch 796: loss = 1.0113, acc = 0.7051, time = 0.001s\n",
      "Epoch 797: loss = 1.0098, acc = 0.7147, time = 0.001s\n",
      "Epoch 798: loss = 1.0046, acc = 0.7146, time = 0.002s\n",
      "Epoch 799: loss = 0.9959, acc = 0.7241, time = 0.000s\n",
      "Epoch 800: loss = 0.9914, acc = 0.7251, time = 0.000s\n",
      "Epoch 801: loss = 0.9926, acc = 0.7235, time = 0.000s\n",
      "Epoch 802: loss = 0.9956, acc = 0.7221, time = 0.000s\n",
      "Epoch 803: loss = 0.9956, acc = 0.7222, time = 0.000s\n",
      "Epoch 804: loss = 0.9923, acc = 0.7251, time = 0.000s\n",
      "Epoch 805: loss = 0.9896, acc = 0.7268, time = 0.000s\n",
      "Epoch 806: loss = 0.9896, acc = 0.7268, time = 0.000s\n",
      "Epoch 807: loss = 0.9911, acc = 0.7221, time = 0.000s\n",
      "Epoch 808: loss = 0.9902, acc = 0.7262, time = 0.015s\n",
      "Epoch 809: loss = 0.9879, acc = 0.7277, time = 0.000s\n",
      "Epoch 810: loss = 0.9869, acc = 0.7258, time = 0.004s\n",
      "Epoch 811: loss = 0.9878, acc = 0.7270, time = 0.002s\n",
      "Epoch 812: loss = 0.9887, acc = 0.7234, time = 0.001s\n",
      "Epoch 813: loss = 0.9870, acc = 0.7282, time = 0.002s\n",
      "Epoch 814: loss = 0.9856, acc = 0.7269, time = 0.004s\n",
      "Epoch 815: loss = 0.9858, acc = 0.7258, time = 0.002s\n",
      "Epoch 816: loss = 0.9863, acc = 0.7271, time = 0.002s\n",
      "Epoch 817: loss = 0.9857, acc = 0.7260, time = 0.003s\n",
      "Epoch 818: loss = 0.9846, acc = 0.7285, time = 0.002s\n",
      "Epoch 819: loss = 0.9846, acc = 0.7273, time = 0.003s\n",
      "Epoch 820: loss = 0.9848, acc = 0.7264, time = 0.002s\n",
      "Epoch 821: loss = 0.9843, acc = 0.7274, time = 0.000s\n",
      "Epoch 822: loss = 0.9837, acc = 0.7273, time = 0.003s\n",
      "Epoch 823: loss = 0.9837, acc = 0.7284, time = 0.002s\n",
      "Epoch 824: loss = 0.9836, acc = 0.7277, time = 0.000s\n",
      "Epoch 825: loss = 0.9829, acc = 0.7267, time = 0.003s\n",
      "Epoch 826: loss = 0.9824, acc = 0.7278, time = 0.002s\n",
      "Epoch 827: loss = 0.9826, acc = 0.7283, time = 0.001s\n",
      "Epoch 828: loss = 0.9828, acc = 0.7288, time = 0.001s\n",
      "Epoch 829: loss = 0.9824, acc = 0.7281, time = 0.002s\n",
      "Epoch 830: loss = 0.9817, acc = 0.7280, time = 0.001s\n",
      "Epoch 831: loss = 0.9813, acc = 0.7274, time = 0.001s\n",
      "Epoch 832: loss = 0.9815, acc = 0.7284, time = 0.001s\n",
      "Epoch 833: loss = 0.9815, acc = 0.7277, time = 0.000s\n",
      "Epoch 834: loss = 0.9811, acc = 0.7282, time = 0.000s\n",
      "Epoch 835: loss = 0.9806, acc = 0.7279, time = 0.000s\n",
      "Epoch 836: loss = 0.9805, acc = 0.7275, time = 0.005s\n",
      "Epoch 837: loss = 0.9806, acc = 0.7286, time = 0.000s\n",
      "Epoch 838: loss = 0.9804, acc = 0.7284, time = 0.000s\n",
      "Epoch 839: loss = 0.9801, acc = 0.7282, time = 0.000s\n",
      "Epoch 840: loss = 0.9799, acc = 0.7286, time = 0.000s\n",
      "Epoch 841: loss = 0.9798, acc = 0.7279, time = 0.000s\n",
      "Epoch 842: loss = 0.9799, acc = 0.7281, time = 0.000s\n",
      "Epoch 843: loss = 0.9799, acc = 0.7269, time = 0.000s\n",
      "Epoch 844: loss = 0.9799, acc = 0.7282, time = 0.000s\n",
      "Epoch 845: loss = 0.9800, acc = 0.7268, time = 0.000s\n",
      "Epoch 846: loss = 0.9807, acc = 0.7240, time = 0.000s\n",
      "Epoch 847: loss = 0.9814, acc = 0.7274, time = 0.000s\n",
      "Epoch 848: loss = 0.9827, acc = 0.7223, time = 0.000s\n",
      "Epoch 849: loss = 0.9836, acc = 0.7277, time = 0.000s\n",
      "Epoch 850: loss = 0.9850, acc = 0.7219, time = 0.000s\n",
      "Epoch 851: loss = 0.9851, acc = 0.7268, time = 0.000s\n",
      "Epoch 852: loss = 0.9852, acc = 0.7222, time = 0.000s\n",
      "Epoch 853: loss = 0.9836, acc = 0.7282, time = 0.000s\n",
      "Epoch 854: loss = 0.9824, acc = 0.7272, time = 0.000s\n",
      "Epoch 855: loss = 0.9817, acc = 0.7277, time = 0.000s\n",
      "Epoch 856: loss = 0.9825, acc = 0.7277, time = 0.000s\n",
      "Epoch 857: loss = 0.9847, acc = 0.7255, time = 0.000s\n",
      "Epoch 858: loss = 0.9871, acc = 0.7244, time = 0.000s\n",
      "Epoch 859: loss = 0.9901, acc = 0.7193, time = 0.000s\n",
      "Epoch 860: loss = 0.9907, acc = 0.7236, time = 0.000s\n",
      "Epoch 861: loss = 0.9902, acc = 0.7202, time = 0.000s\n",
      "Epoch 862: loss = 0.9847, acc = 0.7270, time = 0.016s\n",
      "Epoch 863: loss = 0.9798, acc = 0.7265, time = 0.000s\n",
      "Epoch 864: loss = 0.9790, acc = 0.7288, time = 0.000s\n",
      "Epoch 865: loss = 0.9817, acc = 0.7251, time = 0.000s\n",
      "Epoch 866: loss = 0.9839, acc = 0.7217, time = 0.000s\n",
      "Epoch 867: loss = 0.9815, acc = 0.7276, time = 0.000s\n",
      "Epoch 868: loss = 0.9780, acc = 0.7278, time = 0.000s\n",
      "Epoch 869: loss = 0.9768, acc = 0.7297, time = 0.000s\n",
      "Epoch 870: loss = 0.9781, acc = 0.7275, time = 0.000s\n",
      "Epoch 871: loss = 0.9791, acc = 0.7248, time = 0.000s\n",
      "Epoch 872: loss = 0.9779, acc = 0.7284, time = 0.000s\n",
      "Epoch 873: loss = 0.9767, acc = 0.7268, time = 0.000s\n",
      "Epoch 874: loss = 0.9764, acc = 0.7288, time = 0.000s\n",
      "Epoch 875: loss = 0.9763, acc = 0.7286, time = 0.016s\n",
      "Epoch 876: loss = 0.9757, acc = 0.7303, time = 0.002s\n",
      "Epoch 877: loss = 0.9752, acc = 0.7306, time = 0.000s\n",
      "Epoch 878: loss = 0.9756, acc = 0.7268, time = 0.000s\n",
      "Epoch 879: loss = 0.9756, acc = 0.7299, time = 0.000s\n",
      "Epoch 880: loss = 0.9749, acc = 0.7283, time = 0.000s\n",
      "Epoch 881: loss = 0.9738, acc = 0.7307, time = 0.000s\n",
      "Epoch 882: loss = 0.9736, acc = 0.7307, time = 0.007s\n",
      "Epoch 883: loss = 0.9742, acc = 0.7278, time = 0.001s\n",
      "Epoch 884: loss = 0.9744, acc = 0.7307, time = 0.001s\n",
      "Epoch 885: loss = 0.9738, acc = 0.7283, time = 0.001s\n",
      "Epoch 886: loss = 0.9729, acc = 0.7301, time = 0.002s\n",
      "Epoch 887: loss = 0.9726, acc = 0.7299, time = 0.000s\n",
      "Epoch 888: loss = 0.9729, acc = 0.7287, time = 0.000s\n",
      "Epoch 889: loss = 0.9731, acc = 0.7311, time = 0.000s\n",
      "Epoch 890: loss = 0.9727, acc = 0.7278, time = 0.000s\n",
      "Epoch 891: loss = 0.9721, acc = 0.7317, time = 0.000s\n",
      "Epoch 892: loss = 0.9717, acc = 0.7307, time = 0.000s\n",
      "Epoch 893: loss = 0.9717, acc = 0.7280, time = 0.000s\n",
      "Epoch 894: loss = 0.9718, acc = 0.7311, time = 0.000s\n",
      "Epoch 895: loss = 0.9718, acc = 0.7285, time = 0.000s\n",
      "Epoch 896: loss = 0.9715, acc = 0.7315, time = 0.000s\n",
      "Epoch 897: loss = 0.9711, acc = 0.7289, time = 0.000s\n",
      "Epoch 898: loss = 0.9709, acc = 0.7300, time = 0.000s\n",
      "Epoch 899: loss = 0.9709, acc = 0.7300, time = 0.000s\n",
      "Epoch 900: loss = 0.9709, acc = 0.7295, time = 0.016s\n",
      "Epoch 901: loss = 0.9710, acc = 0.7316, time = 0.000s\n",
      "Epoch 902: loss = 0.9711, acc = 0.7297, time = 0.000s\n",
      "Epoch 903: loss = 0.9712, acc = 0.7310, time = 0.000s\n",
      "Epoch 904: loss = 0.9719, acc = 0.7300, time = 0.000s\n",
      "Epoch 905: loss = 0.9734, acc = 0.7297, time = 0.000s\n",
      "Epoch 906: loss = 0.9757, acc = 0.7281, time = 0.000s\n",
      "Epoch 907: loss = 0.9792, acc = 0.7255, time = 0.000s\n",
      "Epoch 908: loss = 0.9817, acc = 0.7227, time = 0.000s\n",
      "Epoch 909: loss = 0.9824, acc = 0.7190, time = 0.016s\n",
      "Epoch 910: loss = 0.9816, acc = 0.7230, time = 0.000s\n",
      "Epoch 911: loss = 0.9838, acc = 0.7169, time = 0.000s\n",
      "Epoch 912: loss = 0.9888, acc = 0.7195, time = 0.000s\n",
      "Epoch 913: loss = 0.9971, acc = 0.7085, time = 0.000s\n",
      "Epoch 914: loss = 1.0061, acc = 0.7123, time = 0.000s\n",
      "Epoch 915: loss = 1.0193, acc = 0.6962, time = 0.000s\n",
      "Epoch 916: loss = 1.0306, acc = 0.6991, time = 0.000s\n",
      "Epoch 917: loss = 1.0248, acc = 0.6970, time = 0.000s\n",
      "Epoch 918: loss = 0.9949, acc = 0.7212, time = 0.016s\n",
      "Epoch 919: loss = 0.9773, acc = 0.7259, time = 0.000s\n",
      "Epoch 920: loss = 0.9910, acc = 0.7164, time = 0.000s\n",
      "Epoch 921: loss = 0.9948, acc = 0.7205, time = 0.000s\n",
      "Epoch 922: loss = 0.9821, acc = 0.7190, time = 0.000s\n",
      "Epoch 923: loss = 0.9817, acc = 0.7272, time = 0.000s\n",
      "Epoch 924: loss = 0.9843, acc = 0.7245, time = 0.000s\n",
      "Epoch 925: loss = 0.9808, acc = 0.7258, time = 0.000s\n",
      "Epoch 926: loss = 0.9800, acc = 0.7274, time = 0.000s\n",
      "Epoch 927: loss = 0.9781, acc = 0.7272, time = 0.000s\n",
      "Epoch 928: loss = 0.9765, acc = 0.7263, time = 0.000s\n",
      "Epoch 929: loss = 0.9770, acc = 0.7304, time = 0.016s\n",
      "Epoch 930: loss = 0.9751, acc = 0.7277, time = 0.000s\n",
      "Epoch 931: loss = 0.9732, acc = 0.7271, time = 0.000s\n",
      "Epoch 932: loss = 0.9736, acc = 0.7294, time = 0.000s\n",
      "Epoch 933: loss = 0.9727, acc = 0.7273, time = 0.000s\n",
      "Epoch 934: loss = 0.9715, acc = 0.7281, time = 0.000s\n",
      "Epoch 935: loss = 0.9711, acc = 0.7315, time = 0.000s\n",
      "Epoch 936: loss = 0.9705, acc = 0.7303, time = 0.000s\n",
      "Epoch 937: loss = 0.9705, acc = 0.7288, time = 0.000s\n",
      "Epoch 938: loss = 0.9688, acc = 0.7305, time = 0.000s\n",
      "Epoch 939: loss = 0.9690, acc = 0.7272, time = 0.000s\n",
      "Epoch 940: loss = 0.9697, acc = 0.7304, time = 0.000s\n",
      "Epoch 941: loss = 0.9673, acc = 0.7300, time = 0.000s\n",
      "Epoch 942: loss = 0.9679, acc = 0.7320, time = 0.000s\n",
      "Epoch 943: loss = 0.9684, acc = 0.7291, time = 0.000s\n",
      "Epoch 944: loss = 0.9667, acc = 0.7298, time = 0.000s\n",
      "Epoch 945: loss = 0.9674, acc = 0.7321, time = 0.000s\n",
      "Epoch 946: loss = 0.9672, acc = 0.7295, time = 0.000s\n",
      "Epoch 947: loss = 0.9659, acc = 0.7312, time = 0.000s\n",
      "Epoch 948: loss = 0.9668, acc = 0.7317, time = 0.016s\n",
      "Epoch 949: loss = 0.9663, acc = 0.7318, time = 0.000s\n",
      "Epoch 950: loss = 0.9654, acc = 0.7320, time = 0.000s\n",
      "Epoch 951: loss = 0.9661, acc = 0.7316, time = 0.004s\n",
      "Epoch 952: loss = 0.9657, acc = 0.7318, time = 0.002s\n",
      "Epoch 953: loss = 0.9649, acc = 0.7317, time = 0.002s\n",
      "Epoch 954: loss = 0.9652, acc = 0.7318, time = 0.001s\n",
      "Epoch 955: loss = 0.9650, acc = 0.7330, time = 0.000s\n",
      "Epoch 956: loss = 0.9645, acc = 0.7314, time = 0.000s\n",
      "Epoch 957: loss = 0.9646, acc = 0.7313, time = 0.000s\n",
      "Epoch 958: loss = 0.9644, acc = 0.7331, time = 0.000s\n",
      "Epoch 959: loss = 0.9640, acc = 0.7315, time = 0.000s\n",
      "Epoch 960: loss = 0.9640, acc = 0.7320, time = 0.000s\n",
      "Epoch 961: loss = 0.9639, acc = 0.7325, time = 0.000s\n",
      "Epoch 962: loss = 0.9635, acc = 0.7320, time = 0.000s\n",
      "Epoch 963: loss = 0.9634, acc = 0.7324, time = 0.015s\n",
      "Epoch 964: loss = 0.9633, acc = 0.7319, time = 0.001s\n",
      "Epoch 965: loss = 0.9631, acc = 0.7332, time = 0.000s\n",
      "Epoch 966: loss = 0.9629, acc = 0.7324, time = 0.000s\n",
      "Epoch 967: loss = 0.9627, acc = 0.7326, time = 0.000s\n",
      "Epoch 968: loss = 0.9626, acc = 0.7328, time = 0.000s\n",
      "Epoch 969: loss = 0.9624, acc = 0.7332, time = 0.000s\n",
      "Epoch 970: loss = 0.9623, acc = 0.7327, time = 0.000s\n",
      "Epoch 971: loss = 0.9621, acc = 0.7323, time = 0.000s\n",
      "Epoch 972: loss = 0.9620, acc = 0.7338, time = 0.000s\n",
      "Epoch 973: loss = 0.9618, acc = 0.7330, time = 0.017s\n",
      "Epoch 974: loss = 0.9616, acc = 0.7322, time = 0.001s\n",
      "Epoch 975: loss = 0.9615, acc = 0.7328, time = 0.002s\n",
      "Epoch 976: loss = 0.9614, acc = 0.7328, time = 0.002s\n",
      "Epoch 977: loss = 0.9612, acc = 0.7329, time = 0.001s\n",
      "Epoch 978: loss = 0.9611, acc = 0.7327, time = 0.002s\n",
      "Epoch 979: loss = 0.9610, acc = 0.7327, time = 0.001s\n",
      "Epoch 980: loss = 0.9609, acc = 0.7337, time = 0.001s\n",
      "Epoch 981: loss = 0.9609, acc = 0.7332, time = 0.000s\n",
      "Epoch 982: loss = 0.9609, acc = 0.7336, time = 0.000s\n",
      "Epoch 983: loss = 0.9610, acc = 0.7329, time = 0.000s\n",
      "Epoch 984: loss = 0.9612, acc = 0.7335, time = 0.000s\n",
      "Epoch 985: loss = 0.9617, acc = 0.7349, time = 0.007s\n",
      "Epoch 986: loss = 0.9626, acc = 0.7323, time = 0.000s\n",
      "Epoch 987: loss = 0.9641, acc = 0.7332, time = 0.000s\n",
      "Epoch 988: loss = 0.9658, acc = 0.7306, time = 0.000s\n",
      "Epoch 989: loss = 0.9678, acc = 0.7311, time = 0.000s\n",
      "Epoch 990: loss = 0.9679, acc = 0.7290, time = 0.000s\n",
      "Epoch 991: loss = 0.9667, acc = 0.7257, time = 0.000s\n",
      "Epoch 992: loss = 0.9640, acc = 0.7322, time = 0.000s\n",
      "Epoch 993: loss = 0.9624, acc = 0.7265, time = 0.000s\n",
      "Epoch 994: loss = 0.9624, acc = 0.7346, time = 0.016s\n",
      "Epoch 995: loss = 0.9634, acc = 0.7261, time = 0.000s\n",
      "Epoch 996: loss = 0.9641, acc = 0.7332, time = 0.000s\n",
      "Epoch 997: loss = 0.9635, acc = 0.7304, time = 0.000s\n",
      "Epoch 998: loss = 0.9620, acc = 0.7325, time = 0.000s\n",
      "Epoch 999: loss = 0.9613, acc = 0.7319, time = 0.000s\n",
      "Epoch 1000: loss = 0.9620, acc = 0.7318, time = 0.000s\n",
      "best epoch: 984 Best acc: 0.7349300980567932\n",
      "Best accuracy on validation split: 73.49%\n",
      "Best hyperparameters: {'activation': 'tanh', 'alpha': 0.001, 'h1': 100, 'h2': 100, 'h3': 50, 'lr': 0.01}\n",
      "Accuracy: 56.48%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "Xtrain, Ytrain, Xtest, Ytest, le = split_data(traindata, testdata)\n",
    "best_mlp, epoch_times = find_best_mlp_3layer(Xtrain, Ytrain, 30)\n",
    "\n",
    "best_mlp.eval()\n",
    "with torch.no_grad():\n",
    "    logits = best_mlp(Xtest)\n",
    "    y_pred = logits.argmax(dim=1)\n",
    "    acc = (y_pred == Ytest).float().mean().item()\n",
    "    pred_proba = F.softmax(logits, dim=1)\n",
    "    print(f\"Accuracy: {acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "y_true_decoded = le.inverse_transform(Ytest.cpu().numpy())\n",
    "y_pred_decoded = le.inverse_transform(y_pred.cpu().numpy())\n",
    "y_score = pred_proba.cpu().numpy()\n",
    "y_true_bin = label_binarize(Ytest.cpu().numpy(), classes=list(range(30)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = os.path.join(\"results\", f\"gtd{partition}.txt\")\n",
    "\n",
    "# Make sure the directory exists\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Write a string to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "    file.write(f\"Precision weighted: {precision_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"Recall weighted: {recall_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score weighted: {f1_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"ROCAUC Weighted: {roc_auc_score(y_true_bin, y_score, average='weighted', multi_class='ovr'):.4f}\\n\")\n",
    "\n",
    "\n",
    "    file.write(f\"Precision micro: {precision_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"Recall micro: {recall_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score micro: {f1_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"ROCAUC micro: {roc_auc_score(y_true_bin, y_score, average='micro', multi_class='ovr'):.4f}\\n\")\n",
    "\n",
    "    file.write(f\"Precision macro: {precision_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"Recall macro: {recall_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score macro: {f1_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"ROCAUC macro: {roc_auc_score(y_true_bin, y_score, average='macro', multi_class='ovr'):.4f}\\n\")\n",
    "\n",
    "with open(f\"results/epoch_logs_gtd{partition}\", \"w\") as f:\n",
    "    f.write('\\n'.join(str(x) for x in epoch_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "                          Abu Sayyaf Group (ASG)       0.35      0.50      0.41       144\n",
      "        African National Congress (South Africa)       0.58      0.86      0.70       144\n",
      "                                Al-Qaida in Iraq       0.50      0.66      0.57       144\n",
      "        Al-Qaida in the Arabian Peninsula (AQAP)       0.46      0.35      0.40       144\n",
      "                                      Al-Shabaab       0.31      0.28      0.29       144\n",
      "             Basque Fatherland and Freedom (ETA)       0.61      0.74      0.67       144\n",
      "                                      Boko Haram       0.51      0.30      0.38       144\n",
      "  Communist Party of India - Maoist (CPI-Maoist)       0.62      0.58      0.60       144\n",
      "       Corsican National Liberation Front (FLNC)       0.53      0.83      0.65       144\n",
      "                       Donetsk People's Republic       0.64      0.69      0.66       144\n",
      "Farabundo Marti National Liberation Front (FMLN)       0.54      0.60      0.57       144\n",
      "                               Fulani extremists       0.68      0.82      0.74       144\n",
      "                 Houthi extremists (Ansar Allah)       0.51      0.57      0.54       144\n",
      "                     Irish Republican Army (IRA)       0.82      0.78      0.80       144\n",
      "     Islamic State of Iraq and the Levant (ISIL)       0.41      0.37      0.39       144\n",
      "                  Kurdistan Workers' Party (PKK)       0.35      0.31      0.32       144\n",
      "         Liberation Tigers of Tamil Eelam (LTTE)       0.61      0.57      0.59       144\n",
      "         Manuel Rodriguez Patriotic Front (FPMR)       0.65      0.69      0.67       144\n",
      "                                         Maoists       0.44      0.49      0.47       144\n",
      "                               Muslim extremists       0.41      0.36      0.38       144\n",
      "      National Liberation Army of Colombia (ELN)       0.67      0.45      0.54       144\n",
      "                         New People's Army (NPA)       0.63      0.69      0.66       144\n",
      "               Nicaraguan Democratic Force (FDN)       0.63      0.73      0.68       144\n",
      "                                    Palestinians       0.80      0.61      0.69       144\n",
      "   Revolutionary Armed Forces of Colombia (FARC)       0.75      0.53      0.63       144\n",
      "                               Shining Path (SL)       0.63      0.38      0.48       144\n",
      "                                 Sikh Extremists       0.68      0.70      0.69       144\n",
      "                                         Taliban       0.36      0.29      0.32       144\n",
      "                 Tehrik-i-Taliban Pakistan (TTP)       0.65      0.56      0.60       144\n",
      "       Tupac Amaru Revolutionary Movement (MRTA)       0.74      0.64      0.69       144\n",
      "\n",
      "                                        accuracy                           0.56      4320\n",
      "                                       macro avg       0.57      0.56      0.56      4320\n",
      "                                    weighted avg       0.57      0.56      0.56      4320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_decoded, y_pred_decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP3Layer(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=13, out_features=100, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=50, out_features=30, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(cm_normalized,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels,\n",
    "                cmap=\"viridis\",\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "    plt.title(f\"Normalized Confusion Matrix (Partition {partition})\", fontsize=18)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "    plt.ylabel(\"True Label\", fontsize=14)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = f\"results/confusion_matrix_partition_{partition}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved confusion matrix for partition {partition} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved confusion matrix for partition 478 to results/confusion_matrix_partition_478.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all unique class labels from the truths\n",
    "class_labels = np.unique(y_true_decoded)\n",
    "\n",
    "plot_confusion_matrix(y_true_decoded, y_pred_decoded, labels=class_labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
