{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = f'../../../../../data/top30groups/LongLatCombined/scaledtrain1/train{partition}.csv'\n",
    "testpath = f'../../../../../data/top30groups/LongLatCombined/scaledtest1/test{partition}.csv'\n",
    "\n",
    "traindata = pd.read_csv(trainpath, encoding='ISO-8859-1')\n",
    "testdata = pd.read_csv(testpath, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2700, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6300, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(dftrain, dftest):\n",
    "    Xtrain = dftrain.drop(columns=['gname']).values\n",
    "    Ytrain = dftrain['gname'].values\n",
    "    Xtest = dftest.drop(columns=['gname']).values\n",
    "    Ytest = dftest['gname'].values\n",
    "\n",
    "    # Encode labels as integers\n",
    "    le = LabelEncoder()\n",
    "    Ytrain = le.fit_transform(Ytrain)\n",
    "    Ytest = le.transform(Ytest)\n",
    "\n",
    "    Xtrain = Xtrain.astype(float)\n",
    "    Xtest = Xtest.astype(float)\n",
    "\n",
    "    # Convert to torch tensors and move to GPU\n",
    "    Xtrain = torch.tensor(Xtrain, dtype=torch.float32).to(\"cuda\")\n",
    "    Ytrain = torch.tensor(Ytrain, dtype=torch.long).to(\"cuda\")\n",
    "    Xtest = torch.tensor(Xtest, dtype=torch.float32).to(\"cuda\")\n",
    "    Ytest = torch.tensor(Ytest, dtype=torch.long).to(\"cuda\")\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import time\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1, hidden2, output_dim, activation='relu'):\n",
    "        super().__init__()\n",
    "        act_fn = nn.ReLU() if activation == 'relu' else nn.Tanh()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden1),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_model(model, Xtrain, Ytrain, lr, alpha, searching=False, max_epochs=1000):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=alpha)\n",
    "\n",
    "    epoch_times = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    best_acc = -1\n",
    "    best_epoch = -1\n",
    "    best_state_dict = None\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Training step\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(Xtrain)\n",
    "        loss = criterion(output, Ytrain)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy on full training set\n",
    "        if not searching:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = output.argmax(dim=1)\n",
    "                acc = (pred == Ytrain).float().mean().item()\n",
    "                train_accuracies.append(acc)\n",
    "\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_epoch = epoch\n",
    "                    best_state_dict = model.state_dict()\n",
    "\n",
    "            end_time = time.time()\n",
    "            epoch_times.append(end_time - start_time)\n",
    "\n",
    "            print(f\"Epoch {epoch+1:03d}: loss = {loss.item():.4f}, acc = {acc:.4f}, time = {end_time - start_time:.3f}s\")\n",
    "\n",
    "    # Restore best model weights\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    if not searching:\n",
    "        print(f\"best epoch: {best_epoch} Best acc: {best_acc}\")\n",
    "\n",
    "    return model, epoch_times, train_accuracies, best_epoch, best_acc\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, Xval, Yval):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(Xval).argmax(dim=1)\n",
    "        acc = (pred == Yval).float().mean().item()\n",
    "    return acc\n",
    "\n",
    "def find_best_mlp(Xtrain, Ytrain, num_classes, n_iter=20, max_epochs=1000):\n",
    "    input_dim = Xtrain.shape[1]\n",
    "\n",
    "    param_dist = {\n",
    "        'hidden1': [50, 100, 150, 200],\n",
    "        'hidden2': [25, 50, 100],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'lr': [0.0001, 0.001, 0.01],\n",
    "        'alpha': [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "    }\n",
    "\n",
    "    best_acc = -1\n",
    "    best_params = None\n",
    "\n",
    "    for params in list(ParameterSampler(param_dist, n_iter=n_iter, random_state=42)):\n",
    "        indices = torch.randperm(Xtrain.size(0))\n",
    "        split = int(0.8 * len(indices))\n",
    "        train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "        model = SimpleMLP(\n",
    "            input_dim=input_dim,\n",
    "            hidden1=params['hidden1'],\n",
    "            hidden2=params['hidden2'],\n",
    "            output_dim=num_classes,\n",
    "            activation=params['activation']\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        _ = train_model(model, Xtrain[train_idx], Ytrain[train_idx],\n",
    "                    lr=params['lr'], alpha=params['alpha'], searching=True, max_epochs=max_epochs)\n",
    "\n",
    "        acc = evaluate_model(model, Xtrain[val_idx], Ytrain[val_idx])\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_params = params\n",
    "\n",
    "    \n",
    "    final_model = SimpleMLP(\n",
    "        input_dim=input_dim,\n",
    "        hidden1=best_params['hidden1'],\n",
    "        hidden2=best_params['hidden2'],\n",
    "        output_dim=num_classes,\n",
    "        activation=best_params['activation']\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    _, epoch_times, train_accuracies, best_epoch, best_acc = train_model(final_model, Xtrain, Ytrain,\n",
    "                lr=best_params['lr'], alpha=best_params['alpha'], searching=False, max_epochs=max_epochs)\n",
    "\n",
    "    print(f\"Best accuracy on validation split: {best_acc * 100:.2f}%\")\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "    return final_model, epoch_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: loss = 3.4035, acc = 0.0368, time = 0.002s\n",
      "Epoch 002: loss = 3.3851, acc = 0.0478, time = 0.003s\n",
      "Epoch 003: loss = 3.3668, acc = 0.0683, time = 0.001s\n",
      "Epoch 004: loss = 3.3487, acc = 0.0843, time = 0.001s\n",
      "Epoch 005: loss = 3.3307, acc = 0.1189, time = 0.002s\n",
      "Epoch 006: loss = 3.3128, acc = 0.1335, time = 0.001s\n",
      "Epoch 007: loss = 3.2949, acc = 0.1427, time = 0.001s\n",
      "Epoch 008: loss = 3.2770, acc = 0.1506, time = 0.002s\n",
      "Epoch 009: loss = 3.2591, acc = 0.1622, time = 0.001s\n",
      "Epoch 010: loss = 3.2411, acc = 0.1719, time = 0.001s\n",
      "Epoch 011: loss = 3.2230, acc = 0.1827, time = 0.003s\n",
      "Epoch 012: loss = 3.2047, acc = 0.1960, time = 0.001s\n",
      "Epoch 013: loss = 3.1862, acc = 0.2087, time = 0.002s\n",
      "Epoch 014: loss = 3.1675, acc = 0.2217, time = 0.002s\n",
      "Epoch 015: loss = 3.1485, acc = 0.2303, time = 0.000s\n",
      "Epoch 016: loss = 3.1293, acc = 0.2411, time = 0.000s\n",
      "Epoch 017: loss = 3.1097, acc = 0.2438, time = 0.000s\n",
      "Epoch 018: loss = 3.0899, acc = 0.2513, time = 0.006s\n",
      "Epoch 019: loss = 3.0698, acc = 0.2597, time = 0.001s\n",
      "Epoch 020: loss = 3.0493, acc = 0.2659, time = 0.002s\n",
      "Epoch 021: loss = 3.0285, acc = 0.2722, time = 0.001s\n",
      "Epoch 022: loss = 3.0074, acc = 0.2760, time = 0.002s\n",
      "Epoch 023: loss = 2.9860, acc = 0.2811, time = 0.001s\n",
      "Epoch 024: loss = 2.9642, acc = 0.2862, time = 0.001s\n",
      "Epoch 025: loss = 2.9422, acc = 0.2889, time = 0.001s\n",
      "Epoch 026: loss = 2.9199, acc = 0.2925, time = 0.001s\n",
      "Epoch 027: loss = 2.8973, acc = 0.2970, time = 0.002s\n",
      "Epoch 028: loss = 2.8744, acc = 0.3000, time = 0.001s\n",
      "Epoch 029: loss = 2.8514, acc = 0.3025, time = 0.001s\n",
      "Epoch 030: loss = 2.8281, acc = 0.3079, time = 0.001s\n",
      "Epoch 031: loss = 2.8047, acc = 0.3108, time = 0.002s\n",
      "Epoch 032: loss = 2.7811, acc = 0.3149, time = 0.002s\n",
      "Epoch 033: loss = 2.7574, acc = 0.3189, time = 0.001s\n",
      "Epoch 034: loss = 2.7335, acc = 0.3221, time = 0.000s\n",
      "Epoch 035: loss = 2.7096, acc = 0.3300, time = 0.003s\n",
      "Epoch 036: loss = 2.6857, acc = 0.3352, time = 0.001s\n",
      "Epoch 037: loss = 2.6617, acc = 0.3408, time = 0.001s\n",
      "Epoch 038: loss = 2.6378, acc = 0.3505, time = 0.001s\n",
      "Epoch 039: loss = 2.6139, acc = 0.3563, time = 0.001s\n",
      "Epoch 040: loss = 2.5901, acc = 0.3640, time = 0.001s\n",
      "Epoch 041: loss = 2.5663, acc = 0.3724, time = 0.002s\n",
      "Epoch 042: loss = 2.5427, acc = 0.3795, time = 0.002s\n",
      "Epoch 043: loss = 2.5192, acc = 0.3844, time = 0.001s\n",
      "Epoch 044: loss = 2.4959, acc = 0.3887, time = 0.003s\n",
      "Epoch 045: loss = 2.4727, acc = 0.3916, time = 0.002s\n",
      "Epoch 046: loss = 2.4498, acc = 0.3948, time = 0.000s\n",
      "Epoch 047: loss = 2.4271, acc = 0.3989, time = 0.002s\n",
      "Epoch 048: loss = 2.4046, acc = 0.4037, time = 0.000s\n",
      "Epoch 049: loss = 2.3824, acc = 0.4059, time = 0.003s\n",
      "Epoch 050: loss = 2.3604, acc = 0.4094, time = 0.002s\n",
      "Epoch 051: loss = 2.3387, acc = 0.4130, time = 0.001s\n",
      "Epoch 052: loss = 2.3173, acc = 0.4183, time = 0.001s\n",
      "Epoch 053: loss = 2.2962, acc = 0.4211, time = 0.001s\n",
      "Epoch 054: loss = 2.2754, acc = 0.4222, time = 0.001s\n",
      "Epoch 055: loss = 2.2548, acc = 0.4241, time = 0.002s\n",
      "Epoch 056: loss = 2.2346, acc = 0.4262, time = 0.001s\n",
      "Epoch 057: loss = 2.2147, acc = 0.4290, time = 0.000s\n",
      "Epoch 058: loss = 2.1951, acc = 0.4325, time = 0.002s\n",
      "Epoch 059: loss = 2.1759, acc = 0.4352, time = 0.002s\n",
      "Epoch 060: loss = 2.1569, acc = 0.4386, time = 0.001s\n",
      "Epoch 061: loss = 2.1383, acc = 0.4443, time = 0.001s\n",
      "Epoch 062: loss = 2.1199, acc = 0.4506, time = 0.001s\n",
      "Epoch 063: loss = 2.1019, acc = 0.4543, time = 0.000s\n",
      "Epoch 064: loss = 2.0842, acc = 0.4571, time = 0.002s\n",
      "Epoch 065: loss = 2.0668, acc = 0.4597, time = 0.002s\n",
      "Epoch 066: loss = 2.0497, acc = 0.4625, time = 0.001s\n",
      "Epoch 067: loss = 2.0330, acc = 0.4651, time = 0.001s\n",
      "Epoch 068: loss = 2.0165, acc = 0.4678, time = 0.002s\n",
      "Epoch 069: loss = 2.0003, acc = 0.4694, time = 0.001s\n",
      "Epoch 070: loss = 1.9845, acc = 0.4732, time = 0.001s\n",
      "Epoch 071: loss = 1.9689, acc = 0.4770, time = 0.002s\n",
      "Epoch 072: loss = 1.9536, acc = 0.4798, time = 0.001s\n",
      "Epoch 073: loss = 1.9386, acc = 0.4821, time = 0.001s\n",
      "Epoch 074: loss = 1.9238, acc = 0.4852, time = 0.001s\n",
      "Epoch 075: loss = 1.9093, acc = 0.4887, time = 0.001s\n",
      "Epoch 076: loss = 1.8951, acc = 0.4919, time = 0.001s\n",
      "Epoch 077: loss = 1.8811, acc = 0.4956, time = 0.002s\n",
      "Epoch 078: loss = 1.8674, acc = 0.4984, time = 0.002s\n",
      "Epoch 079: loss = 1.8539, acc = 0.5022, time = 0.001s\n",
      "Epoch 080: loss = 1.8407, acc = 0.5057, time = 0.000s\n",
      "Epoch 081: loss = 1.8277, acc = 0.5081, time = 0.002s\n",
      "Epoch 082: loss = 1.8149, acc = 0.5127, time = 0.002s\n",
      "Epoch 083: loss = 1.8023, acc = 0.5156, time = 0.001s\n",
      "Epoch 084: loss = 1.7899, acc = 0.5195, time = 0.002s\n",
      "Epoch 085: loss = 1.7778, acc = 0.5232, time = 0.001s\n",
      "Epoch 086: loss = 1.7658, acc = 0.5287, time = 0.001s\n",
      "Epoch 087: loss = 1.7541, acc = 0.5327, time = 0.001s\n",
      "Epoch 088: loss = 1.7425, acc = 0.5352, time = 0.001s\n",
      "Epoch 089: loss = 1.7311, acc = 0.5371, time = 0.002s\n",
      "Epoch 090: loss = 1.7199, acc = 0.5406, time = 0.001s\n",
      "Epoch 091: loss = 1.7089, acc = 0.5405, time = 0.000s\n",
      "Epoch 092: loss = 1.6980, acc = 0.5422, time = 0.003s\n",
      "Epoch 093: loss = 1.6873, acc = 0.5478, time = 0.000s\n",
      "Epoch 094: loss = 1.6768, acc = 0.5529, time = 0.000s\n",
      "Epoch 095: loss = 1.6664, acc = 0.5565, time = 0.003s\n",
      "Epoch 096: loss = 1.6562, acc = 0.5614, time = 0.001s\n",
      "Epoch 097: loss = 1.6461, acc = 0.5654, time = 0.001s\n",
      "Epoch 098: loss = 1.6362, acc = 0.5692, time = 0.001s\n",
      "Epoch 099: loss = 1.6264, acc = 0.5737, time = 0.002s\n",
      "Epoch 100: loss = 1.6167, acc = 0.5762, time = 0.000s\n",
      "Epoch 101: loss = 1.6072, acc = 0.5795, time = 0.002s\n",
      "Epoch 102: loss = 1.5978, acc = 0.5825, time = 0.002s\n",
      "Epoch 103: loss = 1.5885, acc = 0.5873, time = 0.001s\n",
      "Epoch 104: loss = 1.5793, acc = 0.5897, time = 0.001s\n",
      "Epoch 105: loss = 1.5702, acc = 0.5922, time = 0.001s\n",
      "Epoch 106: loss = 1.5613, acc = 0.5951, time = 0.001s\n",
      "Epoch 107: loss = 1.5525, acc = 0.5970, time = 0.002s\n",
      "Epoch 108: loss = 1.5437, acc = 0.5994, time = 0.002s\n",
      "Epoch 109: loss = 1.5351, acc = 0.6003, time = 0.001s\n",
      "Epoch 110: loss = 1.5266, acc = 0.6025, time = 0.003s\n",
      "Epoch 111: loss = 1.5181, acc = 0.6035, time = 0.001s\n",
      "Epoch 112: loss = 1.5098, acc = 0.6065, time = 0.001s\n",
      "Epoch 113: loss = 1.5016, acc = 0.6086, time = 0.001s\n",
      "Epoch 114: loss = 1.4934, acc = 0.6098, time = 0.000s\n",
      "Epoch 115: loss = 1.4853, acc = 0.6105, time = 0.002s\n",
      "Epoch 116: loss = 1.4773, acc = 0.6114, time = 0.002s\n",
      "Epoch 117: loss = 1.4694, acc = 0.6130, time = 0.002s\n",
      "Epoch 118: loss = 1.4615, acc = 0.6143, time = 0.001s\n",
      "Epoch 119: loss = 1.4538, acc = 0.6156, time = 0.002s\n",
      "Epoch 120: loss = 1.4461, acc = 0.6176, time = 0.002s\n",
      "Epoch 121: loss = 1.4385, acc = 0.6192, time = 0.002s\n",
      "Epoch 122: loss = 1.4309, acc = 0.6219, time = 0.001s\n",
      "Epoch 123: loss = 1.4234, acc = 0.6240, time = 0.001s\n",
      "Epoch 124: loss = 1.4160, acc = 0.6260, time = 0.001s\n",
      "Epoch 125: loss = 1.4086, acc = 0.6286, time = 0.002s\n",
      "Epoch 126: loss = 1.4013, acc = 0.6295, time = 0.001s\n",
      "Epoch 127: loss = 1.3941, acc = 0.6317, time = 0.001s\n",
      "Epoch 128: loss = 1.3869, acc = 0.6341, time = 0.002s\n",
      "Epoch 129: loss = 1.3798, acc = 0.6375, time = 0.002s\n",
      "Epoch 130: loss = 1.3727, acc = 0.6383, time = 0.000s\n",
      "Epoch 131: loss = 1.3657, acc = 0.6397, time = 0.000s\n",
      "Epoch 132: loss = 1.3587, acc = 0.6410, time = 0.004s\n",
      "Epoch 133: loss = 1.3518, acc = 0.6421, time = 0.001s\n",
      "Epoch 134: loss = 1.3450, acc = 0.6435, time = 0.002s\n",
      "Epoch 135: loss = 1.3382, acc = 0.6459, time = 0.002s\n",
      "Epoch 136: loss = 1.3314, acc = 0.6481, time = 0.002s\n",
      "Epoch 137: loss = 1.3247, acc = 0.6490, time = 0.003s\n",
      "Epoch 138: loss = 1.3180, acc = 0.6514, time = 0.001s\n",
      "Epoch 139: loss = 1.3114, acc = 0.6541, time = 0.002s\n",
      "Epoch 140: loss = 1.3048, acc = 0.6560, time = 0.000s\n",
      "Epoch 141: loss = 1.2982, acc = 0.6575, time = 0.002s\n",
      "Epoch 142: loss = 1.2917, acc = 0.6586, time = 0.002s\n",
      "Epoch 143: loss = 1.2853, acc = 0.6603, time = 0.002s\n",
      "Epoch 144: loss = 1.2789, acc = 0.6625, time = 0.002s\n",
      "Epoch 145: loss = 1.2725, acc = 0.6643, time = 0.002s\n",
      "Epoch 146: loss = 1.2661, acc = 0.6659, time = 0.002s\n",
      "Epoch 147: loss = 1.2598, acc = 0.6675, time = 0.001s\n",
      "Epoch 148: loss = 1.2536, acc = 0.6689, time = 0.002s\n",
      "Epoch 149: loss = 1.2473, acc = 0.6713, time = 0.002s\n",
      "Epoch 150: loss = 1.2411, acc = 0.6722, time = 0.001s\n",
      "Epoch 151: loss = 1.2350, acc = 0.6733, time = 0.001s\n",
      "Epoch 152: loss = 1.2289, acc = 0.6744, time = 0.002s\n",
      "Epoch 153: loss = 1.2228, acc = 0.6768, time = 0.002s\n",
      "Epoch 154: loss = 1.2167, acc = 0.6783, time = 0.001s\n",
      "Epoch 155: loss = 1.2107, acc = 0.6798, time = 0.003s\n",
      "Epoch 156: loss = 1.2048, acc = 0.6840, time = 0.001s\n",
      "Epoch 157: loss = 1.1988, acc = 0.6854, time = 0.001s\n",
      "Epoch 158: loss = 1.1929, acc = 0.6868, time = 0.002s\n",
      "Epoch 159: loss = 1.1870, acc = 0.6886, time = 0.003s\n",
      "Epoch 160: loss = 1.1812, acc = 0.6900, time = 0.001s\n",
      "Epoch 161: loss = 1.1754, acc = 0.6919, time = 0.002s\n",
      "Epoch 162: loss = 1.1696, acc = 0.6930, time = 0.002s\n",
      "Epoch 163: loss = 1.1639, acc = 0.6954, time = 0.002s\n",
      "Epoch 164: loss = 1.1582, acc = 0.6970, time = 0.002s\n",
      "Epoch 165: loss = 1.1526, acc = 0.6992, time = 0.002s\n",
      "Epoch 166: loss = 1.1469, acc = 0.7005, time = 0.001s\n",
      "Epoch 167: loss = 1.1413, acc = 0.7008, time = 0.001s\n",
      "Epoch 168: loss = 1.1358, acc = 0.7024, time = 0.001s\n",
      "Epoch 169: loss = 1.1303, acc = 0.7032, time = 0.000s\n",
      "Epoch 170: loss = 1.1248, acc = 0.7029, time = 0.001s\n",
      "Epoch 171: loss = 1.1193, acc = 0.7040, time = 0.002s\n",
      "Epoch 172: loss = 1.1139, acc = 0.7056, time = 0.002s\n",
      "Epoch 173: loss = 1.1085, acc = 0.7073, time = 0.001s\n",
      "Epoch 174: loss = 1.1032, acc = 0.7083, time = 0.002s\n",
      "Epoch 175: loss = 1.0978, acc = 0.7102, time = 0.001s\n",
      "Epoch 176: loss = 1.0925, acc = 0.7116, time = 0.001s\n",
      "Epoch 177: loss = 1.0873, acc = 0.7129, time = 0.001s\n",
      "Epoch 178: loss = 1.0821, acc = 0.7137, time = 0.001s\n",
      "Epoch 179: loss = 1.0769, acc = 0.7144, time = 0.001s\n",
      "Epoch 180: loss = 1.0717, acc = 0.7159, time = 0.001s\n",
      "Epoch 181: loss = 1.0666, acc = 0.7171, time = 0.000s\n",
      "Epoch 182: loss = 1.0615, acc = 0.7184, time = 0.003s\n",
      "Epoch 183: loss = 1.0564, acc = 0.7197, time = 0.001s\n",
      "Epoch 184: loss = 1.0514, acc = 0.7208, time = 0.001s\n",
      "Epoch 185: loss = 1.0464, acc = 0.7225, time = 0.001s\n",
      "Epoch 186: loss = 1.0414, acc = 0.7233, time = 0.001s\n",
      "Epoch 187: loss = 1.0365, acc = 0.7241, time = 0.001s\n",
      "Epoch 188: loss = 1.0316, acc = 0.7249, time = 0.001s\n",
      "Epoch 189: loss = 1.0267, acc = 0.7260, time = 0.001s\n",
      "Epoch 190: loss = 1.0219, acc = 0.7268, time = 0.002s\n",
      "Epoch 191: loss = 1.0171, acc = 0.7276, time = 0.001s\n",
      "Epoch 192: loss = 1.0123, acc = 0.7290, time = 0.002s\n",
      "Epoch 193: loss = 1.0076, acc = 0.7295, time = 0.001s\n",
      "Epoch 194: loss = 1.0029, acc = 0.7308, time = 0.001s\n",
      "Epoch 195: loss = 0.9982, acc = 0.7317, time = 0.003s\n",
      "Epoch 196: loss = 0.9935, acc = 0.7329, time = 0.001s\n",
      "Epoch 197: loss = 0.9889, acc = 0.7329, time = 0.002s\n",
      "Epoch 198: loss = 0.9843, acc = 0.7338, time = 0.001s\n",
      "Epoch 199: loss = 0.9798, acc = 0.7357, time = 0.001s\n",
      "Epoch 200: loss = 0.9752, acc = 0.7367, time = 0.001s\n",
      "Epoch 201: loss = 0.9707, acc = 0.7378, time = 0.002s\n",
      "Epoch 202: loss = 0.9663, acc = 0.7387, time = 0.002s\n",
      "Epoch 203: loss = 0.9618, acc = 0.7395, time = 0.000s\n",
      "Epoch 204: loss = 0.9574, acc = 0.7410, time = 0.002s\n",
      "Epoch 205: loss = 0.9530, acc = 0.7417, time = 0.001s\n",
      "Epoch 206: loss = 0.9487, acc = 0.7425, time = 0.002s\n",
      "Epoch 207: loss = 0.9444, acc = 0.7427, time = 0.001s\n",
      "Epoch 208: loss = 0.9401, acc = 0.7433, time = 0.001s\n",
      "Epoch 209: loss = 0.9358, acc = 0.7437, time = 0.001s\n",
      "Epoch 210: loss = 0.9316, acc = 0.7451, time = 0.003s\n",
      "Epoch 211: loss = 0.9274, acc = 0.7462, time = 0.001s\n",
      "Epoch 212: loss = 0.9232, acc = 0.7465, time = 0.002s\n",
      "Epoch 213: loss = 0.9191, acc = 0.7481, time = 0.002s\n",
      "Epoch 214: loss = 0.9150, acc = 0.7489, time = 0.002s\n",
      "Epoch 215: loss = 0.9109, acc = 0.7500, time = 0.001s\n",
      "Epoch 216: loss = 0.9068, acc = 0.7508, time = 0.001s\n",
      "Epoch 217: loss = 0.9028, acc = 0.7521, time = 0.002s\n",
      "Epoch 218: loss = 0.8988, acc = 0.7538, time = 0.002s\n",
      "Epoch 219: loss = 0.8949, acc = 0.7548, time = 0.001s\n",
      "Epoch 220: loss = 0.8909, acc = 0.7560, time = 0.002s\n",
      "Epoch 221: loss = 0.8870, acc = 0.7573, time = 0.000s\n",
      "Epoch 222: loss = 0.8831, acc = 0.7576, time = 0.002s\n",
      "Epoch 223: loss = 0.8793, acc = 0.7581, time = 0.002s\n",
      "Epoch 224: loss = 0.8754, acc = 0.7590, time = 0.000s\n",
      "Epoch 225: loss = 0.8716, acc = 0.7600, time = 0.000s\n",
      "Epoch 226: loss = 0.8679, acc = 0.7605, time = 0.000s\n",
      "Epoch 227: loss = 0.8641, acc = 0.7611, time = 0.006s\n",
      "Epoch 228: loss = 0.8604, acc = 0.7624, time = 0.002s\n",
      "Epoch 229: loss = 0.8567, acc = 0.7624, time = 0.002s\n",
      "Epoch 230: loss = 0.8530, acc = 0.7629, time = 0.002s\n",
      "Epoch 231: loss = 0.8494, acc = 0.7637, time = 0.001s\n",
      "Epoch 232: loss = 0.8458, acc = 0.7643, time = 0.001s\n",
      "Epoch 233: loss = 0.8422, acc = 0.7649, time = 0.001s\n",
      "Epoch 234: loss = 0.8386, acc = 0.7670, time = 0.001s\n",
      "Epoch 235: loss = 0.8351, acc = 0.7675, time = 0.001s\n",
      "Epoch 236: loss = 0.8316, acc = 0.7686, time = 0.001s\n",
      "Epoch 237: loss = 0.8281, acc = 0.7697, time = 0.003s\n",
      "Epoch 238: loss = 0.8246, acc = 0.7713, time = 0.002s\n",
      "Epoch 239: loss = 0.8212, acc = 0.7719, time = 0.000s\n",
      "Epoch 240: loss = 0.8178, acc = 0.7725, time = 0.004s\n",
      "Epoch 241: loss = 0.8144, acc = 0.7735, time = 0.002s\n",
      "Epoch 242: loss = 0.8110, acc = 0.7744, time = 0.001s\n",
      "Epoch 243: loss = 0.8077, acc = 0.7749, time = 0.002s\n",
      "Epoch 244: loss = 0.8044, acc = 0.7757, time = 0.001s\n",
      "Epoch 245: loss = 0.8011, acc = 0.7762, time = 0.000s\n",
      "Epoch 246: loss = 0.7979, acc = 0.7775, time = 0.004s\n",
      "Epoch 247: loss = 0.7946, acc = 0.7786, time = 0.001s\n",
      "Epoch 248: loss = 0.7914, acc = 0.7795, time = 0.001s\n",
      "Epoch 249: loss = 0.7882, acc = 0.7822, time = 0.001s\n",
      "Epoch 250: loss = 0.7850, acc = 0.7832, time = 0.001s\n",
      "Epoch 251: loss = 0.7819, acc = 0.7840, time = 0.001s\n",
      "Epoch 252: loss = 0.7788, acc = 0.7846, time = 0.001s\n",
      "Epoch 253: loss = 0.7757, acc = 0.7852, time = 0.001s\n",
      "Epoch 254: loss = 0.7726, acc = 0.7870, time = 0.001s\n",
      "Epoch 255: loss = 0.7695, acc = 0.7881, time = 0.001s\n",
      "Epoch 256: loss = 0.7665, acc = 0.7894, time = 0.002s\n",
      "Epoch 257: loss = 0.7635, acc = 0.7902, time = 0.001s\n",
      "Epoch 258: loss = 0.7605, acc = 0.7908, time = 0.001s\n",
      "Epoch 259: loss = 0.7575, acc = 0.7922, time = 0.002s\n",
      "Epoch 260: loss = 0.7546, acc = 0.7937, time = 0.000s\n",
      "Epoch 261: loss = 0.7517, acc = 0.7944, time = 0.000s\n",
      "Epoch 262: loss = 0.7488, acc = 0.7957, time = 0.003s\n",
      "Epoch 263: loss = 0.7459, acc = 0.7963, time = 0.001s\n",
      "Epoch 264: loss = 0.7430, acc = 0.7971, time = 0.002s\n",
      "Epoch 265: loss = 0.7402, acc = 0.7978, time = 0.002s\n",
      "Epoch 266: loss = 0.7374, acc = 0.7986, time = 0.001s\n",
      "Epoch 267: loss = 0.7346, acc = 0.7995, time = 0.001s\n",
      "Epoch 268: loss = 0.7318, acc = 0.8008, time = 0.001s\n",
      "Epoch 269: loss = 0.7290, acc = 0.8013, time = 0.000s\n",
      "Epoch 270: loss = 0.7263, acc = 0.8013, time = 0.001s\n",
      "Epoch 271: loss = 0.7236, acc = 0.8024, time = 0.002s\n",
      "Epoch 272: loss = 0.7209, acc = 0.8030, time = 0.002s\n",
      "Epoch 273: loss = 0.7182, acc = 0.8041, time = 0.000s\n",
      "Epoch 274: loss = 0.7155, acc = 0.8044, time = 0.001s\n",
      "Epoch 275: loss = 0.7129, acc = 0.8054, time = 0.002s\n",
      "Epoch 276: loss = 0.7103, acc = 0.8059, time = 0.001s\n",
      "Epoch 277: loss = 0.7077, acc = 0.8079, time = 0.002s\n",
      "Epoch 278: loss = 0.7051, acc = 0.8083, time = 0.001s\n",
      "Epoch 279: loss = 0.7025, acc = 0.8094, time = 0.003s\n",
      "Epoch 280: loss = 0.7000, acc = 0.8102, time = 0.001s\n",
      "Epoch 281: loss = 0.6974, acc = 0.8102, time = 0.001s\n",
      "Epoch 282: loss = 0.6949, acc = 0.8108, time = 0.001s\n",
      "Epoch 283: loss = 0.6924, acc = 0.8121, time = 0.001s\n",
      "Epoch 284: loss = 0.6900, acc = 0.8125, time = 0.001s\n",
      "Epoch 285: loss = 0.6875, acc = 0.8138, time = 0.001s\n",
      "Epoch 286: loss = 0.6851, acc = 0.8146, time = 0.002s\n",
      "Epoch 287: loss = 0.6827, acc = 0.8154, time = 0.001s\n",
      "Epoch 288: loss = 0.6803, acc = 0.8167, time = 0.001s\n",
      "Epoch 289: loss = 0.6779, acc = 0.8181, time = 0.002s\n",
      "Epoch 290: loss = 0.6755, acc = 0.8183, time = 0.002s\n",
      "Epoch 291: loss = 0.6732, acc = 0.8184, time = 0.002s\n",
      "Epoch 292: loss = 0.6708, acc = 0.8194, time = 0.001s\n",
      "Epoch 293: loss = 0.6685, acc = 0.8198, time = 0.002s\n",
      "Epoch 294: loss = 0.6662, acc = 0.8211, time = 0.001s\n",
      "Epoch 295: loss = 0.6640, acc = 0.8216, time = 0.002s\n",
      "Epoch 296: loss = 0.6617, acc = 0.8221, time = 0.001s\n",
      "Epoch 297: loss = 0.6594, acc = 0.8224, time = 0.001s\n",
      "Epoch 298: loss = 0.6572, acc = 0.8235, time = 0.001s\n",
      "Epoch 299: loss = 0.6550, acc = 0.8237, time = 0.001s\n",
      "Epoch 300: loss = 0.6528, acc = 0.8241, time = 0.001s\n",
      "Epoch 301: loss = 0.6506, acc = 0.8252, time = 0.002s\n",
      "Epoch 302: loss = 0.6485, acc = 0.8259, time = 0.002s\n",
      "Epoch 303: loss = 0.6463, acc = 0.8263, time = 0.001s\n",
      "Epoch 304: loss = 0.6442, acc = 0.8263, time = 0.002s\n",
      "Epoch 305: loss = 0.6421, acc = 0.8270, time = 0.001s\n",
      "Epoch 306: loss = 0.6399, acc = 0.8276, time = 0.002s\n",
      "Epoch 307: loss = 0.6379, acc = 0.8276, time = 0.001s\n",
      "Epoch 308: loss = 0.6358, acc = 0.8279, time = 0.002s\n",
      "Epoch 309: loss = 0.6337, acc = 0.8281, time = 0.001s\n",
      "Epoch 310: loss = 0.6317, acc = 0.8286, time = 0.001s\n",
      "Epoch 311: loss = 0.6296, acc = 0.8290, time = 0.001s\n",
      "Epoch 312: loss = 0.6276, acc = 0.8303, time = 0.002s\n",
      "Epoch 313: loss = 0.6256, acc = 0.8305, time = 0.001s\n",
      "Epoch 314: loss = 0.6236, acc = 0.8308, time = 0.001s\n",
      "Epoch 315: loss = 0.6217, acc = 0.8311, time = 0.001s\n",
      "Epoch 316: loss = 0.6197, acc = 0.8314, time = 0.000s\n",
      "Epoch 317: loss = 0.6178, acc = 0.8317, time = 0.002s\n",
      "Epoch 318: loss = 0.6158, acc = 0.8317, time = 0.002s\n",
      "Epoch 319: loss = 0.6139, acc = 0.8319, time = 0.001s\n",
      "Epoch 320: loss = 0.6120, acc = 0.8324, time = 0.001s\n",
      "Epoch 321: loss = 0.6101, acc = 0.8327, time = 0.001s\n",
      "Epoch 322: loss = 0.6082, acc = 0.8333, time = 0.001s\n",
      "Epoch 323: loss = 0.6064, acc = 0.8343, time = 0.001s\n",
      "Epoch 324: loss = 0.6045, acc = 0.8348, time = 0.001s\n",
      "Epoch 325: loss = 0.6027, acc = 0.8351, time = 0.002s\n",
      "Epoch 326: loss = 0.6008, acc = 0.8360, time = 0.000s\n",
      "Epoch 327: loss = 0.5990, acc = 0.8363, time = 0.002s\n",
      "Epoch 328: loss = 0.5972, acc = 0.8365, time = 0.000s\n",
      "Epoch 329: loss = 0.5954, acc = 0.8365, time = 0.000s\n",
      "Epoch 330: loss = 0.5936, acc = 0.8367, time = 0.002s\n",
      "Epoch 331: loss = 0.5918, acc = 0.8371, time = 0.001s\n",
      "Epoch 332: loss = 0.5901, acc = 0.8375, time = 0.001s\n",
      "Epoch 333: loss = 0.5883, acc = 0.8379, time = 0.001s\n",
      "Epoch 334: loss = 0.5866, acc = 0.8386, time = 0.001s\n",
      "Epoch 335: loss = 0.5849, acc = 0.8387, time = 0.001s\n",
      "Epoch 336: loss = 0.5832, acc = 0.8390, time = 0.002s\n",
      "Epoch 337: loss = 0.5815, acc = 0.8390, time = 0.000s\n",
      "Epoch 338: loss = 0.5798, acc = 0.8392, time = 0.004s\n",
      "Epoch 339: loss = 0.5781, acc = 0.8395, time = 0.001s\n",
      "Epoch 340: loss = 0.5764, acc = 0.8398, time = 0.001s\n",
      "Epoch 341: loss = 0.5748, acc = 0.8403, time = 0.002s\n",
      "Epoch 342: loss = 0.5731, acc = 0.8406, time = 0.002s\n",
      "Epoch 343: loss = 0.5715, acc = 0.8408, time = 0.001s\n",
      "Epoch 344: loss = 0.5698, acc = 0.8413, time = 0.002s\n",
      "Epoch 345: loss = 0.5682, acc = 0.8416, time = 0.000s\n",
      "Epoch 346: loss = 0.5666, acc = 0.8416, time = 0.003s\n",
      "Epoch 347: loss = 0.5650, acc = 0.8417, time = 0.001s\n",
      "Epoch 348: loss = 0.5634, acc = 0.8421, time = 0.001s\n",
      "Epoch 349: loss = 0.5619, acc = 0.8425, time = 0.001s\n",
      "Epoch 350: loss = 0.5603, acc = 0.8429, time = 0.002s\n",
      "Epoch 351: loss = 0.5587, acc = 0.8433, time = 0.001s\n",
      "Epoch 352: loss = 0.5572, acc = 0.8433, time = 0.001s\n",
      "Epoch 353: loss = 0.5556, acc = 0.8441, time = 0.001s\n",
      "Epoch 354: loss = 0.5541, acc = 0.8448, time = 0.000s\n",
      "Epoch 355: loss = 0.5526, acc = 0.8451, time = 0.002s\n",
      "Epoch 356: loss = 0.5511, acc = 0.8454, time = 0.000s\n",
      "Epoch 357: loss = 0.5496, acc = 0.8452, time = 0.003s\n",
      "Epoch 358: loss = 0.5481, acc = 0.8454, time = 0.001s\n",
      "Epoch 359: loss = 0.5466, acc = 0.8456, time = 0.001s\n",
      "Epoch 360: loss = 0.5451, acc = 0.8460, time = 0.001s\n",
      "Epoch 361: loss = 0.5436, acc = 0.8467, time = 0.002s\n",
      "Epoch 362: loss = 0.5422, acc = 0.8470, time = 0.001s\n",
      "Epoch 363: loss = 0.5407, acc = 0.8471, time = 0.002s\n",
      "Epoch 364: loss = 0.5393, acc = 0.8475, time = 0.001s\n",
      "Epoch 365: loss = 0.5379, acc = 0.8475, time = 0.003s\n",
      "Epoch 366: loss = 0.5364, acc = 0.8479, time = 0.000s\n",
      "Epoch 367: loss = 0.5350, acc = 0.8483, time = 0.000s\n",
      "Epoch 368: loss = 0.5336, acc = 0.8483, time = 0.000s\n",
      "Epoch 369: loss = 0.5322, acc = 0.8486, time = 0.006s\n",
      "Epoch 370: loss = 0.5308, acc = 0.8490, time = 0.001s\n",
      "Epoch 371: loss = 0.5294, acc = 0.8497, time = 0.000s\n",
      "Epoch 372: loss = 0.5280, acc = 0.8500, time = 0.000s\n",
      "Epoch 373: loss = 0.5267, acc = 0.8506, time = 0.000s\n",
      "Epoch 374: loss = 0.5253, acc = 0.8506, time = 0.005s\n",
      "Epoch 375: loss = 0.5240, acc = 0.8510, time = 0.001s\n",
      "Epoch 376: loss = 0.5226, acc = 0.8511, time = 0.003s\n",
      "Epoch 377: loss = 0.5213, acc = 0.8516, time = 0.001s\n",
      "Epoch 378: loss = 0.5199, acc = 0.8519, time = 0.002s\n",
      "Epoch 379: loss = 0.5186, acc = 0.8521, time = 0.002s\n",
      "Epoch 380: loss = 0.5173, acc = 0.8524, time = 0.001s\n",
      "Epoch 381: loss = 0.5160, acc = 0.8527, time = 0.002s\n",
      "Epoch 382: loss = 0.5147, acc = 0.8529, time = 0.001s\n",
      "Epoch 383: loss = 0.5134, acc = 0.8532, time = 0.002s\n",
      "Epoch 384: loss = 0.5121, acc = 0.8535, time = 0.000s\n",
      "Epoch 385: loss = 0.5108, acc = 0.8537, time = 0.003s\n",
      "Epoch 386: loss = 0.5095, acc = 0.8538, time = 0.001s\n",
      "Epoch 387: loss = 0.5082, acc = 0.8546, time = 0.001s\n",
      "Epoch 388: loss = 0.5070, acc = 0.8546, time = 0.001s\n",
      "Epoch 389: loss = 0.5057, acc = 0.8549, time = 0.001s\n",
      "Epoch 390: loss = 0.5045, acc = 0.8551, time = 0.002s\n",
      "Epoch 391: loss = 0.5032, acc = 0.8554, time = 0.002s\n",
      "Epoch 392: loss = 0.5020, acc = 0.8557, time = 0.000s\n",
      "Epoch 393: loss = 0.5008, acc = 0.8567, time = 0.003s\n",
      "Epoch 394: loss = 0.4995, acc = 0.8568, time = 0.000s\n",
      "Epoch 395: loss = 0.4983, acc = 0.8568, time = 0.002s\n",
      "Epoch 396: loss = 0.4971, acc = 0.8570, time = 0.001s\n",
      "Epoch 397: loss = 0.4959, acc = 0.8567, time = 0.001s\n",
      "Epoch 398: loss = 0.4947, acc = 0.8570, time = 0.001s\n",
      "Epoch 399: loss = 0.4935, acc = 0.8571, time = 0.002s\n",
      "Epoch 400: loss = 0.4923, acc = 0.8571, time = 0.001s\n",
      "Epoch 401: loss = 0.4912, acc = 0.8575, time = 0.001s\n",
      "Epoch 402: loss = 0.4900, acc = 0.8576, time = 0.001s\n",
      "Epoch 403: loss = 0.4888, acc = 0.8578, time = 0.002s\n",
      "Epoch 404: loss = 0.4876, acc = 0.8581, time = 0.001s\n",
      "Epoch 405: loss = 0.4865, acc = 0.8589, time = 0.001s\n",
      "Epoch 406: loss = 0.4853, acc = 0.8589, time = 0.002s\n",
      "Epoch 407: loss = 0.4842, acc = 0.8592, time = 0.001s\n",
      "Epoch 408: loss = 0.4831, acc = 0.8595, time = 0.002s\n",
      "Epoch 409: loss = 0.4819, acc = 0.8598, time = 0.001s\n",
      "Epoch 410: loss = 0.4808, acc = 0.8598, time = 0.002s\n",
      "Epoch 411: loss = 0.4797, acc = 0.8602, time = 0.001s\n",
      "Epoch 412: loss = 0.4786, acc = 0.8605, time = 0.000s\n",
      "Epoch 413: loss = 0.4775, acc = 0.8605, time = 0.001s\n",
      "Epoch 414: loss = 0.4764, acc = 0.8611, time = 0.001s\n",
      "Epoch 415: loss = 0.4753, acc = 0.8611, time = 0.001s\n",
      "Epoch 416: loss = 0.4742, acc = 0.8617, time = 0.001s\n",
      "Epoch 417: loss = 0.4731, acc = 0.8616, time = 0.001s\n",
      "Epoch 418: loss = 0.4720, acc = 0.8622, time = 0.001s\n",
      "Epoch 419: loss = 0.4709, acc = 0.8624, time = 0.002s\n",
      "Epoch 420: loss = 0.4699, acc = 0.8633, time = 0.002s\n",
      "Epoch 421: loss = 0.4688, acc = 0.8637, time = 0.001s\n",
      "Epoch 422: loss = 0.4677, acc = 0.8643, time = 0.000s\n",
      "Epoch 423: loss = 0.4667, acc = 0.8644, time = 0.002s\n",
      "Epoch 424: loss = 0.4656, acc = 0.8646, time = 0.002s\n",
      "Epoch 425: loss = 0.4646, acc = 0.8651, time = 0.001s\n",
      "Epoch 426: loss = 0.4636, acc = 0.8654, time = 0.002s\n",
      "Epoch 427: loss = 0.4625, acc = 0.8659, time = 0.001s\n",
      "Epoch 428: loss = 0.4615, acc = 0.8660, time = 0.001s\n",
      "Epoch 429: loss = 0.4605, acc = 0.8659, time = 0.000s\n",
      "Epoch 430: loss = 0.4595, acc = 0.8659, time = 0.001s\n",
      "Epoch 431: loss = 0.4585, acc = 0.8662, time = 0.001s\n",
      "Epoch 432: loss = 0.4575, acc = 0.8667, time = 0.001s\n",
      "Epoch 433: loss = 0.4565, acc = 0.8663, time = 0.002s\n",
      "Epoch 434: loss = 0.4555, acc = 0.8665, time = 0.002s\n",
      "Epoch 435: loss = 0.4545, acc = 0.8667, time = 0.000s\n",
      "Epoch 436: loss = 0.4535, acc = 0.8668, time = 0.000s\n",
      "Epoch 437: loss = 0.4525, acc = 0.8673, time = 0.000s\n",
      "Epoch 438: loss = 0.4515, acc = 0.8675, time = 0.005s\n",
      "Epoch 439: loss = 0.4506, acc = 0.8673, time = 0.002s\n",
      "Epoch 440: loss = 0.4496, acc = 0.8675, time = 0.002s\n",
      "Epoch 441: loss = 0.4486, acc = 0.8675, time = 0.001s\n",
      "Epoch 442: loss = 0.4477, acc = 0.8675, time = 0.001s\n",
      "Epoch 443: loss = 0.4467, acc = 0.8676, time = 0.001s\n",
      "Epoch 444: loss = 0.4458, acc = 0.8681, time = 0.001s\n",
      "Epoch 445: loss = 0.4449, acc = 0.8684, time = 0.002s\n",
      "Epoch 446: loss = 0.4439, acc = 0.8686, time = 0.002s\n",
      "Epoch 447: loss = 0.4430, acc = 0.8686, time = 0.002s\n",
      "Epoch 448: loss = 0.4421, acc = 0.8687, time = 0.002s\n",
      "Epoch 449: loss = 0.4411, acc = 0.8690, time = 0.002s\n",
      "Epoch 450: loss = 0.4402, acc = 0.8694, time = 0.001s\n",
      "Epoch 451: loss = 0.4393, acc = 0.8692, time = 0.002s\n",
      "Epoch 452: loss = 0.4384, acc = 0.8692, time = 0.003s\n",
      "Epoch 453: loss = 0.4375, acc = 0.8694, time = 0.002s\n",
      "Epoch 454: loss = 0.4366, acc = 0.8697, time = 0.001s\n",
      "Epoch 455: loss = 0.4357, acc = 0.8698, time = 0.000s\n",
      "Epoch 456: loss = 0.4348, acc = 0.8700, time = 0.004s\n",
      "Epoch 457: loss = 0.4339, acc = 0.8703, time = 0.001s\n",
      "Epoch 458: loss = 0.4331, acc = 0.8702, time = 0.002s\n",
      "Epoch 459: loss = 0.4322, acc = 0.8703, time = 0.002s\n",
      "Epoch 460: loss = 0.4313, acc = 0.8703, time = 0.001s\n",
      "Epoch 461: loss = 0.4304, acc = 0.8708, time = 0.002s\n",
      "Epoch 462: loss = 0.4296, acc = 0.8708, time = 0.000s\n",
      "Epoch 463: loss = 0.4287, acc = 0.8710, time = 0.002s\n",
      "Epoch 464: loss = 0.4279, acc = 0.8708, time = 0.000s\n",
      "Epoch 465: loss = 0.4270, acc = 0.8710, time = 0.003s\n",
      "Epoch 466: loss = 0.4262, acc = 0.8721, time = 0.002s\n",
      "Epoch 467: loss = 0.4253, acc = 0.8722, time = 0.001s\n",
      "Epoch 468: loss = 0.4245, acc = 0.8724, time = 0.001s\n",
      "Epoch 469: loss = 0.4236, acc = 0.8724, time = 0.001s\n",
      "Epoch 470: loss = 0.4228, acc = 0.8725, time = 0.001s\n",
      "Epoch 471: loss = 0.4220, acc = 0.8729, time = 0.002s\n",
      "Epoch 472: loss = 0.4212, acc = 0.8732, time = 0.001s\n",
      "Epoch 473: loss = 0.4203, acc = 0.8738, time = 0.001s\n",
      "Epoch 474: loss = 0.4195, acc = 0.8740, time = 0.002s\n",
      "Epoch 475: loss = 0.4187, acc = 0.8744, time = 0.000s\n",
      "Epoch 476: loss = 0.4179, acc = 0.8748, time = 0.002s\n",
      "Epoch 477: loss = 0.4171, acc = 0.8748, time = 0.000s\n",
      "Epoch 478: loss = 0.4163, acc = 0.8748, time = 0.003s\n",
      "Epoch 479: loss = 0.4155, acc = 0.8748, time = 0.001s\n",
      "Epoch 480: loss = 0.4147, acc = 0.8749, time = 0.001s\n",
      "Epoch 481: loss = 0.4139, acc = 0.8751, time = 0.002s\n",
      "Epoch 482: loss = 0.4131, acc = 0.8751, time = 0.001s\n",
      "Epoch 483: loss = 0.4123, acc = 0.8752, time = 0.001s\n",
      "Epoch 484: loss = 0.4116, acc = 0.8751, time = 0.001s\n",
      "Epoch 485: loss = 0.4108, acc = 0.8754, time = 0.002s\n",
      "Epoch 486: loss = 0.4100, acc = 0.8757, time = 0.001s\n",
      "Epoch 487: loss = 0.4092, acc = 0.8760, time = 0.000s\n",
      "Epoch 488: loss = 0.4085, acc = 0.8760, time = 0.002s\n",
      "Epoch 489: loss = 0.4077, acc = 0.8762, time = 0.002s\n",
      "Epoch 490: loss = 0.4070, acc = 0.8765, time = 0.001s\n",
      "Epoch 491: loss = 0.4062, acc = 0.8765, time = 0.001s\n",
      "Epoch 492: loss = 0.4055, acc = 0.8765, time = 0.002s\n",
      "Epoch 493: loss = 0.4047, acc = 0.8767, time = 0.001s\n",
      "Epoch 494: loss = 0.4040, acc = 0.8768, time = 0.002s\n",
      "Epoch 495: loss = 0.4032, acc = 0.8768, time = 0.000s\n",
      "Epoch 496: loss = 0.4025, acc = 0.8768, time = 0.002s\n",
      "Epoch 497: loss = 0.4017, acc = 0.8768, time = 0.002s\n",
      "Epoch 498: loss = 0.4010, acc = 0.8771, time = 0.000s\n",
      "Epoch 499: loss = 0.4003, acc = 0.8771, time = 0.000s\n",
      "Epoch 500: loss = 0.3996, acc = 0.8775, time = 0.003s\n",
      "Epoch 501: loss = 0.3988, acc = 0.8776, time = 0.001s\n",
      "Epoch 502: loss = 0.3981, acc = 0.8779, time = 0.001s\n",
      "Epoch 503: loss = 0.3974, acc = 0.8787, time = 0.001s\n",
      "Epoch 504: loss = 0.3967, acc = 0.8789, time = 0.002s\n",
      "Epoch 505: loss = 0.3960, acc = 0.8790, time = 0.002s\n",
      "Epoch 506: loss = 0.3953, acc = 0.8794, time = 0.000s\n",
      "Epoch 507: loss = 0.3946, acc = 0.8795, time = 0.002s\n",
      "Epoch 508: loss = 0.3939, acc = 0.8795, time = 0.001s\n",
      "Epoch 509: loss = 0.3932, acc = 0.8795, time = 0.001s\n",
      "Epoch 510: loss = 0.3925, acc = 0.8802, time = 0.000s\n",
      "Epoch 511: loss = 0.3918, acc = 0.8808, time = 0.004s\n",
      "Epoch 512: loss = 0.3911, acc = 0.8808, time = 0.000s\n",
      "Epoch 513: loss = 0.3904, acc = 0.8810, time = 0.003s\n",
      "Epoch 514: loss = 0.3897, acc = 0.8811, time = 0.001s\n",
      "Epoch 515: loss = 0.3891, acc = 0.8814, time = 0.001s\n",
      "Epoch 516: loss = 0.3884, acc = 0.8814, time = 0.003s\n",
      "Epoch 517: loss = 0.3877, acc = 0.8816, time = 0.003s\n",
      "Epoch 518: loss = 0.3870, acc = 0.8816, time = 0.002s\n",
      "Epoch 519: loss = 0.3864, acc = 0.8816, time = 0.002s\n",
      "Epoch 520: loss = 0.3857, acc = 0.8819, time = 0.001s\n",
      "Epoch 521: loss = 0.3850, acc = 0.8819, time = 0.002s\n",
      "Epoch 522: loss = 0.3844, acc = 0.8821, time = 0.001s\n",
      "Epoch 523: loss = 0.3837, acc = 0.8822, time = 0.002s\n",
      "Epoch 524: loss = 0.3831, acc = 0.8822, time = 0.001s\n",
      "Epoch 525: loss = 0.3824, acc = 0.8827, time = 0.001s\n",
      "Epoch 526: loss = 0.3818, acc = 0.8827, time = 0.002s\n",
      "Epoch 527: loss = 0.3811, acc = 0.8827, time = 0.001s\n",
      "Epoch 528: loss = 0.3805, acc = 0.8829, time = 0.002s\n",
      "Epoch 529: loss = 0.3798, acc = 0.8833, time = 0.000s\n",
      "Epoch 530: loss = 0.3792, acc = 0.8832, time = 0.003s\n",
      "Epoch 531: loss = 0.3785, acc = 0.8833, time = 0.001s\n",
      "Epoch 532: loss = 0.3779, acc = 0.8841, time = 0.001s\n",
      "Epoch 533: loss = 0.3773, acc = 0.8843, time = 0.001s\n",
      "Epoch 534: loss = 0.3766, acc = 0.8843, time = 0.001s\n",
      "Epoch 535: loss = 0.3760, acc = 0.8846, time = 0.001s\n",
      "Epoch 536: loss = 0.3754, acc = 0.8848, time = 0.002s\n",
      "Epoch 537: loss = 0.3748, acc = 0.8848, time = 0.001s\n",
      "Epoch 538: loss = 0.3741, acc = 0.8848, time = 0.001s\n",
      "Epoch 539: loss = 0.3735, acc = 0.8848, time = 0.000s\n",
      "Epoch 540: loss = 0.3729, acc = 0.8846, time = 0.002s\n",
      "Epoch 541: loss = 0.3723, acc = 0.8848, time = 0.002s\n",
      "Epoch 542: loss = 0.3717, acc = 0.8849, time = 0.001s\n",
      "Epoch 543: loss = 0.3711, acc = 0.8848, time = 0.001s\n",
      "Epoch 544: loss = 0.3705, acc = 0.8848, time = 0.001s\n",
      "Epoch 545: loss = 0.3699, acc = 0.8852, time = 0.000s\n",
      "Epoch 546: loss = 0.3693, acc = 0.8854, time = 0.000s\n",
      "Epoch 547: loss = 0.3687, acc = 0.8860, time = 0.003s\n",
      "Epoch 548: loss = 0.3681, acc = 0.8867, time = 0.001s\n",
      "Epoch 549: loss = 0.3675, acc = 0.8867, time = 0.002s\n",
      "Epoch 550: loss = 0.3669, acc = 0.8867, time = 0.001s\n",
      "Epoch 551: loss = 0.3663, acc = 0.8871, time = 0.002s\n",
      "Epoch 552: loss = 0.3657, acc = 0.8873, time = 0.000s\n",
      "Epoch 553: loss = 0.3651, acc = 0.8876, time = 0.003s\n",
      "Epoch 554: loss = 0.3645, acc = 0.8878, time = 0.001s\n",
      "Epoch 555: loss = 0.3640, acc = 0.8878, time = 0.001s\n",
      "Epoch 556: loss = 0.3634, acc = 0.8879, time = 0.001s\n",
      "Epoch 557: loss = 0.3628, acc = 0.8879, time = 0.001s\n",
      "Epoch 558: loss = 0.3622, acc = 0.8879, time = 0.003s\n",
      "Epoch 559: loss = 0.3617, acc = 0.8881, time = 0.001s\n",
      "Epoch 560: loss = 0.3611, acc = 0.8881, time = 0.000s\n",
      "Epoch 561: loss = 0.3605, acc = 0.8889, time = 0.003s\n",
      "Epoch 562: loss = 0.3599, acc = 0.8887, time = 0.002s\n",
      "Epoch 563: loss = 0.3594, acc = 0.8887, time = 0.001s\n",
      "Epoch 564: loss = 0.3588, acc = 0.8889, time = 0.001s\n",
      "Epoch 565: loss = 0.3583, acc = 0.8889, time = 0.001s\n",
      "Epoch 566: loss = 0.3577, acc = 0.8889, time = 0.001s\n",
      "Epoch 567: loss = 0.3571, acc = 0.8889, time = 0.002s\n",
      "Epoch 568: loss = 0.3566, acc = 0.8890, time = 0.002s\n",
      "Epoch 569: loss = 0.3560, acc = 0.8892, time = 0.002s\n",
      "Epoch 570: loss = 0.3555, acc = 0.8894, time = 0.000s\n",
      "Epoch 571: loss = 0.3549, acc = 0.8900, time = 0.003s\n",
      "Epoch 572: loss = 0.3544, acc = 0.8902, time = 0.002s\n",
      "Epoch 573: loss = 0.3538, acc = 0.8903, time = 0.002s\n",
      "Epoch 574: loss = 0.3533, acc = 0.8903, time = 0.001s\n",
      "Epoch 575: loss = 0.3528, acc = 0.8903, time = 0.002s\n",
      "Epoch 576: loss = 0.3522, acc = 0.8908, time = 0.001s\n",
      "Epoch 577: loss = 0.3517, acc = 0.8908, time = 0.002s\n",
      "Epoch 578: loss = 0.3511, acc = 0.8908, time = 0.002s\n",
      "Epoch 579: loss = 0.3506, acc = 0.8913, time = 0.002s\n",
      "Epoch 580: loss = 0.3501, acc = 0.8916, time = 0.002s\n",
      "Epoch 581: loss = 0.3496, acc = 0.8914, time = 0.001s\n",
      "Epoch 582: loss = 0.3490, acc = 0.8914, time = 0.001s\n",
      "Epoch 583: loss = 0.3485, acc = 0.8916, time = 0.002s\n",
      "Epoch 584: loss = 0.3480, acc = 0.8917, time = 0.002s\n",
      "Epoch 585: loss = 0.3475, acc = 0.8919, time = 0.001s\n",
      "Epoch 586: loss = 0.3469, acc = 0.8924, time = 0.003s\n",
      "Epoch 587: loss = 0.3464, acc = 0.8925, time = 0.001s\n",
      "Epoch 588: loss = 0.3459, acc = 0.8927, time = 0.002s\n",
      "Epoch 589: loss = 0.3454, acc = 0.8927, time = 0.001s\n",
      "Epoch 590: loss = 0.3449, acc = 0.8929, time = 0.002s\n",
      "Epoch 591: loss = 0.3444, acc = 0.8935, time = 0.001s\n",
      "Epoch 592: loss = 0.3438, acc = 0.8940, time = 0.002s\n",
      "Epoch 593: loss = 0.3433, acc = 0.8940, time = 0.002s\n",
      "Epoch 594: loss = 0.3428, acc = 0.8940, time = 0.001s\n",
      "Epoch 595: loss = 0.3423, acc = 0.8940, time = 0.001s\n",
      "Epoch 596: loss = 0.3418, acc = 0.8946, time = 0.001s\n",
      "Epoch 597: loss = 0.3413, acc = 0.8948, time = 0.002s\n",
      "Epoch 598: loss = 0.3408, acc = 0.8948, time = 0.002s\n",
      "Epoch 599: loss = 0.3403, acc = 0.8948, time = 0.000s\n",
      "Epoch 600: loss = 0.3398, acc = 0.8948, time = 0.000s\n",
      "Epoch 601: loss = 0.3393, acc = 0.8948, time = 0.004s\n",
      "Epoch 602: loss = 0.3388, acc = 0.8948, time = 0.002s\n",
      "Epoch 603: loss = 0.3383, acc = 0.8948, time = 0.001s\n",
      "Epoch 604: loss = 0.3379, acc = 0.8949, time = 0.000s\n",
      "Epoch 605: loss = 0.3374, acc = 0.8949, time = 0.004s\n",
      "Epoch 606: loss = 0.3369, acc = 0.8952, time = 0.002s\n",
      "Epoch 607: loss = 0.3364, acc = 0.8956, time = 0.001s\n",
      "Epoch 608: loss = 0.3359, acc = 0.8956, time = 0.001s\n",
      "Epoch 609: loss = 0.3354, acc = 0.8957, time = 0.001s\n",
      "Epoch 610: loss = 0.3350, acc = 0.8959, time = 0.001s\n",
      "Epoch 611: loss = 0.3345, acc = 0.8959, time = 0.002s\n",
      "Epoch 612: loss = 0.3340, acc = 0.8960, time = 0.000s\n",
      "Epoch 613: loss = 0.3335, acc = 0.8960, time = 0.002s\n",
      "Epoch 614: loss = 0.3330, acc = 0.8960, time = 0.000s\n",
      "Epoch 615: loss = 0.3326, acc = 0.8962, time = 0.002s\n",
      "Epoch 616: loss = 0.3321, acc = 0.8962, time = 0.001s\n",
      "Epoch 617: loss = 0.3316, acc = 0.8962, time = 0.002s\n",
      "Epoch 618: loss = 0.3312, acc = 0.8962, time = 0.002s\n",
      "Epoch 619: loss = 0.3307, acc = 0.8965, time = 0.001s\n",
      "Epoch 620: loss = 0.3302, acc = 0.8967, time = 0.001s\n",
      "Epoch 621: loss = 0.3298, acc = 0.8967, time = 0.003s\n",
      "Epoch 622: loss = 0.3293, acc = 0.8967, time = 0.002s\n",
      "Epoch 623: loss = 0.3288, acc = 0.8967, time = 0.000s\n",
      "Epoch 624: loss = 0.3284, acc = 0.8968, time = 0.002s\n",
      "Epoch 625: loss = 0.3279, acc = 0.8970, time = 0.002s\n",
      "Epoch 626: loss = 0.3275, acc = 0.8976, time = 0.001s\n",
      "Epoch 627: loss = 0.3270, acc = 0.8978, time = 0.001s\n",
      "Epoch 628: loss = 0.3266, acc = 0.8978, time = 0.001s\n",
      "Epoch 629: loss = 0.3261, acc = 0.8979, time = 0.001s\n",
      "Epoch 630: loss = 0.3257, acc = 0.8979, time = 0.001s\n",
      "Epoch 631: loss = 0.3252, acc = 0.8983, time = 0.001s\n",
      "Epoch 632: loss = 0.3248, acc = 0.8984, time = 0.001s\n",
      "Epoch 633: loss = 0.3243, acc = 0.8986, time = 0.002s\n",
      "Epoch 634: loss = 0.3239, acc = 0.8987, time = 0.000s\n",
      "Epoch 635: loss = 0.3234, acc = 0.8989, time = 0.002s\n",
      "Epoch 636: loss = 0.3230, acc = 0.8990, time = 0.001s\n",
      "Epoch 637: loss = 0.3226, acc = 0.8992, time = 0.001s\n",
      "Epoch 638: loss = 0.3221, acc = 0.8994, time = 0.001s\n",
      "Epoch 639: loss = 0.3217, acc = 0.8992, time = 0.001s\n",
      "Epoch 640: loss = 0.3212, acc = 0.8997, time = 0.001s\n",
      "Epoch 641: loss = 0.3208, acc = 0.8997, time = 0.000s\n",
      "Epoch 642: loss = 0.3204, acc = 0.8998, time = 0.002s\n",
      "Epoch 643: loss = 0.3199, acc = 0.9002, time = 0.002s\n",
      "Epoch 644: loss = 0.3195, acc = 0.9002, time = 0.000s\n",
      "Epoch 645: loss = 0.3191, acc = 0.9005, time = 0.000s\n",
      "Epoch 646: loss = 0.3186, acc = 0.9006, time = 0.003s\n",
      "Epoch 647: loss = 0.3182, acc = 0.9006, time = 0.002s\n",
      "Epoch 648: loss = 0.3178, acc = 0.9008, time = 0.001s\n",
      "Epoch 649: loss = 0.3174, acc = 0.9011, time = 0.002s\n",
      "Epoch 650: loss = 0.3169, acc = 0.9011, time = 0.000s\n",
      "Epoch 651: loss = 0.3165, acc = 0.9013, time = 0.000s\n",
      "Epoch 652: loss = 0.3161, acc = 0.9013, time = 0.005s\n",
      "Epoch 653: loss = 0.3157, acc = 0.9014, time = 0.001s\n",
      "Epoch 654: loss = 0.3153, acc = 0.9014, time = 0.001s\n",
      "Epoch 655: loss = 0.3148, acc = 0.9014, time = 0.000s\n",
      "Epoch 656: loss = 0.3144, acc = 0.9016, time = 0.002s\n",
      "Epoch 657: loss = 0.3140, acc = 0.9016, time = 0.002s\n",
      "Epoch 658: loss = 0.3136, acc = 0.9016, time = 0.001s\n",
      "Epoch 659: loss = 0.3132, acc = 0.9019, time = 0.002s\n",
      "Epoch 660: loss = 0.3128, acc = 0.9021, time = 0.001s\n",
      "Epoch 661: loss = 0.3124, acc = 0.9024, time = 0.001s\n",
      "Epoch 662: loss = 0.3120, acc = 0.9025, time = 0.001s\n",
      "Epoch 663: loss = 0.3115, acc = 0.9027, time = 0.001s\n",
      "Epoch 664: loss = 0.3111, acc = 0.9029, time = 0.001s\n",
      "Epoch 665: loss = 0.3107, acc = 0.9029, time = 0.001s\n",
      "Epoch 666: loss = 0.3103, acc = 0.9030, time = 0.000s\n",
      "Epoch 667: loss = 0.3099, acc = 0.9033, time = 0.003s\n",
      "Epoch 668: loss = 0.3095, acc = 0.9035, time = 0.002s\n",
      "Epoch 669: loss = 0.3091, acc = 0.9035, time = 0.001s\n",
      "Epoch 670: loss = 0.3087, acc = 0.9035, time = 0.001s\n",
      "Epoch 671: loss = 0.3083, acc = 0.9037, time = 0.001s\n",
      "Epoch 672: loss = 0.3079, acc = 0.9037, time = 0.001s\n",
      "Epoch 673: loss = 0.3075, acc = 0.9037, time = 0.001s\n",
      "Epoch 674: loss = 0.3071, acc = 0.9037, time = 0.002s\n",
      "Epoch 675: loss = 0.3067, acc = 0.9037, time = 0.001s\n",
      "Epoch 676: loss = 0.3064, acc = 0.9038, time = 0.001s\n",
      "Epoch 677: loss = 0.3060, acc = 0.9038, time = 0.000s\n",
      "Epoch 678: loss = 0.3056, acc = 0.9040, time = 0.000s\n",
      "Epoch 679: loss = 0.3052, acc = 0.9040, time = 0.004s\n",
      "Epoch 680: loss = 0.3048, acc = 0.9040, time = 0.001s\n",
      "Epoch 681: loss = 0.3044, acc = 0.9041, time = 0.001s\n",
      "Epoch 682: loss = 0.3040, acc = 0.9041, time = 0.001s\n",
      "Epoch 683: loss = 0.3036, acc = 0.9043, time = 0.001s\n",
      "Epoch 684: loss = 0.3032, acc = 0.9046, time = 0.001s\n",
      "Epoch 685: loss = 0.3029, acc = 0.9046, time = 0.002s\n",
      "Epoch 686: loss = 0.3025, acc = 0.9046, time = 0.002s\n",
      "Epoch 687: loss = 0.3021, acc = 0.9048, time = 0.000s\n",
      "Epoch 688: loss = 0.3017, acc = 0.9052, time = 0.003s\n",
      "Epoch 689: loss = 0.3013, acc = 0.9059, time = 0.001s\n",
      "Epoch 690: loss = 0.3010, acc = 0.9060, time = 0.001s\n",
      "Epoch 691: loss = 0.3006, acc = 0.9062, time = 0.001s\n",
      "Epoch 692: loss = 0.3002, acc = 0.9063, time = 0.001s\n",
      "Epoch 693: loss = 0.2998, acc = 0.9067, time = 0.001s\n",
      "Epoch 694: loss = 0.2995, acc = 0.9067, time = 0.001s\n",
      "Epoch 695: loss = 0.2991, acc = 0.9068, time = 0.001s\n",
      "Epoch 696: loss = 0.2987, acc = 0.9068, time = 0.001s\n",
      "Epoch 697: loss = 0.2983, acc = 0.9068, time = 0.002s\n",
      "Epoch 698: loss = 0.2980, acc = 0.9071, time = 0.000s\n",
      "Epoch 699: loss = 0.2976, acc = 0.9073, time = 0.002s\n",
      "Epoch 700: loss = 0.2972, acc = 0.9073, time = 0.000s\n",
      "Epoch 701: loss = 0.2969, acc = 0.9075, time = 0.003s\n",
      "Epoch 702: loss = 0.2965, acc = 0.9079, time = 0.001s\n",
      "Epoch 703: loss = 0.2961, acc = 0.9079, time = 0.001s\n",
      "Epoch 704: loss = 0.2958, acc = 0.9083, time = 0.001s\n",
      "Epoch 705: loss = 0.2954, acc = 0.9086, time = 0.001s\n",
      "Epoch 706: loss = 0.2951, acc = 0.9086, time = 0.002s\n",
      "Epoch 707: loss = 0.2947, acc = 0.9086, time = 0.000s\n",
      "Epoch 708: loss = 0.2943, acc = 0.9089, time = 0.002s\n",
      "Epoch 709: loss = 0.2940, acc = 0.9089, time = 0.002s\n",
      "Epoch 710: loss = 0.2936, acc = 0.9089, time = 0.001s\n",
      "Epoch 711: loss = 0.2933, acc = 0.9090, time = 0.001s\n",
      "Epoch 712: loss = 0.2929, acc = 0.9090, time = 0.001s\n",
      "Epoch 713: loss = 0.2925, acc = 0.9090, time = 0.001s\n",
      "Epoch 714: loss = 0.2922, acc = 0.9092, time = 0.001s\n",
      "Epoch 715: loss = 0.2918, acc = 0.9095, time = 0.001s\n",
      "Epoch 716: loss = 0.2915, acc = 0.9094, time = 0.002s\n",
      "Epoch 717: loss = 0.2911, acc = 0.9094, time = 0.001s\n",
      "Epoch 718: loss = 0.2908, acc = 0.9095, time = 0.002s\n",
      "Epoch 719: loss = 0.2904, acc = 0.9095, time = 0.000s\n",
      "Epoch 720: loss = 0.2901, acc = 0.9098, time = 0.003s\n",
      "Epoch 721: loss = 0.2897, acc = 0.9100, time = 0.001s\n",
      "Epoch 722: loss = 0.2894, acc = 0.9102, time = 0.000s\n",
      "Epoch 723: loss = 0.2890, acc = 0.9102, time = 0.003s\n",
      "Epoch 724: loss = 0.2887, acc = 0.9102, time = 0.002s\n",
      "Epoch 725: loss = 0.2883, acc = 0.9103, time = 0.002s\n",
      "Epoch 726: loss = 0.2880, acc = 0.9103, time = 0.001s\n",
      "Epoch 727: loss = 0.2877, acc = 0.9102, time = 0.001s\n",
      "Epoch 728: loss = 0.2873, acc = 0.9102, time = 0.001s\n",
      "Epoch 729: loss = 0.2870, acc = 0.9103, time = 0.001s\n",
      "Epoch 730: loss = 0.2866, acc = 0.9103, time = 0.001s\n",
      "Epoch 731: loss = 0.2863, acc = 0.9103, time = 0.001s\n",
      "Epoch 732: loss = 0.2859, acc = 0.9103, time = 0.001s\n",
      "Epoch 733: loss = 0.2856, acc = 0.9105, time = 0.001s\n",
      "Epoch 734: loss = 0.2853, acc = 0.9105, time = 0.001s\n",
      "Epoch 735: loss = 0.2849, acc = 0.9105, time = 0.001s\n",
      "Epoch 736: loss = 0.2846, acc = 0.9105, time = 0.001s\n",
      "Epoch 737: loss = 0.2843, acc = 0.9106, time = 0.000s\n",
      "Epoch 738: loss = 0.2839, acc = 0.9106, time = 0.002s\n",
      "Epoch 739: loss = 0.2836, acc = 0.9106, time = 0.002s\n",
      "Epoch 740: loss = 0.2833, acc = 0.9110, time = 0.003s\n",
      "Epoch 741: loss = 0.2829, acc = 0.9110, time = 0.001s\n",
      "Epoch 742: loss = 0.2826, acc = 0.9108, time = 0.001s\n",
      "Epoch 743: loss = 0.2823, acc = 0.9111, time = 0.001s\n",
      "Epoch 744: loss = 0.2819, acc = 0.9113, time = 0.001s\n",
      "Epoch 745: loss = 0.2816, acc = 0.9113, time = 0.003s\n",
      "Epoch 746: loss = 0.2813, acc = 0.9114, time = 0.001s\n",
      "Epoch 747: loss = 0.2810, acc = 0.9116, time = 0.001s\n",
      "Epoch 748: loss = 0.2806, acc = 0.9116, time = 0.002s\n",
      "Epoch 749: loss = 0.2803, acc = 0.9116, time = 0.001s\n",
      "Epoch 750: loss = 0.2800, acc = 0.9116, time = 0.001s\n",
      "Epoch 751: loss = 0.2797, acc = 0.9119, time = 0.002s\n",
      "Epoch 752: loss = 0.2793, acc = 0.9119, time = 0.000s\n",
      "Epoch 753: loss = 0.2790, acc = 0.9119, time = 0.004s\n",
      "Epoch 754: loss = 0.2787, acc = 0.9121, time = 0.001s\n",
      "Epoch 755: loss = 0.2784, acc = 0.9122, time = 0.002s\n",
      "Epoch 756: loss = 0.2780, acc = 0.9125, time = 0.001s\n",
      "Epoch 757: loss = 0.2777, acc = 0.9125, time = 0.001s\n",
      "Epoch 758: loss = 0.2774, acc = 0.9125, time = 0.000s\n",
      "Epoch 759: loss = 0.2771, acc = 0.9125, time = 0.003s\n",
      "Epoch 760: loss = 0.2768, acc = 0.9127, time = 0.002s\n",
      "Epoch 761: loss = 0.2765, acc = 0.9127, time = 0.000s\n",
      "Epoch 762: loss = 0.2761, acc = 0.9127, time = 0.002s\n",
      "Epoch 763: loss = 0.2758, acc = 0.9129, time = 0.000s\n",
      "Epoch 764: loss = 0.2755, acc = 0.9125, time = 0.003s\n",
      "Epoch 765: loss = 0.2752, acc = 0.9129, time = 0.001s\n",
      "Epoch 766: loss = 0.2749, acc = 0.9129, time = 0.001s\n",
      "Epoch 767: loss = 0.2746, acc = 0.9132, time = 0.003s\n",
      "Epoch 768: loss = 0.2743, acc = 0.9132, time = 0.000s\n",
      "Epoch 769: loss = 0.2739, acc = 0.9133, time = 0.004s\n",
      "Epoch 770: loss = 0.2736, acc = 0.9135, time = 0.000s\n",
      "Epoch 771: loss = 0.2733, acc = 0.9137, time = 0.000s\n",
      "Epoch 772: loss = 0.2730, acc = 0.9138, time = 0.003s\n",
      "Epoch 773: loss = 0.2727, acc = 0.9138, time = 0.001s\n",
      "Epoch 774: loss = 0.2724, acc = 0.9138, time = 0.001s\n",
      "Epoch 775: loss = 0.2721, acc = 0.9140, time = 0.002s\n",
      "Epoch 776: loss = 0.2718, acc = 0.9140, time = 0.001s\n",
      "Epoch 777: loss = 0.2715, acc = 0.9141, time = 0.001s\n",
      "Epoch 778: loss = 0.2712, acc = 0.9146, time = 0.001s\n",
      "Epoch 779: loss = 0.2709, acc = 0.9146, time = 0.001s\n",
      "Epoch 780: loss = 0.2706, acc = 0.9149, time = 0.002s\n",
      "Epoch 781: loss = 0.2703, acc = 0.9151, time = 0.000s\n",
      "Epoch 782: loss = 0.2700, acc = 0.9156, time = 0.000s\n",
      "Epoch 783: loss = 0.2697, acc = 0.9159, time = 0.000s\n",
      "Epoch 784: loss = 0.2694, acc = 0.9160, time = 0.007s\n",
      "Epoch 785: loss = 0.2691, acc = 0.9162, time = 0.001s\n",
      "Epoch 786: loss = 0.2688, acc = 0.9163, time = 0.002s\n",
      "Epoch 787: loss = 0.2685, acc = 0.9163, time = 0.001s\n",
      "Epoch 788: loss = 0.2682, acc = 0.9163, time = 0.002s\n",
      "Epoch 789: loss = 0.2679, acc = 0.9163, time = 0.000s\n",
      "Epoch 790: loss = 0.2676, acc = 0.9165, time = 0.000s\n",
      "Epoch 791: loss = 0.2673, acc = 0.9167, time = 0.006s\n",
      "Epoch 792: loss = 0.2670, acc = 0.9167, time = 0.001s\n",
      "Epoch 793: loss = 0.2667, acc = 0.9168, time = 0.000s\n",
      "Epoch 794: loss = 0.2664, acc = 0.9171, time = 0.000s\n",
      "Epoch 795: loss = 0.2661, acc = 0.9171, time = 0.006s\n",
      "Epoch 796: loss = 0.2658, acc = 0.9171, time = 0.002s\n",
      "Epoch 797: loss = 0.2655, acc = 0.9171, time = 0.001s\n",
      "Epoch 798: loss = 0.2652, acc = 0.9170, time = 0.001s\n",
      "Epoch 799: loss = 0.2649, acc = 0.9171, time = 0.000s\n",
      "Epoch 800: loss = 0.2646, acc = 0.9176, time = 0.000s\n",
      "Epoch 801: loss = 0.2643, acc = 0.9179, time = 0.003s\n",
      "Epoch 802: loss = 0.2640, acc = 0.9179, time = 0.001s\n",
      "Epoch 803: loss = 0.2637, acc = 0.9181, time = 0.001s\n",
      "Epoch 804: loss = 0.2635, acc = 0.9181, time = 0.001s\n",
      "Epoch 805: loss = 0.2632, acc = 0.9181, time = 0.001s\n",
      "Epoch 806: loss = 0.2629, acc = 0.9184, time = 0.002s\n",
      "Epoch 807: loss = 0.2626, acc = 0.9184, time = 0.001s\n",
      "Epoch 808: loss = 0.2623, acc = 0.9184, time = 0.002s\n",
      "Epoch 809: loss = 0.2620, acc = 0.9186, time = 0.001s\n",
      "Epoch 810: loss = 0.2617, acc = 0.9189, time = 0.001s\n",
      "Epoch 811: loss = 0.2615, acc = 0.9189, time = 0.001s\n",
      "Epoch 812: loss = 0.2612, acc = 0.9190, time = 0.003s\n",
      "Epoch 813: loss = 0.2609, acc = 0.9192, time = 0.001s\n",
      "Epoch 814: loss = 0.2606, acc = 0.9190, time = 0.000s\n",
      "Epoch 815: loss = 0.2603, acc = 0.9190, time = 0.002s\n",
      "Epoch 816: loss = 0.2600, acc = 0.9190, time = 0.002s\n",
      "Epoch 817: loss = 0.2598, acc = 0.9192, time = 0.000s\n",
      "Epoch 818: loss = 0.2595, acc = 0.9192, time = 0.002s\n",
      "Epoch 819: loss = 0.2592, acc = 0.9195, time = 0.000s\n",
      "Epoch 820: loss = 0.2589, acc = 0.9195, time = 0.002s\n",
      "Epoch 821: loss = 0.2586, acc = 0.9200, time = 0.000s\n",
      "Epoch 822: loss = 0.2584, acc = 0.9202, time = 0.002s\n",
      "Epoch 823: loss = 0.2581, acc = 0.9202, time = 0.002s\n",
      "Epoch 824: loss = 0.2578, acc = 0.9202, time = 0.000s\n",
      "Epoch 825: loss = 0.2575, acc = 0.9202, time = 0.002s\n",
      "Epoch 826: loss = 0.2572, acc = 0.9203, time = 0.000s\n",
      "Epoch 827: loss = 0.2570, acc = 0.9203, time = 0.002s\n",
      "Epoch 828: loss = 0.2567, acc = 0.9203, time = 0.000s\n",
      "Epoch 829: loss = 0.2564, acc = 0.9203, time = 0.002s\n",
      "Epoch 830: loss = 0.2561, acc = 0.9203, time = 0.000s\n",
      "Epoch 831: loss = 0.2559, acc = 0.9203, time = 0.004s\n",
      "Epoch 832: loss = 0.2556, acc = 0.9205, time = 0.001s\n",
      "Epoch 833: loss = 0.2553, acc = 0.9206, time = 0.000s\n",
      "Epoch 834: loss = 0.2550, acc = 0.9206, time = 0.003s\n",
      "Epoch 835: loss = 0.2548, acc = 0.9208, time = 0.001s\n",
      "Epoch 836: loss = 0.2545, acc = 0.9208, time = 0.001s\n",
      "Epoch 837: loss = 0.2542, acc = 0.9210, time = 0.001s\n",
      "Epoch 838: loss = 0.2540, acc = 0.9211, time = 0.001s\n",
      "Epoch 839: loss = 0.2537, acc = 0.9213, time = 0.001s\n",
      "Epoch 840: loss = 0.2534, acc = 0.9214, time = 0.002s\n",
      "Epoch 841: loss = 0.2531, acc = 0.9216, time = 0.001s\n",
      "Epoch 842: loss = 0.2529, acc = 0.9217, time = 0.001s\n",
      "Epoch 843: loss = 0.2526, acc = 0.9219, time = 0.002s\n",
      "Epoch 844: loss = 0.2523, acc = 0.9219, time = 0.001s\n",
      "Epoch 845: loss = 0.2521, acc = 0.9219, time = 0.002s\n",
      "Epoch 846: loss = 0.2518, acc = 0.9221, time = 0.001s\n",
      "Epoch 847: loss = 0.2515, acc = 0.9224, time = 0.001s\n",
      "Epoch 848: loss = 0.2513, acc = 0.9224, time = 0.001s\n",
      "Epoch 849: loss = 0.2510, acc = 0.9224, time = 0.001s\n",
      "Epoch 850: loss = 0.2507, acc = 0.9224, time = 0.001s\n",
      "Epoch 851: loss = 0.2505, acc = 0.9225, time = 0.001s\n",
      "Epoch 852: loss = 0.2502, acc = 0.9227, time = 0.002s\n",
      "Epoch 853: loss = 0.2499, acc = 0.9227, time = 0.002s\n",
      "Epoch 854: loss = 0.2497, acc = 0.9230, time = 0.000s\n",
      "Epoch 855: loss = 0.2494, acc = 0.9233, time = 0.001s\n",
      "Epoch 856: loss = 0.2492, acc = 0.9233, time = 0.002s\n",
      "Epoch 857: loss = 0.2489, acc = 0.9233, time = 0.001s\n",
      "Epoch 858: loss = 0.2486, acc = 0.9237, time = 0.001s\n",
      "Epoch 859: loss = 0.2484, acc = 0.9238, time = 0.001s\n",
      "Epoch 860: loss = 0.2481, acc = 0.9240, time = 0.001s\n",
      "Epoch 861: loss = 0.2479, acc = 0.9243, time = 0.001s\n",
      "Epoch 862: loss = 0.2476, acc = 0.9243, time = 0.002s\n",
      "Epoch 863: loss = 0.2473, acc = 0.9246, time = 0.002s\n",
      "Epoch 864: loss = 0.2471, acc = 0.9248, time = 0.000s\n",
      "Epoch 865: loss = 0.2468, acc = 0.9249, time = 0.002s\n",
      "Epoch 866: loss = 0.2466, acc = 0.9252, time = 0.001s\n",
      "Epoch 867: loss = 0.2463, acc = 0.9252, time = 0.001s\n",
      "Epoch 868: loss = 0.2460, acc = 0.9252, time = 0.000s\n",
      "Epoch 869: loss = 0.2458, acc = 0.9254, time = 0.003s\n",
      "Epoch 870: loss = 0.2455, acc = 0.9257, time = 0.000s\n",
      "Epoch 871: loss = 0.2453, acc = 0.9259, time = 0.000s\n",
      "Epoch 872: loss = 0.2450, acc = 0.9257, time = 0.000s\n",
      "Epoch 873: loss = 0.2448, acc = 0.9256, time = 0.005s\n",
      "Epoch 874: loss = 0.2445, acc = 0.9256, time = 0.002s\n",
      "Epoch 875: loss = 0.2442, acc = 0.9256, time = 0.002s\n",
      "Epoch 876: loss = 0.2440, acc = 0.9256, time = 0.000s\n",
      "Epoch 877: loss = 0.2437, acc = 0.9257, time = 0.002s\n",
      "Epoch 878: loss = 0.2435, acc = 0.9259, time = 0.002s\n",
      "Epoch 879: loss = 0.2432, acc = 0.9260, time = 0.001s\n",
      "Epoch 880: loss = 0.2430, acc = 0.9262, time = 0.001s\n",
      "Epoch 881: loss = 0.2427, acc = 0.9263, time = 0.001s\n",
      "Epoch 882: loss = 0.2425, acc = 0.9265, time = 0.001s\n",
      "Epoch 883: loss = 0.2422, acc = 0.9265, time = 0.002s\n",
      "Epoch 884: loss = 0.2420, acc = 0.9265, time = 0.001s\n",
      "Epoch 885: loss = 0.2417, acc = 0.9265, time = 0.001s\n",
      "Epoch 886: loss = 0.2415, acc = 0.9265, time = 0.001s\n",
      "Epoch 887: loss = 0.2412, acc = 0.9265, time = 0.001s\n",
      "Epoch 888: loss = 0.2410, acc = 0.9267, time = 0.001s\n",
      "Epoch 889: loss = 0.2407, acc = 0.9268, time = 0.001s\n",
      "Epoch 890: loss = 0.2405, acc = 0.9268, time = 0.002s\n",
      "Epoch 891: loss = 0.2402, acc = 0.9268, time = 0.001s\n",
      "Epoch 892: loss = 0.2400, acc = 0.9271, time = 0.001s\n",
      "Epoch 893: loss = 0.2397, acc = 0.9271, time = 0.001s\n",
      "Epoch 894: loss = 0.2395, acc = 0.9271, time = 0.001s\n",
      "Epoch 895: loss = 0.2392, acc = 0.9271, time = 0.001s\n",
      "Epoch 896: loss = 0.2390, acc = 0.9273, time = 0.001s\n",
      "Epoch 897: loss = 0.2387, acc = 0.9273, time = 0.000s\n",
      "Epoch 898: loss = 0.2385, acc = 0.9276, time = 0.003s\n",
      "Epoch 899: loss = 0.2382, acc = 0.9278, time = 0.001s\n",
      "Epoch 900: loss = 0.2380, acc = 0.9278, time = 0.002s\n",
      "Epoch 901: loss = 0.2378, acc = 0.9278, time = 0.002s\n",
      "Epoch 902: loss = 0.2375, acc = 0.9278, time = 0.001s\n",
      "Epoch 903: loss = 0.2373, acc = 0.9279, time = 0.001s\n",
      "Epoch 904: loss = 0.2370, acc = 0.9281, time = 0.001s\n",
      "Epoch 905: loss = 0.2368, acc = 0.9281, time = 0.000s\n",
      "Epoch 906: loss = 0.2365, acc = 0.9281, time = 0.003s\n",
      "Epoch 907: loss = 0.2363, acc = 0.9283, time = 0.001s\n",
      "Epoch 908: loss = 0.2361, acc = 0.9283, time = 0.001s\n",
      "Epoch 909: loss = 0.2358, acc = 0.9283, time = 0.001s\n",
      "Epoch 910: loss = 0.2356, acc = 0.9284, time = 0.001s\n",
      "Epoch 911: loss = 0.2353, acc = 0.9284, time = 0.002s\n",
      "Epoch 912: loss = 0.2351, acc = 0.9286, time = 0.002s\n",
      "Epoch 913: loss = 0.2348, acc = 0.9289, time = 0.000s\n",
      "Epoch 914: loss = 0.2346, acc = 0.9290, time = 0.003s\n",
      "Epoch 915: loss = 0.2344, acc = 0.9290, time = 0.000s\n",
      "Epoch 916: loss = 0.2341, acc = 0.9290, time = 0.002s\n",
      "Epoch 917: loss = 0.2339, acc = 0.9292, time = 0.001s\n",
      "Epoch 918: loss = 0.2336, acc = 0.9290, time = 0.002s\n",
      "Epoch 919: loss = 0.2334, acc = 0.9290, time = 0.001s\n",
      "Epoch 920: loss = 0.2332, acc = 0.9290, time = 0.001s\n",
      "Epoch 921: loss = 0.2329, acc = 0.9290, time = 0.001s\n",
      "Epoch 922: loss = 0.2327, acc = 0.9292, time = 0.001s\n",
      "Epoch 923: loss = 0.2325, acc = 0.9294, time = 0.001s\n",
      "Epoch 924: loss = 0.2322, acc = 0.9297, time = 0.000s\n",
      "Epoch 925: loss = 0.2320, acc = 0.9297, time = 0.002s\n",
      "Epoch 926: loss = 0.2317, acc = 0.9298, time = 0.000s\n",
      "Epoch 927: loss = 0.2315, acc = 0.9298, time = 0.005s\n",
      "Epoch 928: loss = 0.2313, acc = 0.9298, time = 0.001s\n",
      "Epoch 929: loss = 0.2310, acc = 0.9298, time = 0.002s\n",
      "Epoch 930: loss = 0.2308, acc = 0.9298, time = 0.002s\n",
      "Epoch 931: loss = 0.2306, acc = 0.9300, time = 0.001s\n",
      "Epoch 932: loss = 0.2303, acc = 0.9302, time = 0.001s\n",
      "Epoch 933: loss = 0.2301, acc = 0.9302, time = 0.001s\n",
      "Epoch 934: loss = 0.2299, acc = 0.9300, time = 0.001s\n",
      "Epoch 935: loss = 0.2296, acc = 0.9300, time = 0.002s\n",
      "Epoch 936: loss = 0.2294, acc = 0.9302, time = 0.001s\n",
      "Epoch 937: loss = 0.2292, acc = 0.9305, time = 0.001s\n",
      "Epoch 938: loss = 0.2289, acc = 0.9305, time = 0.000s\n",
      "Epoch 939: loss = 0.2287, acc = 0.9305, time = 0.003s\n",
      "Epoch 940: loss = 0.2285, acc = 0.9305, time = 0.002s\n",
      "Epoch 941: loss = 0.2282, acc = 0.9306, time = 0.001s\n",
      "Epoch 942: loss = 0.2280, acc = 0.9306, time = 0.001s\n",
      "Epoch 943: loss = 0.2278, acc = 0.9308, time = 0.000s\n",
      "Epoch 944: loss = 0.2275, acc = 0.9308, time = 0.000s\n",
      "Epoch 945: loss = 0.2273, acc = 0.9308, time = 0.003s\n",
      "Epoch 946: loss = 0.2271, acc = 0.9308, time = 0.000s\n",
      "Epoch 947: loss = 0.2269, acc = 0.9310, time = 0.004s\n",
      "Epoch 948: loss = 0.2266, acc = 0.9311, time = 0.000s\n",
      "Epoch 949: loss = 0.2264, acc = 0.9311, time = 0.002s\n",
      "Epoch 950: loss = 0.2262, acc = 0.9311, time = 0.002s\n",
      "Epoch 951: loss = 0.2259, acc = 0.9311, time = 0.001s\n",
      "Epoch 952: loss = 0.2257, acc = 0.9313, time = 0.002s\n",
      "Epoch 953: loss = 0.2255, acc = 0.9313, time = 0.001s\n",
      "Epoch 954: loss = 0.2252, acc = 0.9313, time = 0.000s\n",
      "Epoch 955: loss = 0.2250, acc = 0.9314, time = 0.001s\n",
      "Epoch 956: loss = 0.2248, acc = 0.9314, time = 0.001s\n",
      "Epoch 957: loss = 0.2246, acc = 0.9316, time = 0.001s\n",
      "Epoch 958: loss = 0.2243, acc = 0.9316, time = 0.002s\n",
      "Epoch 959: loss = 0.2241, acc = 0.9316, time = 0.000s\n",
      "Epoch 960: loss = 0.2239, acc = 0.9316, time = 0.003s\n",
      "Epoch 961: loss = 0.2237, acc = 0.9319, time = 0.000s\n",
      "Epoch 962: loss = 0.2234, acc = 0.9317, time = 0.000s\n",
      "Epoch 963: loss = 0.2232, acc = 0.9319, time = 0.000s\n",
      "Epoch 964: loss = 0.2230, acc = 0.9321, time = 0.000s\n",
      "Epoch 965: loss = 0.2228, acc = 0.9324, time = 0.000s\n",
      "Epoch 966: loss = 0.2225, acc = 0.9324, time = 0.000s\n",
      "Epoch 967: loss = 0.2223, acc = 0.9324, time = 0.000s\n",
      "Epoch 968: loss = 0.2221, acc = 0.9324, time = 0.010s\n",
      "Epoch 969: loss = 0.2219, acc = 0.9324, time = 0.001s\n",
      "Epoch 970: loss = 0.2216, acc = 0.9325, time = 0.000s\n",
      "Epoch 971: loss = 0.2214, acc = 0.9324, time = 0.002s\n",
      "Epoch 972: loss = 0.2212, acc = 0.9325, time = 0.002s\n",
      "Epoch 973: loss = 0.2210, acc = 0.9325, time = 0.001s\n",
      "Epoch 974: loss = 0.2208, acc = 0.9325, time = 0.001s\n",
      "Epoch 975: loss = 0.2205, acc = 0.9325, time = 0.001s\n",
      "Epoch 976: loss = 0.2203, acc = 0.9325, time = 0.002s\n",
      "Epoch 977: loss = 0.2201, acc = 0.9327, time = 0.001s\n",
      "Epoch 978: loss = 0.2199, acc = 0.9327, time = 0.003s\n",
      "Epoch 979: loss = 0.2197, acc = 0.9327, time = 0.001s\n",
      "Epoch 980: loss = 0.2194, acc = 0.9330, time = 0.002s\n",
      "Epoch 981: loss = 0.2192, acc = 0.9330, time = 0.000s\n",
      "Epoch 982: loss = 0.2190, acc = 0.9330, time = 0.002s\n",
      "Epoch 983: loss = 0.2188, acc = 0.9335, time = 0.002s\n",
      "Epoch 984: loss = 0.2186, acc = 0.9337, time = 0.000s\n",
      "Epoch 985: loss = 0.2183, acc = 0.9340, time = 0.000s\n",
      "Epoch 986: loss = 0.2181, acc = 0.9340, time = 0.000s\n",
      "Epoch 987: loss = 0.2179, acc = 0.9340, time = 0.000s\n",
      "Epoch 988: loss = 0.2177, acc = 0.9341, time = 0.000s\n",
      "Epoch 989: loss = 0.2175, acc = 0.9343, time = 0.000s\n",
      "Epoch 990: loss = 0.2172, acc = 0.9343, time = 0.008s\n",
      "Epoch 991: loss = 0.2170, acc = 0.9343, time = 0.002s\n",
      "Epoch 992: loss = 0.2168, acc = 0.9344, time = 0.001s\n",
      "Epoch 993: loss = 0.2166, acc = 0.9344, time = 0.000s\n",
      "Epoch 994: loss = 0.2164, acc = 0.9344, time = 0.006s\n",
      "Epoch 995: loss = 0.2162, acc = 0.9346, time = 0.001s\n",
      "Epoch 996: loss = 0.2159, acc = 0.9348, time = 0.001s\n",
      "Epoch 997: loss = 0.2157, acc = 0.9349, time = 0.000s\n",
      "Epoch 998: loss = 0.2155, acc = 0.9348, time = 0.002s\n",
      "Epoch 999: loss = 0.2153, acc = 0.9348, time = 0.002s\n",
      "Epoch 1000: loss = 0.2151, acc = 0.9348, time = 0.001s\n",
      "best epoch: 996 Best acc: 0.934920608997345\n",
      "Best accuracy on validation split: 93.49%\n",
      "Best hyperparameters: {'lr': 0.001, 'hidden2': 50, 'hidden1': 100, 'alpha': 1e-05, 'activation': 'tanh'}\n",
      "Accuracy: 89.15%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "Xtrain, Ytrain, Xtest, Ytest, le = split_data(traindata, testdata)\n",
    "best_mlp, epoch_times = find_best_mlp(Xtrain, Ytrain, 30)\n",
    "\n",
    "best_mlp.eval()\n",
    "with torch.no_grad():\n",
    "    logits = best_mlp(Xtest)\n",
    "    y_pred = logits.argmax(dim=1)\n",
    "    acc = (y_pred == Ytest).float().mean().item()\n",
    "    pred_proba = F.softmax(logits, dim=1)\n",
    "    print(f\"Accuracy: {acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "y_true_decoded = le.inverse_transform(Ytest.cpu().numpy())\n",
    "y_pred_decoded = le.inverse_transform(y_pred.cpu().numpy())\n",
    "y_score = pred_proba.cpu().numpy()\n",
    "y_true_bin = label_binarize(Ytest.cpu().numpy(), classes=list(range(30)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = os.path.join(\"results\", f\"gtd{partition}.txt\")\n",
    "\n",
    "# Make sure the directory exists\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Write a string to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "    file.write(f\"Precision weighted: {precision_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"Recall weighted: {recall_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score weighted: {f1_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"ROCAUC Weighted: {roc_auc_score(y_true_bin, y_score, average='weighted', multi_class='ovr'):.4f}\\n\")\n",
    "\n",
    "\n",
    "    file.write(f\"Precision micro: {precision_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"Recall micro: {recall_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score micro: {f1_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"ROCAUC micro: {roc_auc_score(y_true_bin, y_score, average='micro', multi_class='ovr'):.4f}\\n\")\n",
    "\n",
    "    file.write(f\"Precision macro: {precision_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"Recall macro: {recall_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score macro: {f1_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"ROCAUC macro: {roc_auc_score(y_true_bin, y_score, average='macro', multi_class='ovr'):.4f}\\n\")\n",
    "\n",
    "with open(f\"results/epoch_logs_gtd{partition}\", \"w\") as f:\n",
    "    f.write('\\n'.join(str(x) for x in epoch_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "                          Abu Sayyaf Group (ASG)       0.93      0.92      0.93        90\n",
      "        African National Congress (South Africa)       0.99      1.00      0.99        90\n",
      "                                Al-Qaida in Iraq       0.81      0.83      0.82        90\n",
      "        Al-Qaida in the Arabian Peninsula (AQAP)       0.82      0.82      0.82        90\n",
      "                                      Al-Shabaab       0.98      0.97      0.97        90\n",
      "             Basque Fatherland and Freedom (ETA)       0.99      1.00      0.99        90\n",
      "                                      Boko Haram       0.94      0.94      0.94        90\n",
      "  Communist Party of India - Maoist (CPI-Maoist)       0.94      0.89      0.91        90\n",
      "       Corsican National Liberation Front (FLNC)       0.98      0.99      0.98        90\n",
      "                       Donetsk People's Republic       1.00      0.93      0.97        90\n",
      "Farabundo Marti National Liberation Front (FMLN)       0.79      0.93      0.85        90\n",
      "                               Fulani extremists       0.94      0.94      0.94        90\n",
      "                 Houthi extremists (Ansar Allah)       0.83      0.87      0.85        90\n",
      "                     Irish Republican Army (IRA)       0.97      1.00      0.98        90\n",
      "     Islamic State of Iraq and the Levant (ISIL)       0.78      0.71      0.74        90\n",
      "                  Kurdistan Workers' Party (PKK)       0.84      0.94      0.89        90\n",
      "         Liberation Tigers of Tamil Eelam (LTTE)       0.97      1.00      0.98        90\n",
      "         Manuel Rodriguez Patriotic Front (FPMR)       1.00      1.00      1.00        90\n",
      "                                         Maoists       0.88      0.93      0.90        90\n",
      "                               Muslim extremists       0.84      0.76      0.80        90\n",
      "      National Liberation Army of Colombia (ELN)       0.87      0.83      0.85        90\n",
      "                         New People's Army (NPA)       0.89      0.94      0.92        90\n",
      "               Nicaraguan Democratic Force (FDN)       0.89      0.73      0.80        90\n",
      "                                    Palestinians       0.99      0.91      0.95        90\n",
      "   Revolutionary Armed Forces of Colombia (FARC)       0.85      0.89      0.87        90\n",
      "                               Shining Path (SL)       0.91      0.81      0.86        90\n",
      "                                 Sikh Extremists       0.86      0.89      0.87        90\n",
      "                                         Taliban       0.76      0.76      0.76        90\n",
      "                 Tehrik-i-Taliban Pakistan (TTP)       0.71      0.67      0.69        90\n",
      "       Tupac Amaru Revolutionary Movement (MRTA)       0.83      0.92      0.87        90\n",
      "\n",
      "                                        accuracy                           0.89      2700\n",
      "                                       macro avg       0.89      0.89      0.89      2700\n",
      "                                    weighted avg       0.89      0.89      0.89      2700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_decoded, y_pred_decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=100, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=50, out_features=30, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(cm_normalized,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels,\n",
    "                cmap=\"viridis\",\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "    plt.title(f\"Normalized Confusion Matrix (Partition {partition})\", fontsize=18)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "    plt.ylabel(\"True Label\", fontsize=14)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = f\"results/confusion_matrix_partition_{partition}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved confusion matrix for partition {partition} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved confusion matrix for partition 300 to results/confusion_matrix_partition_300.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all unique class labels from the truths\n",
    "class_labels = np.unique(y_true_decoded)\n",
    "\n",
    "plot_confusion_matrix(y_true_decoded, y_pred_decoded, labels=class_labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
