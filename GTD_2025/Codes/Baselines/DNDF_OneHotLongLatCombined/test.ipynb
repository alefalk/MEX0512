{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../../data/top30groups/engineered_dfs/df_top30_100.csv'\n",
    "data = pd.read_csv(path, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_geo_data(df):\n",
    "    # Convert to datetime and sort\n",
    "    df['attack_date'] = pd.to_datetime({'year': df['iyear'], 'month': df['imonth'], 'day': df['iday']})\n",
    "    df.sort_values(by=['longitude', 'latitude', 'attack_date'], inplace=True)\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    df = df.drop(columns=['Unnamed: 0', 'country', 'city', 'region', 'provstate', 'natlty1', 'specificity', 'iyear', 'imonth', 'iday'])\n",
    "    \n",
    "\n",
    "    df.sort_values(by=['gname', 'attack_date'], inplace=True)\n",
    "\n",
    "    df = df.drop(columns='attack_date')\n",
    "    print(\"KOLUMNER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\", df.columns)\n",
    "    # Drop duplicates based on location, keep the earliest attack\n",
    "    #df_unique = df.drop_duplicates(subset=['longitude', 'latitude'], keep='first').reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_longlat(df):\n",
    "    # Step 1: Create new 'longlat' feature as tuple\n",
    "    df['longlat'] = list(zip(df['longitude'], df['latitude']))\n",
    "\n",
    "    # Step 2: One-hot encode the new 'longlat' feature\n",
    "    longlat_encoded = pd.get_dummies(df['longlat'], prefix='loc')\n",
    "\n",
    "    # Step 3: Drop original longitude and latitude\n",
    "    df = df.drop(columns=['longitude', 'latitude', 'longlat'])\n",
    "\n",
    "    # Step 4: Concatenate the one-hot encoded features\n",
    "    df = pd.concat([df, longlat_encoded], axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before dropping long/lat duplicates:  (3000, 26)\n",
      "KOLUMNER!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Index(['extended', 'latitude', 'longitude', 'vicinity', 'multiple', 'success',\n",
      "       'suicide', 'attacktype1', 'targtype1', 'target1', 'individual',\n",
      "       'weaptype1', 'nkill', 'property', 'ishostkid', 'gname'],\n",
      "      dtype='object')\n",
      "Entries after dropping long/lat duplicates (#Nodes):  (3000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to only contain unique coordinates\n",
    "print(\"Entries before dropping long/lat duplicates: \", data.shape)\n",
    "df_unique_geo = create_unique_geo_data(data)\n",
    "print(\"Entries after dropping long/lat duplicates (#Nodes): \", df_unique_geo.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1804)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique_geo = encode_longlat(df_unique_geo)\n",
    "df_unique_geo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# creates train and test data, first 70% of each group is added to train and remaining 30% to test\n",
    "def handle_leakage(df):\n",
    "    train_frames = []\n",
    "    test_frames = []\n",
    "\n",
    "    split_point = int(len(df) * 0.7)  # 70% for training\n",
    "    train_df = df[:split_point]\n",
    "    test_df = df[split_point:]          \n",
    "\n",
    "    # Shuffle each DataFrame separately\n",
    "    train_df = shuffle(train_df)\n",
    "    test_df = shuffle(test_df)\n",
    "\n",
    "       # Identify numeric columns (excluding label columns like 'gname')\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Scale train data\n",
    "    scaler = StandardScaler()\n",
    "    train_df[numeric_cols] = scaler.fit_transform(train_df[numeric_cols])\n",
    "\n",
    "    # Apply same scaler to test data\n",
    "    test_df[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
    "\n",
    "    print(train_df.shape)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 1804)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = handle_leakage(df_unique_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 1804)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gname'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "non_numeric_cols = train_df.select_dtypes(exclude=['number', 'bool']).columns\n",
    "print(non_numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gname'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "non_numeric_cols = test_df.select_dtypes(exclude=['number', 'bool']).columns\n",
    "print(non_numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"LongLatCombined/train100.csv\")\n",
    "test_df.to_csv(\"LongLatCombined/test100.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
