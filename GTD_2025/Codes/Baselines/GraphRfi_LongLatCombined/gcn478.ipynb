{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad5ec3d",
   "metadata": {},
   "source": [
    "# Import and Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c345e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')  # Add current directory to path\n",
    "from build_graph_and_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a4eb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cfa874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../../../data/top30groups/LongLatCombined/combined/combined{partition}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36923c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Columns to exclude from scaling\n",
    "exclude_cols = ['gname', 'longitude', 'latitude']\n",
    "\n",
    "# Columns to scale\n",
    "scale_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Scale only selected columns\n",
    "scaler = StandardScaler()\n",
    "df[scale_cols] = scaler.fit_transform(df[scale_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0360bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if not os.path.isdir(f\"Results{partition}\"):\n",
    "    os.mkdir(f\"Results{partition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b6998",
   "metadata": {},
   "source": [
    "# Create longlat feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f55dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata = ['longitude', 'latitude']\n",
    "combined_geo = df.copy()\n",
    "combined_geo['longlat'] = list(zip(df['longitude'], df['latitude']))\n",
    "combined_geo = combined_geo.drop(columns=geodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f94e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def to_tuple_if_needed(val):\n",
    "    if isinstance(val, str):\n",
    "        return ast.literal_eval(val)\n",
    "    return val  # already a tuple\n",
    "\n",
    "combined_geo['longlat'] = combined_geo['longlat'].apply(to_tuple_if_needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3707b0",
   "metadata": {},
   "source": [
    "# Weapon type prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41cc634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/MEX0512/GTD_2025/Codes/Baselines/GraphRfi_OneHotLongLat/build_graph_and_train.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_nrf = torch.tensor(y_nrf, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":844, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m      6\u001b[0m data, y_gcn, y_nrf, non_geo_features, train_mask, test_mask, row_to_node_index, index_to_label \u001b[38;5;241m=\u001b[39m build_graph_data(combined_geo, label_index, continuous_col\u001b[38;5;241m=\u001b[39mcontinuous_cols[i])\n\u001b[1;32m      7\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgtd\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membed_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(label_index)\n\u001b[1;32m     17\u001b[0m }\n\u001b[0;32m---> 18\u001b[0m best_acc, best_epoch, best_precision, best_recall, best_f1, y_pred_decoded, y_true_decoded \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_joint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_gcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_nrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_geo_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_to_node_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_to_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m y_preds\u001b[38;5;241m.\u001b[39mappend(y_pred_decoded)\n\u001b[1;32m     20\u001b[0m y_trues\u001b[38;5;241m.\u001b[39mappend(y_true_decoded)\n",
      "File \u001b[0;32m~/MEX0512/GTD_2025/Codes/Baselines/GraphRfi_OneHotLongLat/build_graph_and_train.py:151\u001b[0m, in \u001b[0;36mtrain_joint\u001b[0;34m(data, edge_index, y_gcn, y_nrf, non_geo_features, train_mask, test_mask, args, row_to_node_index, index_to_label)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Compute both losses\u001b[39;00m\n\u001b[1;32m    150\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m mse_loss(pred[train_mask], y_gcn[train_mask])\n\u001b[0;32m--> 151\u001b[0m out_forest \u001b[38;5;241m=\u001b[39m \u001b[43mneural_forest\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m neural_forest\u001b[38;5;241m.\u001b[39mloss(out_forest[row_train_mask], y_nrf[row_train_mask])\n\u001b[1;32m    154\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss1 \u001b[38;5;241m+\u001b[39m loss2\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/MEX0512/GTD_2025/Codes/Baselines/GraphRfi_OneHotLongLat/ndf.py:233\u001b[0m, in \u001b[0;36mNeuralDecisionForest.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_layer(x)\n\u001b[1;32m    232\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 233\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/MEX0512/GTD_2025/Codes/Baselines/GraphRfi_OneHotLongLat/ndf.py:216\u001b[0m, in \u001b[0;36mForest.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m probs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees:\n\u001b[0;32m--> 216\u001b[0m     mu \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     p\u001b[38;5;241m=\u001b[39mtree\u001b[38;5;241m.\u001b[39mcal_prob(mu,tree\u001b[38;5;241m.\u001b[39mget_pi())\n\u001b[1;32m    218\u001b[0m     probs\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/MEX0512/GTD_2025/Codes/Baselines/GraphRfi_OneHotLongLat/ndf.py:163\u001b[0m, in \u001b[0;36mTree.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m decision \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(decision,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    162\u001b[0m decision_comp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mdecision\n\u001b[0;32m--> 163\u001b[0m decision \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecision_comp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# -> [batch_size,n_leaf,2]\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# compute route probability\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# note: we do not use decision[:,0]\u001b[39;00m\n\u001b[1;32m    167\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":844, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "label_index = {g: i for i, g in enumerate(sorted(df['gname'].unique()))}\n",
    "continuous_cols = ['weaptype1', 'nkill', 'targtype1', 'attacktype1']\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "logs = []\n",
    "for i in range(len(continuous_cols)):\n",
    "    data, y_gcn, y_nrf, non_geo_features, train_mask, test_mask, row_to_node_index, index_to_label = build_graph_data(combined_geo, label_index, continuous_col=continuous_cols[i])\n",
    "    args = {\n",
    "        'partition': f\"gtd{partition}\",\n",
    "        'embed_dim': 16,\n",
    "        'lr': 0.01,\n",
    "        'epochs': 300,\n",
    "        'feat_dropout': 0,\n",
    "        'n_tree': 80,\n",
    "        'tree_depth': 10,\n",
    "        'tree_feature_rate': 0.5,\n",
    "        'n_class': len(label_index)\n",
    "    }\n",
    "    best_acc, best_epoch, best_precision, best_recall, best_f1, y_pred_decoded, y_true_decoded, best_precision_micro, best_recall_micro, best_f1_micro, best_precision_macro, best_recall_macro, best_f1_macro, roc_auc_weighted, roc_auc_micro, roc_auc_macro, epoch_logs\n",
    "    y_preds.append(y_pred_decoded)\n",
    "    y_trues.append(y_true_decoded)\n",
    "    logs.append(epoch_logs)\n",
    "    \n",
    "    with open(f\"Results{partition}/Results_{continuous_cols[i]}_prediction\", \"w\") as f:\n",
    "        f.write(f\"Best acc: {best_acc} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best recall weighted: {best_recall} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best precision weighted: {best_precision} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best f1 weighted: {best_f1} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best recall macro: {best_recall_macro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best precision macro: {best_precision_macro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best f1 macro: {best_f1_macro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best recall micro: {best_recall_micro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best precision micro: {best_precision_micro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best f1 micro: {best_f1_micro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "\n",
    "        f.write(f\"Best auroc weighted: {roc_auc_weighted} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best auroc micro: {roc_auc_micro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best auroc macro: {roc_auc_macro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_pred_decoded, y_true_decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e99383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels, continuous_col):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(cm_normalized,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels,\n",
    "                cmap=\"viridis\",\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "    plt.title(f\"Normalized Confusion Matrix\", fontsize=18)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "    plt.ylabel(\"True Label\", fontsize=14)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = f\"Results{partition}/cm_{partition}_{continuous_col}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved confusion matrix for partition {partition} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bd1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved confusion matrix for partition 100 to Results100/cm_100_weaptype1.png\n",
      "Saved confusion matrix for partition 100 to Results100/cm_100_nkill.png\n",
      "Saved confusion matrix for partition 100 to Results100/cm_100_targtype1.png\n",
      "Saved confusion matrix for partition 100 to Results100/cm_100_attacktype1.png\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(continuous_cols)):\n",
    "    plot_confusion_matrix(y_preds[i], y_trues[i], sorted(df['gname'].unique()), continuous_cols[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
