{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad5ec3d",
   "metadata": {},
   "source": [
    "# Import and Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c345e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')  # Add current directory to path\n",
    "from build_graph_and_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a4eb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cfa874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../../../data/top30groups/LongLatCombined/combined/combined{partition}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36923c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Columns to exclude from scaling\n",
    "exclude_cols = ['gname', 'longitude', 'latitude']\n",
    "\n",
    "# Columns to scale\n",
    "scale_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Scale only selected columns\n",
    "scaler = StandardScaler()\n",
    "df[scale_cols] = scaler.fit_transform(df[scale_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0360bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if not os.path.isdir(f\"Results{partition}\"):\n",
    "    os.mkdir(f\"Results{partition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b6998",
   "metadata": {},
   "source": [
    "# Create longlat feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f55dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata = ['longitude', 'latitude']\n",
    "combined_geo = df.copy()\n",
    "combined_geo['longlat'] = list(zip(df['longitude'], df['latitude']))\n",
    "combined_geo = combined_geo.drop(columns=geodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f94e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def to_tuple_if_needed(val):\n",
    "    if isinstance(val, str):\n",
    "        return ast.literal_eval(val)\n",
    "    return val  # already a tuple\n",
    "\n",
    "combined_geo['longlat'] = combined_geo['longlat'].apply(to_tuple_if_needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3707b0",
   "metadata": {},
   "source": [
    "# Weapon type prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41cc634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/MEX0512/GTD_2025/Codes/Baselines/GraphRfi_OneHotLongLat/build_graph_and_train.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_nrf = torch.tensor(y_nrf, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | GCN MSE Loss: 2104.2805 | NRF Loss: 3.4030 | JOINT Loss: 2107.6836 | NRF Acc: 0.0293\n",
      "Epoch 02 | GCN MSE Loss: 1768.4542 | NRF Loss: 3.4039 | JOINT Loss: 1771.8582 | NRF Acc: 0.0383\n",
      "Epoch 03 | GCN MSE Loss: 1470.0892 | NRF Loss: 3.4091 | JOINT Loss: 1473.4983 | NRF Acc: 0.0402\n",
      "Epoch 04 | GCN MSE Loss: 1208.9305 | NRF Loss: 3.4093 | JOINT Loss: 1212.3398 | NRF Acc: 0.0490\n",
      "Epoch 05 | GCN MSE Loss: 980.3608 | NRF Loss: 3.4098 | JOINT Loss: 983.7706 | NRF Acc: 0.0213\n",
      "Epoch 06 | GCN MSE Loss: 782.0106 | NRF Loss: 3.4098 | JOINT Loss: 785.4204 | NRF Acc: 0.0246\n",
      "Epoch 07 | GCN MSE Loss: 612.3447 | NRF Loss: 3.4092 | JOINT Loss: 615.7540 | NRF Acc: 0.0209\n",
      "Epoch 08 | GCN MSE Loss: 469.3331 | NRF Loss: 3.4085 | JOINT Loss: 472.7415 | NRF Acc: 0.0209\n",
      "Epoch 09 | GCN MSE Loss: 351.1528 | NRF Loss: 3.4088 | JOINT Loss: 354.5616 | NRF Acc: 0.0209\n",
      "Epoch 10 | GCN MSE Loss: 255.3910 | NRF Loss: 3.4075 | JOINT Loss: 258.7986 | NRF Acc: 0.0211\n",
      "Epoch 11 | GCN MSE Loss: 179.6004 | NRF Loss: 3.4051 | JOINT Loss: 183.0055 | NRF Acc: 0.0213\n",
      "Epoch 12 | GCN MSE Loss: 121.2026 | NRF Loss: 3.4042 | JOINT Loss: 124.6068 | NRF Acc: 0.0215\n",
      "Epoch 13 | GCN MSE Loss: 77.9775 | NRF Loss: 3.4014 | JOINT Loss: 81.3789 | NRF Acc: 0.0215\n",
      "Epoch 14 | GCN MSE Loss: 47.8153 | NRF Loss: 3.4006 | JOINT Loss: 51.2158 | NRF Acc: 0.0241\n",
      "Epoch 15 | GCN MSE Loss: 28.5953 | NRF Loss: 3.3989 | JOINT Loss: 31.9943 | NRF Acc: 0.0246\n",
      "Epoch 16 | GCN MSE Loss: 18.3521 | NRF Loss: 3.3951 | JOINT Loss: 21.7472 | NRF Acc: 0.0275\n",
      "Epoch 17 | GCN MSE Loss: 15.2356 | NRF Loss: 3.3945 | JOINT Loss: 18.6301 | NRF Acc: 0.0324\n",
      "Epoch 18 | GCN MSE Loss: 17.4374 | NRF Loss: 3.3939 | JOINT Loss: 20.8313 | NRF Acc: 0.0400\n",
      "Epoch 19 | GCN MSE Loss: 23.2655 | NRF Loss: 3.3932 | JOINT Loss: 26.6587 | NRF Acc: 0.0450\n",
      "Epoch 20 | GCN MSE Loss: 31.1692 | NRF Loss: 3.3899 | JOINT Loss: 34.5591 | NRF Acc: 0.0493\n",
      "Epoch 21 | GCN MSE Loss: 39.7798 | NRF Loss: 3.3857 | JOINT Loss: 43.1655 | NRF Acc: 0.0485\n",
      "Epoch 22 | GCN MSE Loss: 47.9792 | NRF Loss: 3.3805 | JOINT Loss: 51.3597 | NRF Acc: 0.0483\n",
      "Epoch 23 | GCN MSE Loss: 54.9137 | NRF Loss: 3.3751 | JOINT Loss: 58.2888 | NRF Acc: 0.0485\n"
     ]
    }
   ],
   "source": [
    "label_index = {g: i for i, g in enumerate(sorted(df['gname'].unique()))}\n",
    "continuous_cols = ['weaptype1', 'nkill', 'targtype1', 'attacktype1']\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "logs = []\n",
    "for i in range(len(continuous_cols)):\n",
    "    data, y_gcn, y_nrf, non_geo_features, train_mask, test_mask, row_to_node_index, index_to_label = build_graph_data(combined_geo, label_index, continuous_col=continuous_cols[i])\n",
    "    args = {\n",
    "        'partition': f\"gtd{partition}\",\n",
    "        'embed_dim': 16,\n",
    "        'lr': 0.01,\n",
    "        'epochs': 300,\n",
    "        'feat_dropout': 0,\n",
    "        'n_tree': 80,\n",
    "        'tree_depth': 10,\n",
    "        'tree_feature_rate': 0.5,\n",
    "        'n_class': len(label_index)\n",
    "    }\n",
    "    best_acc, best_epoch, best_precision, best_recall, best_f1, y_pred_decoded, y_true_decoded, best_precision_micro, best_recall_micro, best_f1_micro, best_precision_macro, best_recall_macro, best_f1_macro, roc_auc_weighted, roc_auc_micro, roc_auc_macro, epoch_logs\n",
    "    y_preds.append(y_pred_decoded)\n",
    "    y_trues.append(y_true_decoded)\n",
    "    logs.append(epoch_logs)\n",
    "    \n",
    "    with open(f\"Results{partition}/Results_{continuous_cols[i]}_prediction\", \"w\") as f:\n",
    "        f.write(f\"Best acc: {best_acc} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best recall weighted: {best_recall} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best precision weighted: {best_precision} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best f1 weighted: {best_f1} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best recall macro: {best_recall_macro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best precision macro: {best_precision_macro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best f1 macro: {best_f1_macro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best recall micro: {best_recall_micro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best precision micro: {best_precision_micro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best f1 micro: {best_f1_micro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "\n",
    "        f.write(f\"Best auroc weighted: {roc_auc_weighted} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best auroc micro: {roc_auc_micro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")\n",
    "        f.write(f\"Best auroc macro: {roc_auc_macro} in epoch {best_epoch} for {continuous_cols[i]} prediction\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ac09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_pred_decoded, y_true_decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e99383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels, continuous_col):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(cm_normalized,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels,\n",
    "                cmap=\"viridis\",\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "    plt.title(f\"Normalized Confusion Matrix\", fontsize=18)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "    plt.ylabel(\"True Label\", fontsize=14)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = f\"Results{partition}/cm_{partition}_{continuous_col}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved confusion matrix for partition {partition} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bd1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved confusion matrix for partition 100 to Results100/cm_100_weaptype1.png\n",
      "Saved confusion matrix for partition 100 to Results100/cm_100_nkill.png\n",
      "Saved confusion matrix for partition 100 to Results100/cm_100_targtype1.png\n",
      "Saved confusion matrix for partition 100 to Results100/cm_100_attacktype1.png\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(continuous_cols)):\n",
    "    plot_confusion_matrix(y_preds[i], y_trues[i], sorted(df['gname'].unique()), continuous_cols[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
