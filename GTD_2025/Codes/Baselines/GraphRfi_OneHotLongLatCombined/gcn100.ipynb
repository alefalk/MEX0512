{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad5ec3d",
   "metadata": {},
   "source": [
    "# Import and Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c345e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('.')  # Add current directory to path\n",
    "from build_graph_and_train import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4eb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24cfa874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../../../data/top30groups/LongLatCombined/combined/combined{partition}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36923c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Columns to exclude from scaling\n",
    "exclude_cols = ['gname', 'longitude', 'latitude']\n",
    "\n",
    "# Columns to scale\n",
    "scale_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Scale only selected columns\n",
    "scaler = StandardScaler()\n",
    "df[scale_cols] = scaler.fit_transform(df[scale_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0360bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "if not os.path.isdir(f\"Results{partition}\"):\n",
    "    os.mkdir(f\"Results{partition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b6998",
   "metadata": {},
   "source": [
    "# Create longlat feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f55dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata = ['longitude', 'latitude']\n",
    "combined_geo = df.copy()\n",
    "combined_geo['longlat'] = list(zip(df['longitude'], df['latitude']))\n",
    "combined_geo = combined_geo.drop(columns=geodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f94e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def to_tuple_if_needed(val):\n",
    "    if isinstance(val, str):\n",
    "        return ast.literal_eval(val)\n",
    "    return val  # already a tuple\n",
    "\n",
    "combined_geo['longlat'] = combined_geo['longlat'].apply(to_tuple_if_needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3707b0",
   "metadata": {},
   "source": [
    "# Weapon type prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a90beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41cc634b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for weaptype1 prediction...\n",
      "Number of features in NRF input: 1803\n"
     ]
    }
   ],
   "source": [
    "label_index = {g: i for i, g in enumerate(sorted(df['gname'].unique()))}\n",
    "continuous_cols = ['weaptype1', 'nkill', 'targtype1', 'attacktype1']\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "logs = []\n",
    "\n",
    "# Default config (from previous best)\n",
    "default_args = {\n",
    "    'partition': f\"gtd{partition}\",\n",
    "    'embed_dim': 16,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 1000,\n",
    "    'feat_dropout': 0,\n",
    "    'n_tree': 80,\n",
    "    'tree_depth': 10,\n",
    "    'tree_feature_rate': 0.5,\n",
    "    'n_class': len(label_index)\n",
    "}\n",
    "\n",
    "for col in continuous_cols:\n",
    "    print(f\"\\nTraining model for {col} prediction...\")\n",
    "\n",
    "    data, y_gcn, y_nrf, non_geo_features, train_mask, test_mask, row_to_node_index, index_to_label = build_graph_data(\n",
    "        combined_geo, label_index, continuous_col=col)\n",
    "\n",
    "    # Train using default parameters\n",
    "    best_acc, best_epoch, best_precision, best_recall, best_f1, y_pred_decoded, y_true_decoded, \\\n",
    "    best_precision_micro, best_recall_micro, best_f1_micro, best_precision_macro, best_recall_macro, best_f1_macro, \\\n",
    "    roc_auc_weighted, roc_auc_micro, roc_auc_macro, epoch_logs = train_joint(\n",
    "        data, data.edge_index, y_gcn, y_nrf, non_geo_features, train_mask, test_mask,\n",
    "        default_args, row_to_node_index, index_to_label, verbose=True)\n",
    "\n",
    "    y_preds.append(y_pred_decoded)\n",
    "    y_trues.append(y_true_decoded)\n",
    "    logs.append(epoch_logs)\n",
    "\n",
    "    # Save performance metrics\n",
    "    results_path = f\"Results{partition}/Results_{col}_prediction\"\n",
    "    with open(results_path, \"w\") as f:\n",
    "        f.write(f\"Best acc: {best_acc:.4f} at epoch {best_epoch} for {col} prediction\\n\")\n",
    "        f.write(f\"Weighted Precision: {best_precision:.4f}, Recall: {best_recall:.4f}, F1: {best_f1:.4f}\\n\")\n",
    "        f.write(f\"Macro Precision: {best_precision_macro:.4f}, Recall: {best_recall_macro:.4f}, F1: {best_f1_macro:.4f}\\n\")\n",
    "        f.write(f\"Micro Precision: {best_precision_micro:.4f}, Recall: {best_recall_micro:.4f}, F1: {best_f1_micro:.4f}\\n\")\n",
    "        f.write(f\"AUROC Weighted: {roc_auc_weighted:.4f}, Micro: {roc_auc_micro:.4f}, Macro: {roc_auc_macro:.4f}\\n\")\n",
    "\n",
    "    # Save epoch timings\n",
    "    log_path = f\"Results{partition}/epoch_logs_{col}_prediction\"\n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write('\\n'.join(f\"{x:.4f}\" for x in epoch_logs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f65c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_pred_decoded, y_true_decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e99383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels, continuous_col):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(cm_normalized,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels,\n",
    "                cmap=\"viridis\",\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "    plt.title(f\"Normalized Confusion Matrix\", fontsize=18)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "    plt.ylabel(\"True Label\", fontsize=14)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = f\"Results{partition}/cm_{partition}_{continuous_col}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved confusion matrix for partition {partition} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bd1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(continuous_cols)):\n",
    "    plot_confusion_matrix(y_preds[i], y_trues[i], sorted(df['gname'].unique()), continuous_cols[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
