{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec742928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c514401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('top30groups/df_top30_100.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a5acda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6eedd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')  # Add current directory to path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e17f94c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries before dropping long/lat duplicates:  3000\n",
      "Entries after dropping long/lat duplicates (#Nodes):  1782\n"
     ]
    }
   ],
   "source": [
    "from graphrfi_gtd import *\n",
    "\n",
    "# Filter dataset to only contain unique coordinates\n",
    "print(\"Entries before dropping long/lat duplicates: \", len(data))\n",
    "df_unique_geo = create_unique_geo_data(data)\n",
    "print(\"Entries after dropping long/lat duplicates (#Nodes): \", len(df_unique_geo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c21d1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates train and test data, first 70% of each group is added to train and remaining 30% to test\n",
    "def handle_leakage(df):\n",
    "    train_frames = []\n",
    "    test_frames = []\n",
    "\n",
    "    #first 70% of each groups attacks to training set, remainin 30% to testing set\n",
    "    for _, group_data in df.groupby('gname'):\n",
    "        split_point = int(len(group_data) * 0.7)  # 70% for training\n",
    "        train_frames.append(group_data.iloc[:split_point])\n",
    "        test_frames.append(group_data.iloc[split_point:])           \n",
    "\n",
    "\n",
    "    # Concatenate all the group-specific splits into final train and test DataFrames\n",
    "    train_df = pd.concat(train_frames)\n",
    "    test_df = pd.concat(test_frames)\n",
    "\n",
    "    # Shuffle each DataFrame separately\n",
    "    train_df = shuffle(train_df)\n",
    "    test_df = shuffle(test_df)\n",
    "\n",
    "    print(len(train_df))\n",
    "    print(len(test_df))\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8922d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233\n",
      "549\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = handle_leakage(df_unique_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b2b24d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total nodes (unique coordinates): 1782\n",
      "Number of unique labels in this set: 30\n"
     ]
    }
   ],
   "source": [
    "# 1. Build coord_to_index from the full dataset (unique coordinate to node index mapping)\n",
    "full_coords = df_unique_geo[['longitude', 'latitude']]\n",
    "coord_to_index = {(row['longitude'], row['latitude']): i for i, row in full_coords.iterrows()}\n",
    "\n",
    "# 2. Build the global graph from the full dataset (used for both train and test)\n",
    "adj_matrix, feature_matrix, label_index = build_graph_data(df_unique_geo, coord_to_index)\n",
    "\n",
    "train_nodes = []\n",
    "train_labels = []\n",
    "for _, row in train_df.iterrows():\n",
    "    train_nodes.append(coord_to_index[(row['longitude'], row['latitude'])])\n",
    "    train_labels.append(label_index[row['gname']])\n",
    "\n",
    "test_nodes = []\n",
    "test_labels = []\n",
    "for _, row in test_df.iterrows():\n",
    "    test_nodes.append(coord_to_index[(row['longitude'], row['latitude'])])\n",
    "    test_labels.append(label_index[row['gname']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c65eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1782, 1782)\n"
     ]
    }
   ],
   "source": [
    "print(adj_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1220e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class PyTorchGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dcf2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def run_epoch(model, data, labels, mask, optimizer=None):\n",
    "    is_training = optimizer is not None\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    loss = loss_fn(out[mask], labels[mask])\n",
    "\n",
    "    if is_training:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Accuracy\n",
    "    pred = out[mask].argmax(dim=1)\n",
    "    acc = (pred == labels[mask]).float().mean().item()\n",
    "    return acc, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# 1. Convert adjacency matrix to edge_index\n",
    "A_coo = coo_matrix(adj_matrix)\n",
    "edge_index = torch.tensor(np.vstack((A_coo.row, A_coo.col)), dtype=torch.long)\n",
    "\n",
    "# Feature Matrix Tensor\n",
    "coords = np.array(list(coord_to_index.keys()), dtype=np.float32)\n",
    "feature_matrix = coords  # shape: (N, 2), with [longitude, latitude]\n",
    "x = torch.tensor(feature_matrix, dtype=torch.float32)\n",
    "\n",
    "num_nodes = x.shape[0]\n",
    "\n",
    "# Label Tensor\n",
    "y = torch.full((num_nodes,), -1, dtype=torch.long)  # -1 for unlabeled\n",
    "\n",
    "# Create Masks, indicates which ndoes are used in training and testing\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "# Assign group labels for train, set nodes as part of training set\n",
    "for _, row in train_df.iterrows():\n",
    "    coord = (row['longitude'], row['latitude'])\n",
    "    idx = coord_to_index[coord]\n",
    "    y[idx] = label_index[row['gname']]\n",
    "    train_mask[idx] = True\n",
    "\n",
    "# Assign group labels for test, set nodes as part of testing set\n",
    "for _, row in test_df.iterrows():\n",
    "    coord = (row['longitude'], row['latitude'])\n",
    "    idx = coord_to_index[coord]\n",
    "    y[idx] = label_index[row['gname']]\n",
    "    test_mask[idx] = True\n",
    "\n",
    "# Create PyG Data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = PyTorchGCN(in_channels=x.shape[1], hidden_channels=16, num_classes=len(label_index))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "max_test_acc = 0\n",
    "for epoch in range(50):\n",
    "    train_acc, train_loss = run_epoch(model, data, y, train_mask, optimizer)\n",
    "    test_acc, test_loss = run_epoch(model, data, y, test_mask)\n",
    "    if test_acc > max_test_acc:\n",
    "        max_test_acc = test_acc\n",
    "        max_test_acc_epoch = epoch + 1\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f} | Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "print('-----------------------')\n",
    "print(f'Best test acc in epoch {max_test_acc_epoch}, accuracy: {max_test_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
