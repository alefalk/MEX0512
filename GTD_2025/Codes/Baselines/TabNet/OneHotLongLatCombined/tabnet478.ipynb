{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-tabnet in /opt/conda/lib/python3.11/site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (1.5.0)\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (2.3.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (4.66.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->pytorch-tabnet) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install pytorch-tabnet --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = f'../../../../data/top30groups/OneHotLongLatCombined/train1/train{partition}.csv'\n",
    "testpath = f'../../../../data/top30groups/OneHotLongLatCombined/test1/test{partition}.csv'\n",
    "\n",
    "traindata = pd.read_csv(trainpath, encoding='ISO-8859-1')\n",
    "testdata = pd.read_csv(testpath, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def split_data(dftrain, dftest):\n",
    "    Xtrain = dftrain.drop(columns=['gname']).values.astype(float)\n",
    "    Ytrain = dftrain['gname'].values\n",
    "    Xtest = dftest.drop(columns=['gname']).values.astype(float)\n",
    "    Ytest = dftest['gname'].values\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    Ytrain = le.fit_transform(Ytrain)\n",
    "    Ytest = le.transform(Ytest)\n",
    "\n",
    "    #y_pred_decoded = model.label_encoder.inverse_transform(y_pred)\n",
    "    y_true_decoded = le.inverse_transform(Ytest)\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest, y_true_decoded, le\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "class TabNetClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, lambda_sparse=1e-3, optimizer_params=None):\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.optimizer_params = optimizer_params or {'lr': 0.01}\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = TabNetClassifier(\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            lambda_sparse=self.lambda_sparse,\n",
    "            optimizer_params=self.optimizer_params,\n",
    "            seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        self.model.fit(\n",
    "            X, y,\n",
    "            eval_set=[(X, y)],\n",
    "            max_epochs=500,\n",
    "            patience=120,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            eval_metric=['accuracy']\n",
    "        )\n",
    "\n",
    "        self.classes_ = np.unique(y)  \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return (preds == y).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_tabnet.sklearn import TabNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "\n",
    "def find_best_tabnet(Xtrain, Ytrain, n_iter=20):\n",
    "    print(\"Starting TabNet grid search\")\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "    param_dist = {\n",
    "        'n_d': [8, 16, 24],\n",
    "        'n_a': [8, 16, 24],\n",
    "        'n_steps': [3, 4, 5],\n",
    "        'gamma': [1.0, 1.3, 1.5],\n",
    "        'lambda_sparse': [1e-4, 1e-3, 1e-2],\n",
    "        'optimizer_params': [{'lr': 0.01}]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=TabNetClassifierWrapper(),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='accuracy',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(Xtrain, Ytrain)\n",
    "    print(\"Best parameters:\", random_search.best_params_)\n",
    "    print(\"Best accuracy:\", random_search.best_score_)\n",
    "\n",
    "    return random_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.callbacks import Callback\n",
    "import time\n",
    "\n",
    "class EpochTimer(Callback):\n",
    "    def __init__(self):\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch_idx, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch_idx, logs=None):\n",
    "        duration = time.time() - self.start_time\n",
    "        self.epoch_times.append(duration)\n",
    "        print(f\"⏱️ Epoch {epoch_idx + 1} took {duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TabNet grid search\n",
      "CUDA available: True\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "Early stopping occurred at epoch 213 with best_epoch = 93 and best_val_0_accuracy = 0.92852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 214 with best_epoch = 94 and best_val_0_accuracy = 0.93376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 216 with best_epoch = 96 and best_val_0_accuracy = 0.95235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 216 with best_epoch = 96 and best_val_0_accuracy = 0.93151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 217 with best_epoch = 97 and best_val_0_accuracy = 0.94848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 218 with best_epoch = 98 and best_val_0_accuracy = 0.92952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 219 with best_epoch = 99 and best_val_0_accuracy = 0.95372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 220 with best_epoch = 100 and best_val_0_accuracy = 0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 224 with best_epoch = 104 and best_val_0_accuracy = 0.95334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 231 with best_epoch = 111 and best_val_0_accuracy = 0.93538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 214 with best_epoch = 94 and best_val_0_accuracy = 0.91093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 214 with best_epoch = 94 and best_val_0_accuracy = 0.93688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 215 with best_epoch = 95 and best_val_0_accuracy = 0.92902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 215 with best_epoch = 95 and best_val_0_accuracy = 0.91592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
      "    y_pred = method_caller(\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 211, in _get_response_values\n",
      "    y_pred = prediction_method(X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_74284/2810544730.py\", line 41, in predict\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 312, in predict\n",
      "    output, M_loss = self.network(data)\n",
      "                     ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 616, in forward\n",
      "    return self.tabnet(x)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 492, in forward\n",
      "    steps_output, M_loss = self.encoder(x)\n",
      "                           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 172, in forward\n",
      "    M = self.att_transformers[step](prior, att)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 671, in forward\n",
      "    x = self.selector(x)\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/sparsemax.py\", line 109, in forward\n",
      "    return sparsemax(input, self.dim)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py\", line 598, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/sparsemax.py\", line 52, in forward\n",
      "    tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/sparsemax.py\", line 88, in _threshold_and_support\n",
      "    input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":844, please report a bug to PyTorch. \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 218 with best_epoch = 98 and best_val_0_accuracy = 0.92852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 219 with best_epoch = 99 and best_val_0_accuracy = 0.93313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 219 with best_epoch = 99 and best_val_0_accuracy = 0.93875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 220 with best_epoch = 100 and best_val_0_accuracy = 0.93388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
      "    y_pred = method_caller(\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 211, in _get_response_values\n",
      "    y_pred = prediction_method(X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_74284/2810544730.py\", line 41, in predict\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 312, in predict\n",
      "    output, M_loss = self.network(data)\n",
      "                     ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 616, in forward\n",
      "    return self.tabnet(x)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 492, in forward\n",
      "    steps_output, M_loss = self.encoder(x)\n",
      "                           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 172, in forward\n",
      "    M = self.att_transformers[step](prior, att)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 671, in forward\n",
      "    x = self.selector(x)\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/sparsemax.py\", line 109, in forward\n",
      "    return sparsemax(input, self.dim)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py\", line 598, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/sparsemax.py\", line 52, in forward\n",
      "    tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/sparsemax.py\", line 88, in _threshold_and_support\n",
      "    input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":844, please report a bug to PyTorch. \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 223 with best_epoch = 103 and best_val_0_accuracy = 0.90856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 225 with best_epoch = 105 and best_val_0_accuracy = 0.92727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 225 with best_epoch = 105 and best_val_0_accuracy = 0.92091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 225 with best_epoch = 105 and best_val_0_accuracy = 0.91517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 230 with best_epoch = 110 and best_val_0_accuracy = 0.92627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 231 with best_epoch = 111 and best_val_0_accuracy = 0.91043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 213 with best_epoch = 93 and best_val_0_accuracy = 0.91941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 216 with best_epoch = 96 and best_val_0_accuracy = 0.90556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 220 with best_epoch = 100 and best_val_0_accuracy = 0.8644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 222 with best_epoch = 102 and best_val_0_accuracy = 0.91966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 224 with best_epoch = 104 and best_val_0_accuracy = 0.91504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:982: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 971, in _score\n",
      "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 279, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 371, in _score\n",
      "    y_pred = method_caller(\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_scorer.py\", line 89, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sklearn/utils/_response.py\", line 211, in _get_response_values\n",
      "    y_pred = prediction_method(X)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_74284/2810544730.py\", line 41, in predict\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 312, in predict\n",
      "    output, M_loss = self.network(data)\n",
      "                     ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 616, in forward\n",
      "    return self.tabnet(x)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 492, in forward\n",
      "    steps_output, M_loss = self.encoder(x)\n",
      "                           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 172, in forward\n",
      "    M = self.att_transformers[step](prior, att)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/tab_network.py\", line 671, in forward\n",
      "    x = self.selector(x)\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/sparsemax.py\", line 109, in forward\n",
      "    return sparsemax(input, self.dim)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py\", line 598, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/sparsemax.py\", line 52, in forward\n",
      "    tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/sparsemax.py\", line 88, in _threshold_and_support\n",
      "    input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":844, please report a bug to PyTorch. \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 226 with best_epoch = 106 and best_val_0_accuracy = 0.89296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 227 with best_epoch = 107 and best_val_0_accuracy = 0.90669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 215 with best_epoch = 95 and best_val_0_accuracy = 0.95833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 214 with best_epoch = 94 and best_val_0_accuracy = 0.94474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 217 with best_epoch = 97 and best_val_0_accuracy = 0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 217 with best_epoch = 97 and best_val_0_accuracy = 0.94299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 217 with best_epoch = 97 and best_val_0_accuracy = 0.94461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 217 with best_epoch = 97 and best_val_0_accuracy = 0.94274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 214 with best_epoch = 94 and best_val_0_accuracy = 0.94474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 214 with best_epoch = 94 and best_val_0_accuracy = 0.94698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 216 with best_epoch = 96 and best_val_0_accuracy = 0.94399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 221 with best_epoch = 101 and best_val_0_accuracy = 0.94099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 398 with best_epoch = 278 and best_val_0_accuracy = 0.94286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 220 with best_epoch = 100 and best_val_0_accuracy = 0.93201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 221 with best_epoch = 101 and best_val_0_accuracy = 0.95135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 218 with best_epoch = 98 and best_val_0_accuracy = 0.94723\n",
      "\n",
      "Early stopping occurred at epoch 222 with best_epoch = 102 and best_val_0_accuracy = 0.94386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 217 with best_epoch = 97 and best_val_0_accuracy = 0.92041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 225 with best_epoch = 105 and best_val_0_accuracy = 0.88623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 222 with best_epoch = 102 and best_val_0_accuracy = 0.92191\n",
      "\n",
      "Early stopping occurred at epoch 223 with best_epoch = 103 and best_val_0_accuracy = 0.9254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 228 with best_epoch = 108 and best_val_0_accuracy = 0.8977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 221 with best_epoch = 101 and best_val_0_accuracy = 0.92827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 224 with best_epoch = 104 and best_val_0_accuracy = 0.90868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 478 and best_val_0_accuracy = 0.9234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 194 with best_epoch = 74 and best_val_0_accuracy = 0.93772\n",
      "Best parameters: {'optimizer_params': {'lr': 0.01}, 'n_steps': 3, 'n_d': 24, 'n_a': 16, 'lambda_sparse': 0.01, 'gamma': 1.3}\n",
      "Best accuracy: 0.5731536926147704\n",
      "epoch 0  | loss: 4.16552 | val_0_accuracy: 0.02745 |  0:00:00s\n",
      "⏱️ Epoch 1 took 0.37 seconds\n",
      "epoch 1  | loss: 3.77834 | val_0_accuracy: 0.03343 |  0:00:00s\n",
      "⏱️ Epoch 2 took 0.39 seconds\n",
      "epoch 2  | loss: 3.6062  | val_0_accuracy: 0.03343 |  0:00:01s\n",
      "⏱️ Epoch 3 took 0.36 seconds\n",
      "epoch 3  | loss: 3.51343 | val_0_accuracy: 0.04491 |  0:00:01s\n",
      "⏱️ Epoch 4 took 0.42 seconds\n",
      "epoch 4  | loss: 3.42667 | val_0_accuracy: 0.04591 |  0:00:01s\n",
      "⏱️ Epoch 5 took 0.38 seconds\n",
      "epoch 5  | loss: 3.31101 | val_0_accuracy: 0.0494  |  0:00:02s\n",
      "⏱️ Epoch 6 took 0.38 seconds\n",
      "epoch 6  | loss: 3.17824 | val_0_accuracy: 0.07036 |  0:00:02s\n",
      "⏱️ Epoch 7 took 0.38 seconds\n",
      "epoch 7  | loss: 3.06392 | val_0_accuracy: 0.06986 |  0:00:03s\n",
      "⏱️ Epoch 8 took 0.38 seconds\n",
      "epoch 8  | loss: 2.89815 | val_0_accuracy: 0.08084 |  0:00:03s\n",
      "⏱️ Epoch 9 took 0.37 seconds\n",
      "epoch 9  | loss: 2.72502 | val_0_accuracy: 0.08433 |  0:00:03s\n",
      "⏱️ Epoch 10 took 0.36 seconds\n",
      "epoch 10 | loss: 2.54779 | val_0_accuracy: 0.07236 |  0:00:04s\n",
      "⏱️ Epoch 11 took 0.39 seconds\n",
      "epoch 11 | loss: 2.35588 | val_0_accuracy: 0.09232 |  0:00:04s\n",
      "⏱️ Epoch 12 took 0.40 seconds\n",
      "epoch 12 | loss: 2.18096 | val_0_accuracy: 0.1013  |  0:00:04s\n",
      "⏱️ Epoch 13 took 0.40 seconds\n",
      "epoch 13 | loss: 2.05567 | val_0_accuracy: 0.10629 |  0:00:05s\n",
      "⏱️ Epoch 14 took 0.37 seconds\n",
      "epoch 14 | loss: 1.94698 | val_0_accuracy: 0.11028 |  0:00:05s\n",
      "⏱️ Epoch 15 took 0.37 seconds\n",
      "epoch 15 | loss: 1.82531 | val_0_accuracy: 0.09032 |  0:00:06s\n",
      "⏱️ Epoch 16 took 0.40 seconds\n",
      "epoch 16 | loss: 1.7125  | val_0_accuracy: 0.11477 |  0:00:06s\n",
      "⏱️ Epoch 17 took 0.40 seconds\n",
      "epoch 17 | loss: 1.61933 | val_0_accuracy: 0.13024 |  0:00:06s\n",
      "⏱️ Epoch 18 took 0.37 seconds\n",
      "epoch 18 | loss: 1.53704 | val_0_accuracy: 0.13273 |  0:00:07s\n",
      "⏱️ Epoch 19 took 0.36 seconds\n",
      "epoch 19 | loss: 1.43098 | val_0_accuracy: 0.14072 |  0:00:07s\n",
      "⏱️ Epoch 20 took 0.37 seconds\n",
      "epoch 20 | loss: 1.36099 | val_0_accuracy: 0.13473 |  0:00:07s\n",
      "⏱️ Epoch 21 took 0.36 seconds\n",
      "epoch 21 | loss: 1.27329 | val_0_accuracy: 0.13922 |  0:00:08s\n",
      "⏱️ Epoch 22 took 0.36 seconds\n",
      "epoch 22 | loss: 1.20919 | val_0_accuracy: 0.1477  |  0:00:08s\n",
      "⏱️ Epoch 23 took 0.36 seconds\n",
      "epoch 23 | loss: 1.13336 | val_0_accuracy: 0.15968 |  0:00:09s\n",
      "⏱️ Epoch 24 took 0.38 seconds\n",
      "epoch 24 | loss: 1.0847  | val_0_accuracy: 0.18263 |  0:00:09s\n",
      "⏱️ Epoch 25 took 0.45 seconds\n",
      "epoch 25 | loss: 1.0016  | val_0_accuracy: 0.17465 |  0:00:09s\n",
      "⏱️ Epoch 26 took 0.37 seconds\n",
      "epoch 26 | loss: 0.96735 | val_0_accuracy: 0.18912 |  0:00:10s\n",
      "⏱️ Epoch 27 took 0.38 seconds\n",
      "epoch 27 | loss: 0.92654 | val_0_accuracy: 0.19062 |  0:00:10s\n",
      "⏱️ Epoch 28 took 0.36 seconds\n",
      "epoch 28 | loss: 0.89876 | val_0_accuracy: 0.20758 |  0:00:11s\n",
      "⏱️ Epoch 29 took 0.37 seconds\n",
      "epoch 29 | loss: 0.83539 | val_0_accuracy: 0.21407 |  0:00:11s\n",
      "⏱️ Epoch 30 took 0.37 seconds\n",
      "epoch 30 | loss: 0.80826 | val_0_accuracy: 0.21357 |  0:00:11s\n",
      "⏱️ Epoch 31 took 0.39 seconds\n",
      "epoch 31 | loss: 0.78002 | val_0_accuracy: 0.21906 |  0:00:12s\n",
      "⏱️ Epoch 32 took 0.38 seconds\n",
      "epoch 32 | loss: 0.73914 | val_0_accuracy: 0.23054 |  0:00:12s\n",
      "⏱️ Epoch 33 took 0.37 seconds\n",
      "epoch 33 | loss: 0.70231 | val_0_accuracy: 0.23902 |  0:00:12s\n",
      "⏱️ Epoch 34 took 0.37 seconds\n",
      "epoch 34 | loss: 0.67763 | val_0_accuracy: 0.24002 |  0:00:13s\n",
      "⏱️ Epoch 35 took 0.36 seconds\n",
      "epoch 35 | loss: 0.65634 | val_0_accuracy: 0.25299 |  0:00:13s\n",
      "⏱️ Epoch 36 took 0.38 seconds\n",
      "epoch 36 | loss: 0.63205 | val_0_accuracy: 0.26347 |  0:00:14s\n",
      "⏱️ Epoch 37 took 0.39 seconds\n",
      "epoch 37 | loss: 0.61271 | val_0_accuracy: 0.28343 |  0:00:14s\n",
      "⏱️ Epoch 38 took 0.41 seconds\n",
      "epoch 38 | loss: 0.59483 | val_0_accuracy: 0.26747 |  0:00:14s\n",
      "⏱️ Epoch 39 took 0.40 seconds\n",
      "epoch 39 | loss: 0.55943 | val_0_accuracy: 0.27196 |  0:00:15s\n",
      "⏱️ Epoch 40 took 0.39 seconds\n",
      "epoch 40 | loss: 0.56516 | val_0_accuracy: 0.28094 |  0:00:15s\n",
      "⏱️ Epoch 41 took 0.40 seconds\n",
      "epoch 41 | loss: 0.52399 | val_0_accuracy: 0.29341 |  0:00:15s\n",
      "⏱️ Epoch 42 took 0.36 seconds\n",
      "epoch 42 | loss: 0.49698 | val_0_accuracy: 0.2989  |  0:00:16s\n",
      "⏱️ Epoch 43 took 0.41 seconds\n",
      "epoch 43 | loss: 0.47949 | val_0_accuracy: 0.31387 |  0:00:16s\n",
      "⏱️ Epoch 44 took 0.40 seconds\n",
      "epoch 44 | loss: 0.47736 | val_0_accuracy: 0.32186 |  0:00:17s\n",
      "⏱️ Epoch 45 took 0.38 seconds\n",
      "epoch 45 | loss: 0.46509 | val_0_accuracy: 0.31587 |  0:00:17s\n",
      "⏱️ Epoch 46 took 0.41 seconds\n",
      "epoch 46 | loss: 0.45805 | val_0_accuracy: 0.31637 |  0:00:18s\n",
      "⏱️ Epoch 47 took 0.54 seconds\n",
      "epoch 47 | loss: 0.45054 | val_0_accuracy: 0.32735 |  0:00:18s\n",
      "⏱️ Epoch 48 took 0.43 seconds\n",
      "epoch 48 | loss: 0.41507 | val_0_accuracy: 0.34631 |  0:00:18s\n",
      "⏱️ Epoch 49 took 0.41 seconds\n",
      "epoch 49 | loss: 0.41155 | val_0_accuracy: 0.35729 |  0:00:19s\n",
      "⏱️ Epoch 50 took 0.37 seconds\n",
      "epoch 50 | loss: 0.41494 | val_0_accuracy: 0.36128 |  0:00:19s\n",
      "⏱️ Epoch 51 took 0.41 seconds\n",
      "epoch 51 | loss: 0.39256 | val_0_accuracy: 0.36876 |  0:00:20s\n",
      "⏱️ Epoch 52 took 0.36 seconds\n",
      "epoch 52 | loss: 0.38055 | val_0_accuracy: 0.37824 |  0:00:20s\n",
      "⏱️ Epoch 53 took 0.37 seconds\n",
      "epoch 53 | loss: 0.38066 | val_0_accuracy: 0.38373 |  0:00:20s\n",
      "⏱️ Epoch 54 took 0.39 seconds\n",
      "epoch 54 | loss: 0.38316 | val_0_accuracy: 0.38922 |  0:00:21s\n",
      "⏱️ Epoch 55 took 0.39 seconds\n",
      "epoch 55 | loss: 0.38545 | val_0_accuracy: 0.39271 |  0:00:21s\n",
      "⏱️ Epoch 56 took 0.38 seconds\n",
      "epoch 56 | loss: 0.35013 | val_0_accuracy: 0.4007  |  0:00:22s\n",
      "⏱️ Epoch 57 took 0.39 seconds\n",
      "epoch 57 | loss: 0.34274 | val_0_accuracy: 0.41068 |  0:00:22s\n",
      "⏱️ Epoch 58 took 0.40 seconds\n",
      "epoch 58 | loss: 0.34685 | val_0_accuracy: 0.41517 |  0:00:22s\n",
      "⏱️ Epoch 59 took 0.37 seconds\n",
      "epoch 59 | loss: 0.32661 | val_0_accuracy: 0.42715 |  0:00:23s\n",
      "⏱️ Epoch 60 took 0.39 seconds\n",
      "epoch 60 | loss: 0.34864 | val_0_accuracy: 0.43613 |  0:00:23s\n",
      "⏱️ Epoch 61 took 0.40 seconds\n",
      "epoch 61 | loss: 0.31995 | val_0_accuracy: 0.43912 |  0:00:23s\n",
      "⏱️ Epoch 62 took 0.40 seconds\n",
      "epoch 62 | loss: 0.32917 | val_0_accuracy: 0.44611 |  0:00:24s\n",
      "⏱️ Epoch 63 took 0.41 seconds\n",
      "epoch 63 | loss: 0.3107  | val_0_accuracy: 0.46008 |  0:00:24s\n",
      "⏱️ Epoch 64 took 0.40 seconds\n",
      "epoch 64 | loss: 0.28878 | val_0_accuracy: 0.46756 |  0:00:25s\n",
      "⏱️ Epoch 65 took 0.41 seconds\n",
      "epoch 65 | loss: 0.3011  | val_0_accuracy: 0.47954 |  0:00:25s\n",
      "⏱️ Epoch 66 took 0.41 seconds\n",
      "epoch 66 | loss: 0.29423 | val_0_accuracy: 0.48303 |  0:00:26s\n",
      "⏱️ Epoch 67 took 0.43 seconds\n",
      "epoch 67 | loss: 0.28379 | val_0_accuracy: 0.48054 |  0:00:26s\n",
      "⏱️ Epoch 68 took 0.35 seconds\n",
      "epoch 68 | loss: 0.2893  | val_0_accuracy: 0.49551 |  0:00:26s\n",
      "⏱️ Epoch 69 took 0.40 seconds\n",
      "epoch 69 | loss: 0.29001 | val_0_accuracy: 0.499   |  0:00:27s\n",
      "⏱️ Epoch 70 took 0.38 seconds\n",
      "epoch 70 | loss: 0.28128 | val_0_accuracy: 0.50599 |  0:00:27s\n",
      "⏱️ Epoch 71 took 0.42 seconds\n",
      "epoch 71 | loss: 0.26504 | val_0_accuracy: 0.50399 |  0:00:27s\n",
      "⏱️ Epoch 72 took 0.40 seconds\n",
      "epoch 72 | loss: 0.27208 | val_0_accuracy: 0.4995  |  0:00:28s\n",
      "⏱️ Epoch 73 took 0.36 seconds\n",
      "epoch 73 | loss: 0.26266 | val_0_accuracy: 0.51198 |  0:00:28s\n",
      "⏱️ Epoch 74 took 0.35 seconds\n",
      "epoch 74 | loss: 0.25552 | val_0_accuracy: 0.51248 |  0:00:29s\n",
      "⏱️ Epoch 75 took 0.39 seconds\n",
      "epoch 75 | loss: 0.25841 | val_0_accuracy: 0.51198 |  0:00:29s\n",
      "⏱️ Epoch 76 took 0.40 seconds\n",
      "epoch 76 | loss: 0.25556 | val_0_accuracy: 0.51996 |  0:00:29s\n",
      "⏱️ Epoch 77 took 0.43 seconds\n",
      "epoch 77 | loss: 0.25825 | val_0_accuracy: 0.52645 |  0:00:30s\n",
      "⏱️ Epoch 78 took 0.42 seconds\n",
      "epoch 78 | loss: 0.24265 | val_0_accuracy: 0.53094 |  0:00:30s\n",
      "⏱️ Epoch 79 took 0.35 seconds\n",
      "epoch 79 | loss: 0.25658 | val_0_accuracy: 0.53393 |  0:00:31s\n",
      "⏱️ Epoch 80 took 0.37 seconds\n",
      "epoch 80 | loss: 0.23843 | val_0_accuracy: 0.53343 |  0:00:31s\n",
      "⏱️ Epoch 81 took 0.37 seconds\n",
      "epoch 81 | loss: 0.24564 | val_0_accuracy: 0.53792 |  0:00:31s\n",
      "⏱️ Epoch 82 took 0.36 seconds\n",
      "epoch 82 | loss: 0.22764 | val_0_accuracy: 0.54042 |  0:00:32s\n",
      "⏱️ Epoch 83 took 0.39 seconds\n",
      "epoch 83 | loss: 0.22625 | val_0_accuracy: 0.53892 |  0:00:32s\n",
      "⏱️ Epoch 84 took 0.42 seconds\n",
      "epoch 84 | loss: 0.24403 | val_0_accuracy: 0.54341 |  0:00:33s\n",
      "⏱️ Epoch 85 took 0.43 seconds\n",
      "epoch 85 | loss: 0.23196 | val_0_accuracy: 0.53942 |  0:00:33s\n",
      "⏱️ Epoch 86 took 0.38 seconds\n",
      "epoch 86 | loss: 0.23305 | val_0_accuracy: 0.54242 |  0:00:33s\n",
      "⏱️ Epoch 87 took 0.37 seconds\n",
      "epoch 87 | loss: 0.23495 | val_0_accuracy: 0.53992 |  0:00:34s\n",
      "⏱️ Epoch 88 took 0.38 seconds\n",
      "epoch 88 | loss: 0.22961 | val_0_accuracy: 0.54341 |  0:00:34s\n",
      "⏱️ Epoch 89 took 0.37 seconds\n",
      "epoch 89 | loss: 0.22008 | val_0_accuracy: 0.54092 |  0:00:34s\n",
      "⏱️ Epoch 90 took 0.40 seconds\n",
      "epoch 90 | loss: 0.22637 | val_0_accuracy: 0.54591 |  0:00:35s\n",
      "⏱️ Epoch 91 took 0.37 seconds\n",
      "epoch 91 | loss: 0.21428 | val_0_accuracy: 0.54691 |  0:00:35s\n",
      "⏱️ Epoch 92 took 0.38 seconds\n",
      "epoch 92 | loss: 0.22082 | val_0_accuracy: 0.5504  |  0:00:36s\n",
      "⏱️ Epoch 93 took 0.40 seconds\n",
      "epoch 93 | loss: 0.21595 | val_0_accuracy: 0.5509  |  0:00:36s\n",
      "⏱️ Epoch 94 took 0.37 seconds\n",
      "epoch 94 | loss: 0.21078 | val_0_accuracy: 0.5514  |  0:00:36s\n",
      "⏱️ Epoch 95 took 0.38 seconds\n",
      "epoch 95 | loss: 0.21413 | val_0_accuracy: 0.54741 |  0:00:37s\n",
      "⏱️ Epoch 96 took 0.38 seconds\n",
      "epoch 96 | loss: 0.21569 | val_0_accuracy: 0.54691 |  0:00:37s\n",
      "⏱️ Epoch 97 took 0.42 seconds\n",
      "epoch 97 | loss: 0.20288 | val_0_accuracy: 0.5479  |  0:00:38s\n",
      "⏱️ Epoch 98 took 0.40 seconds\n",
      "epoch 98 | loss: 0.20453 | val_0_accuracy: 0.54441 |  0:00:38s\n",
      "⏱️ Epoch 99 took 0.40 seconds\n",
      "epoch 99 | loss: 0.20037 | val_0_accuracy: 0.5479  |  0:00:38s\n",
      "⏱️ Epoch 100 took 0.38 seconds\n",
      "epoch 100| loss: 0.21135 | val_0_accuracy: 0.54591 |  0:00:39s\n",
      "⏱️ Epoch 101 took 0.44 seconds\n",
      "epoch 101| loss: 0.19269 | val_0_accuracy: 0.54641 |  0:00:39s\n",
      "⏱️ Epoch 102 took 0.38 seconds\n",
      "epoch 102| loss: 0.19298 | val_0_accuracy: 0.54691 |  0:00:40s\n",
      "⏱️ Epoch 103 took 0.43 seconds\n",
      "epoch 103| loss: 0.20427 | val_0_accuracy: 0.54192 |  0:00:40s\n",
      "⏱️ Epoch 104 took 0.38 seconds\n",
      "epoch 104| loss: 0.18928 | val_0_accuracy: 0.5489  |  0:00:40s\n",
      "⏱️ Epoch 105 took 0.36 seconds\n",
      "epoch 105| loss: 0.19194 | val_0_accuracy: 0.54541 |  0:00:41s\n",
      "⏱️ Epoch 106 took 0.39 seconds\n",
      "epoch 106| loss: 0.18611 | val_0_accuracy: 0.54142 |  0:00:41s\n",
      "⏱️ Epoch 107 took 0.37 seconds\n",
      "epoch 107| loss: 0.19171 | val_0_accuracy: 0.54341 |  0:00:41s\n",
      "⏱️ Epoch 108 took 0.37 seconds\n",
      "epoch 108| loss: 0.19212 | val_0_accuracy: 0.5479  |  0:00:42s\n",
      "⏱️ Epoch 109 took 0.36 seconds\n",
      "epoch 109| loss: 0.20091 | val_0_accuracy: 0.54591 |  0:00:42s\n",
      "⏱️ Epoch 110 took 0.37 seconds\n",
      "epoch 110| loss: 0.19964 | val_0_accuracy: 0.54242 |  0:00:43s\n",
      "⏱️ Epoch 111 took 0.37 seconds\n",
      "epoch 111| loss: 0.19295 | val_0_accuracy: 0.54341 |  0:00:43s\n",
      "⏱️ Epoch 112 took 0.41 seconds\n",
      "epoch 112| loss: 0.18148 | val_0_accuracy: 0.54591 |  0:00:43s\n",
      "⏱️ Epoch 113 took 0.35 seconds\n",
      "epoch 113| loss: 0.1898  | val_0_accuracy: 0.54691 |  0:00:44s\n",
      "⏱️ Epoch 114 took 0.37 seconds\n",
      "epoch 114| loss: 0.19384 | val_0_accuracy: 0.54391 |  0:00:44s\n",
      "⏱️ Epoch 115 took 0.35 seconds\n",
      "epoch 115| loss: 0.1999  | val_0_accuracy: 0.54391 |  0:00:44s\n",
      "⏱️ Epoch 116 took 0.38 seconds\n",
      "epoch 116| loss: 0.17963 | val_0_accuracy: 0.54491 |  0:00:45s\n",
      "⏱️ Epoch 117 took 0.40 seconds\n",
      "epoch 117| loss: 0.19047 | val_0_accuracy: 0.54092 |  0:00:45s\n",
      "⏱️ Epoch 118 took 0.40 seconds\n",
      "epoch 118| loss: 0.16837 | val_0_accuracy: 0.53892 |  0:00:46s\n",
      "⏱️ Epoch 119 took 0.51 seconds\n",
      "epoch 119| loss: 0.1928  | val_0_accuracy: 0.53493 |  0:00:46s\n",
      "⏱️ Epoch 120 took 0.37 seconds\n",
      "epoch 120| loss: 0.19125 | val_0_accuracy: 0.53942 |  0:00:46s\n",
      "⏱️ Epoch 121 took 0.37 seconds\n",
      "epoch 121| loss: 0.1808  | val_0_accuracy: 0.54391 |  0:00:47s\n",
      "⏱️ Epoch 122 took 0.40 seconds\n",
      "epoch 122| loss: 0.18617 | val_0_accuracy: 0.53643 |  0:00:47s\n",
      "⏱️ Epoch 123 took 0.35 seconds\n",
      "epoch 123| loss: 0.1825  | val_0_accuracy: 0.53942 |  0:00:48s\n",
      "⏱️ Epoch 124 took 0.42 seconds\n",
      "epoch 124| loss: 0.17102 | val_0_accuracy: 0.54092 |  0:00:48s\n",
      "⏱️ Epoch 125 took 0.39 seconds\n",
      "epoch 125| loss: 0.17769 | val_0_accuracy: 0.54142 |  0:00:48s\n",
      "⏱️ Epoch 126 took 0.40 seconds\n",
      "epoch 126| loss: 0.16646 | val_0_accuracy: 0.54142 |  0:00:49s\n",
      "⏱️ Epoch 127 took 0.37 seconds\n",
      "epoch 127| loss: 0.17022 | val_0_accuracy: 0.53942 |  0:00:49s\n",
      "⏱️ Epoch 128 took 0.37 seconds\n",
      "epoch 128| loss: 0.16821 | val_0_accuracy: 0.54042 |  0:00:50s\n",
      "⏱️ Epoch 129 took 0.38 seconds\n",
      "epoch 129| loss: 0.17107 | val_0_accuracy: 0.53743 |  0:00:50s\n",
      "⏱️ Epoch 130 took 0.39 seconds\n",
      "epoch 130| loss: 0.17306 | val_0_accuracy: 0.53792 |  0:00:50s\n",
      "⏱️ Epoch 131 took 0.39 seconds\n",
      "epoch 131| loss: 0.17656 | val_0_accuracy: 0.53194 |  0:00:51s\n",
      "⏱️ Epoch 132 took 0.38 seconds\n",
      "epoch 132| loss: 0.1622  | val_0_accuracy: 0.53393 |  0:00:51s\n",
      "⏱️ Epoch 133 took 0.42 seconds\n",
      "epoch 133| loss: 0.17163 | val_0_accuracy: 0.53992 |  0:00:52s\n",
      "⏱️ Epoch 134 took 0.47 seconds\n",
      "epoch 134| loss: 0.16058 | val_0_accuracy: 0.53792 |  0:00:52s\n",
      "⏱️ Epoch 135 took 0.37 seconds\n",
      "epoch 135| loss: 0.16136 | val_0_accuracy: 0.53792 |  0:00:52s\n",
      "⏱️ Epoch 136 took 0.38 seconds\n",
      "epoch 136| loss: 0.17135 | val_0_accuracy: 0.53942 |  0:00:53s\n",
      "⏱️ Epoch 137 took 0.39 seconds\n",
      "epoch 137| loss: 0.16295 | val_0_accuracy: 0.53743 |  0:00:53s\n",
      "⏱️ Epoch 138 took 0.38 seconds\n",
      "epoch 138| loss: 0.17154 | val_0_accuracy: 0.54242 |  0:00:53s\n",
      "⏱️ Epoch 139 took 0.40 seconds\n",
      "epoch 139| loss: 0.17652 | val_0_accuracy: 0.53343 |  0:00:54s\n",
      "⏱️ Epoch 140 took 0.40 seconds\n",
      "epoch 140| loss: 0.16959 | val_0_accuracy: 0.53443 |  0:00:54s\n",
      "⏱️ Epoch 141 took 0.38 seconds\n",
      "epoch 141| loss: 0.16586 | val_0_accuracy: 0.53543 |  0:00:55s\n",
      "⏱️ Epoch 142 took 0.36 seconds\n",
      "epoch 142| loss: 0.17097 | val_0_accuracy: 0.54092 |  0:00:55s\n",
      "⏱️ Epoch 143 took 0.43 seconds\n",
      "epoch 143| loss: 0.16893 | val_0_accuracy: 0.53593 |  0:00:55s\n",
      "⏱️ Epoch 144 took 0.37 seconds\n",
      "epoch 144| loss: 0.16189 | val_0_accuracy: 0.53693 |  0:00:56s\n",
      "⏱️ Epoch 145 took 0.37 seconds\n",
      "epoch 145| loss: 0.16431 | val_0_accuracy: 0.53643 |  0:00:56s\n",
      "⏱️ Epoch 146 took 0.36 seconds\n",
      "epoch 146| loss: 0.16036 | val_0_accuracy: 0.53543 |  0:00:57s\n",
      "⏱️ Epoch 147 took 0.35 seconds\n",
      "epoch 147| loss: 0.16272 | val_0_accuracy: 0.52645 |  0:00:57s\n",
      "⏱️ Epoch 148 took 0.42 seconds\n",
      "epoch 148| loss: 0.1588  | val_0_accuracy: 0.52944 |  0:00:57s\n",
      "⏱️ Epoch 149 took 0.36 seconds\n",
      "epoch 149| loss: 0.15185 | val_0_accuracy: 0.53044 |  0:00:58s\n",
      "⏱️ Epoch 150 took 0.40 seconds\n",
      "epoch 150| loss: 0.15042 | val_0_accuracy: 0.52645 |  0:00:58s\n",
      "⏱️ Epoch 151 took 0.37 seconds\n",
      "epoch 151| loss: 0.15499 | val_0_accuracy: 0.52794 |  0:00:58s\n",
      "⏱️ Epoch 152 took 0.38 seconds\n",
      "epoch 152| loss: 0.15646 | val_0_accuracy: 0.52944 |  0:00:59s\n",
      "⏱️ Epoch 153 took 0.37 seconds\n",
      "epoch 153| loss: 0.15218 | val_0_accuracy: 0.53493 |  0:00:59s\n",
      "⏱️ Epoch 154 took 0.38 seconds\n",
      "epoch 154| loss: 0.15697 | val_0_accuracy: 0.53244 |  0:01:00s\n",
      "⏱️ Epoch 155 took 0.38 seconds\n",
      "epoch 155| loss: 0.15245 | val_0_accuracy: 0.52545 |  0:01:00s\n",
      "⏱️ Epoch 156 took 0.37 seconds\n",
      "epoch 156| loss: 0.14436 | val_0_accuracy: 0.52994 |  0:01:00s\n",
      "⏱️ Epoch 157 took 0.41 seconds\n",
      "epoch 157| loss: 0.15313 | val_0_accuracy: 0.52794 |  0:01:01s\n",
      "⏱️ Epoch 158 took 0.38 seconds\n",
      "epoch 158| loss: 0.15297 | val_0_accuracy: 0.53244 |  0:01:01s\n",
      "⏱️ Epoch 159 took 0.39 seconds\n",
      "epoch 159| loss: 0.14861 | val_0_accuracy: 0.53144 |  0:01:01s\n",
      "⏱️ Epoch 160 took 0.37 seconds\n",
      "epoch 160| loss: 0.15883 | val_0_accuracy: 0.53343 |  0:01:02s\n",
      "⏱️ Epoch 161 took 0.41 seconds\n",
      "epoch 161| loss: 0.15219 | val_0_accuracy: 0.53942 |  0:01:02s\n",
      "⏱️ Epoch 162 took 0.39 seconds\n",
      "epoch 162| loss: 0.15441 | val_0_accuracy: 0.53144 |  0:01:03s\n",
      "⏱️ Epoch 163 took 0.36 seconds\n",
      "epoch 163| loss: 0.15965 | val_0_accuracy: 0.53044 |  0:01:03s\n",
      "⏱️ Epoch 164 took 0.38 seconds\n",
      "epoch 164| loss: 0.15306 | val_0_accuracy: 0.53493 |  0:01:03s\n",
      "⏱️ Epoch 165 took 0.38 seconds\n",
      "epoch 165| loss: 0.15452 | val_0_accuracy: 0.53643 |  0:01:04s\n",
      "⏱️ Epoch 166 took 0.40 seconds\n",
      "epoch 166| loss: 0.15582 | val_0_accuracy: 0.53194 |  0:01:04s\n",
      "⏱️ Epoch 167 took 0.40 seconds\n",
      "epoch 167| loss: 0.16274 | val_0_accuracy: 0.53044 |  0:01:05s\n",
      "⏱️ Epoch 168 took 0.38 seconds\n",
      "epoch 168| loss: 0.15313 | val_0_accuracy: 0.53293 |  0:01:05s\n",
      "⏱️ Epoch 169 took 0.34 seconds\n",
      "epoch 169| loss: 0.16323 | val_0_accuracy: 0.52794 |  0:01:05s\n",
      "⏱️ Epoch 170 took 0.38 seconds\n",
      "epoch 170| loss: 0.16217 | val_0_accuracy: 0.52994 |  0:01:06s\n",
      "⏱️ Epoch 171 took 0.40 seconds\n",
      "epoch 171| loss: 0.14967 | val_0_accuracy: 0.52944 |  0:01:06s\n",
      "⏱️ Epoch 172 took 0.40 seconds\n",
      "epoch 172| loss: 0.14573 | val_0_accuracy: 0.53393 |  0:01:06s\n",
      "⏱️ Epoch 173 took 0.40 seconds\n",
      "epoch 173| loss: 0.14122 | val_0_accuracy: 0.53293 |  0:01:07s\n",
      "⏱️ Epoch 174 took 0.40 seconds\n",
      "epoch 174| loss: 0.14108 | val_0_accuracy: 0.53443 |  0:01:07s\n",
      "⏱️ Epoch 175 took 0.41 seconds\n",
      "epoch 175| loss: 0.14906 | val_0_accuracy: 0.53244 |  0:01:08s\n",
      "⏱️ Epoch 176 took 0.37 seconds\n",
      "epoch 176| loss: 0.14507 | val_0_accuracy: 0.53393 |  0:01:08s\n",
      "⏱️ Epoch 177 took 0.38 seconds\n",
      "epoch 177| loss: 0.15021 | val_0_accuracy: 0.53393 |  0:01:08s\n",
      "⏱️ Epoch 178 took 0.36 seconds\n",
      "epoch 178| loss: 0.16377 | val_0_accuracy: 0.53443 |  0:01:09s\n",
      "⏱️ Epoch 179 took 0.44 seconds\n",
      "epoch 179| loss: 0.15579 | val_0_accuracy: 0.53743 |  0:01:09s\n",
      "⏱️ Epoch 180 took 0.42 seconds\n",
      "epoch 180| loss: 0.15152 | val_0_accuracy: 0.53244 |  0:01:10s\n",
      "⏱️ Epoch 181 took 0.40 seconds\n",
      "epoch 181| loss: 0.14748 | val_0_accuracy: 0.53144 |  0:01:10s\n",
      "⏱️ Epoch 182 took 0.39 seconds\n",
      "epoch 182| loss: 0.14722 | val_0_accuracy: 0.53493 |  0:01:10s\n",
      "⏱️ Epoch 183 took 0.37 seconds\n",
      "epoch 183| loss: 0.1412  | val_0_accuracy: 0.53443 |  0:01:11s\n",
      "⏱️ Epoch 184 took 0.37 seconds\n",
      "epoch 184| loss: 0.14271 | val_0_accuracy: 0.52894 |  0:01:11s\n",
      "⏱️ Epoch 185 took 0.44 seconds\n",
      "epoch 185| loss: 0.14222 | val_0_accuracy: 0.53393 |  0:01:12s\n",
      "⏱️ Epoch 186 took 0.39 seconds\n",
      "epoch 186| loss: 0.14949 | val_0_accuracy: 0.53443 |  0:01:12s\n",
      "⏱️ Epoch 187 took 0.35 seconds\n",
      "epoch 187| loss: 0.14177 | val_0_accuracy: 0.52645 |  0:01:12s\n",
      "⏱️ Epoch 188 took 0.37 seconds\n",
      "epoch 188| loss: 0.14163 | val_0_accuracy: 0.52894 |  0:01:13s\n",
      "⏱️ Epoch 189 took 0.38 seconds\n",
      "epoch 189| loss: 0.13455 | val_0_accuracy: 0.52944 |  0:01:13s\n",
      "⏱️ Epoch 190 took 0.39 seconds\n",
      "epoch 190| loss: 0.13594 | val_0_accuracy: 0.53244 |  0:01:14s\n",
      "⏱️ Epoch 191 took 0.38 seconds\n",
      "epoch 191| loss: 0.13434 | val_0_accuracy: 0.53044 |  0:01:14s\n",
      "⏱️ Epoch 192 took 0.41 seconds\n",
      "epoch 192| loss: 0.14213 | val_0_accuracy: 0.53244 |  0:01:14s\n",
      "⏱️ Epoch 193 took 0.39 seconds\n",
      "epoch 193| loss: 0.1332  | val_0_accuracy: 0.52495 |  0:01:15s\n",
      "⏱️ Epoch 194 took 0.38 seconds\n",
      "epoch 194| loss: 0.13725 | val_0_accuracy: 0.52745 |  0:01:15s\n",
      "⏱️ Epoch 195 took 0.40 seconds\n",
      "epoch 195| loss: 0.13762 | val_0_accuracy: 0.53244 |  0:01:15s\n",
      "⏱️ Epoch 196 took 0.40 seconds\n",
      "epoch 196| loss: 0.13403 | val_0_accuracy: 0.53593 |  0:01:16s\n",
      "⏱️ Epoch 197 took 0.37 seconds\n",
      "epoch 197| loss: 0.13235 | val_0_accuracy: 0.53493 |  0:01:16s\n",
      "⏱️ Epoch 198 took 0.36 seconds\n",
      "epoch 198| loss: 0.13043 | val_0_accuracy: 0.52994 |  0:01:17s\n",
      "⏱️ Epoch 199 took 0.37 seconds\n",
      "epoch 199| loss: 0.13002 | val_0_accuracy: 0.53343 |  0:01:17s\n",
      "⏱️ Epoch 200 took 0.37 seconds\n",
      "epoch 200| loss: 0.12315 | val_0_accuracy: 0.53992 |  0:01:17s\n",
      "⏱️ Epoch 201 took 0.35 seconds\n",
      "epoch 201| loss: 0.13361 | val_0_accuracy: 0.54192 |  0:01:18s\n",
      "⏱️ Epoch 202 took 0.38 seconds\n",
      "epoch 202| loss: 0.12281 | val_0_accuracy: 0.53293 |  0:01:18s\n",
      "⏱️ Epoch 203 took 0.37 seconds\n",
      "epoch 203| loss: 0.13007 | val_0_accuracy: 0.53293 |  0:01:19s\n",
      "⏱️ Epoch 204 took 0.48 seconds\n",
      "epoch 204| loss: 0.1421  | val_0_accuracy: 0.53942 |  0:01:19s\n",
      "⏱️ Epoch 205 took 0.36 seconds\n",
      "epoch 205| loss: 0.13414 | val_0_accuracy: 0.53892 |  0:01:19s\n",
      "⏱️ Epoch 206 took 0.39 seconds\n",
      "epoch 206| loss: 0.13824 | val_0_accuracy: 0.53593 |  0:01:20s\n",
      "⏱️ Epoch 207 took 0.40 seconds\n",
      "epoch 207| loss: 0.1483  | val_0_accuracy: 0.53693 |  0:01:20s\n",
      "⏱️ Epoch 208 took 0.40 seconds\n",
      "epoch 208| loss: 0.14554 | val_0_accuracy: 0.53743 |  0:01:20s\n",
      "⏱️ Epoch 209 took 0.40 seconds\n",
      "epoch 209| loss: 0.13542 | val_0_accuracy: 0.53593 |  0:01:21s\n",
      "⏱️ Epoch 210 took 0.40 seconds\n",
      "epoch 210| loss: 0.13286 | val_0_accuracy: 0.53293 |  0:01:21s\n",
      "⏱️ Epoch 211 took 0.39 seconds\n",
      "epoch 211| loss: 0.14136 | val_0_accuracy: 0.53892 |  0:01:22s\n",
      "⏱️ Epoch 212 took 0.38 seconds\n",
      "epoch 212| loss: 0.1411  | val_0_accuracy: 0.53942 |  0:01:22s\n",
      "⏱️ Epoch 213 took 0.40 seconds\n",
      "epoch 213| loss: 0.13666 | val_0_accuracy: 0.53443 |  0:01:22s\n",
      "⏱️ Epoch 214 took 0.34 seconds\n",
      "epoch 214| loss: 0.1419  | val_0_accuracy: 0.53293 |  0:01:23s\n",
      "⏱️ Epoch 215 took 0.39 seconds\n",
      "epoch 215| loss: 0.13522 | val_0_accuracy: 0.53643 |  0:01:23s\n",
      "⏱️ Epoch 216 took 0.40 seconds\n",
      "epoch 216| loss: 0.13263 | val_0_accuracy: 0.54341 |  0:01:24s\n",
      "⏱️ Epoch 217 took 0.38 seconds\n",
      "epoch 217| loss: 0.12674 | val_0_accuracy: 0.53693 |  0:01:24s\n",
      "⏱️ Epoch 218 took 0.35 seconds\n",
      "epoch 218| loss: 0.1328  | val_0_accuracy: 0.53693 |  0:01:24s\n",
      "⏱️ Epoch 219 took 0.36 seconds\n",
      "epoch 219| loss: 0.13606 | val_0_accuracy: 0.53693 |  0:01:25s\n",
      "⏱️ Epoch 220 took 0.37 seconds\n",
      "epoch 220| loss: 0.14268 | val_0_accuracy: 0.53543 |  0:01:25s\n",
      "⏱️ Epoch 221 took 0.35 seconds\n",
      "epoch 221| loss: 0.13183 | val_0_accuracy: 0.52994 |  0:01:25s\n",
      "⏱️ Epoch 222 took 0.38 seconds\n",
      "epoch 222| loss: 0.13859 | val_0_accuracy: 0.53593 |  0:01:26s\n",
      "⏱️ Epoch 223 took 0.45 seconds\n",
      "epoch 223| loss: 0.13526 | val_0_accuracy: 0.54391 |  0:01:26s\n",
      "⏱️ Epoch 224 took 0.42 seconds\n",
      "epoch 224| loss: 0.14038 | val_0_accuracy: 0.54042 |  0:01:27s\n",
      "⏱️ Epoch 225 took 0.37 seconds\n",
      "epoch 225| loss: 0.13297 | val_0_accuracy: 0.54291 |  0:01:27s\n",
      "⏱️ Epoch 226 took 0.40 seconds\n",
      "epoch 226| loss: 0.12469 | val_0_accuracy: 0.54291 |  0:01:27s\n",
      "⏱️ Epoch 227 took 0.41 seconds\n",
      "epoch 227| loss: 0.1442  | val_0_accuracy: 0.53992 |  0:01:28s\n",
      "⏱️ Epoch 228 took 0.42 seconds\n",
      "epoch 228| loss: 0.1316  | val_0_accuracy: 0.53443 |  0:01:28s\n",
      "⏱️ Epoch 229 took 0.35 seconds\n",
      "epoch 229| loss: 0.11914 | val_0_accuracy: 0.53743 |  0:01:29s\n",
      "⏱️ Epoch 230 took 0.38 seconds\n",
      "epoch 230| loss: 0.12787 | val_0_accuracy: 0.53842 |  0:01:29s\n",
      "⏱️ Epoch 231 took 0.40 seconds\n",
      "epoch 231| loss: 0.12455 | val_0_accuracy: 0.53493 |  0:01:29s\n",
      "⏱️ Epoch 232 took 0.37 seconds\n",
      "epoch 232| loss: 0.13491 | val_0_accuracy: 0.53792 |  0:01:30s\n",
      "⏱️ Epoch 233 took 0.41 seconds\n",
      "epoch 233| loss: 0.12922 | val_0_accuracy: 0.53693 |  0:01:30s\n",
      "⏱️ Epoch 234 took 0.39 seconds\n",
      "epoch 234| loss: 0.12141 | val_0_accuracy: 0.54291 |  0:01:31s\n",
      "⏱️ Epoch 235 took 0.36 seconds\n",
      "epoch 235| loss: 0.12617 | val_0_accuracy: 0.53593 |  0:01:31s\n",
      "⏱️ Epoch 236 took 0.38 seconds\n",
      "epoch 236| loss: 0.13224 | val_0_accuracy: 0.53792 |  0:01:31s\n",
      "⏱️ Epoch 237 took 0.40 seconds\n",
      "epoch 237| loss: 0.13112 | val_0_accuracy: 0.53244 |  0:01:32s\n",
      "⏱️ Epoch 238 took 0.39 seconds\n",
      "epoch 238| loss: 0.12908 | val_0_accuracy: 0.53244 |  0:01:32s\n",
      "⏱️ Epoch 239 took 0.38 seconds\n",
      "epoch 239| loss: 0.13935 | val_0_accuracy: 0.53842 |  0:01:32s\n",
      "⏱️ Epoch 240 took 0.37 seconds\n",
      "epoch 240| loss: 0.13796 | val_0_accuracy: 0.53693 |  0:01:33s\n",
      "⏱️ Epoch 241 took 0.40 seconds\n",
      "epoch 241| loss: 0.12841 | val_0_accuracy: 0.52844 |  0:01:33s\n",
      "⏱️ Epoch 242 took 0.39 seconds\n",
      "epoch 242| loss: 0.13287 | val_0_accuracy: 0.53892 |  0:01:34s\n",
      "⏱️ Epoch 243 took 0.44 seconds\n",
      "epoch 243| loss: 0.12716 | val_0_accuracy: 0.54291 |  0:01:34s\n",
      "⏱️ Epoch 244 took 0.37 seconds\n",
      "epoch 244| loss: 0.12671 | val_0_accuracy: 0.53992 |  0:01:34s\n",
      "⏱️ Epoch 245 took 0.40 seconds\n",
      "epoch 245| loss: 0.1268  | val_0_accuracy: 0.54291 |  0:01:35s\n",
      "⏱️ Epoch 246 took 0.40 seconds\n",
      "epoch 246| loss: 0.13812 | val_0_accuracy: 0.53942 |  0:01:35s\n",
      "⏱️ Epoch 247 took 0.40 seconds\n",
      "epoch 247| loss: 0.12816 | val_0_accuracy: 0.54591 |  0:01:36s\n",
      "⏱️ Epoch 248 took 0.40 seconds\n",
      "epoch 248| loss: 0.13712 | val_0_accuracy: 0.54641 |  0:01:36s\n",
      "⏱️ Epoch 249 took 0.36 seconds\n",
      "epoch 249| loss: 0.14318 | val_0_accuracy: 0.54491 |  0:01:36s\n",
      "⏱️ Epoch 250 took 0.39 seconds\n",
      "epoch 250| loss: 0.13035 | val_0_accuracy: 0.54092 |  0:01:37s\n",
      "⏱️ Epoch 251 took 0.39 seconds\n",
      "epoch 251| loss: 0.13136 | val_0_accuracy: 0.54291 |  0:01:37s\n",
      "⏱️ Epoch 252 took 0.40 seconds\n",
      "epoch 252| loss: 0.12989 | val_0_accuracy: 0.54042 |  0:01:38s\n",
      "⏱️ Epoch 253 took 0.40 seconds\n",
      "epoch 253| loss: 0.13665 | val_0_accuracy: 0.54341 |  0:01:38s\n",
      "⏱️ Epoch 254 took 0.40 seconds\n",
      "epoch 254| loss: 0.13246 | val_0_accuracy: 0.54391 |  0:01:38s\n",
      "⏱️ Epoch 255 took 0.41 seconds\n",
      "epoch 255| loss: 0.1359  | val_0_accuracy: 0.54092 |  0:01:39s\n",
      "⏱️ Epoch 256 took 0.38 seconds\n",
      "epoch 256| loss: 0.12329 | val_0_accuracy: 0.53693 |  0:01:39s\n",
      "⏱️ Epoch 257 took 0.38 seconds\n",
      "epoch 257| loss: 0.12729 | val_0_accuracy: 0.53693 |  0:01:40s\n",
      "⏱️ Epoch 258 took 0.38 seconds\n",
      "epoch 258| loss: 0.11993 | val_0_accuracy: 0.54042 |  0:01:40s\n",
      "⏱️ Epoch 259 took 0.40 seconds\n",
      "epoch 259| loss: 0.13155 | val_0_accuracy: 0.53693 |  0:01:40s\n",
      "⏱️ Epoch 260 took 0.36 seconds\n",
      "epoch 260| loss: 0.12192 | val_0_accuracy: 0.54192 |  0:01:41s\n",
      "⏱️ Epoch 261 took 0.36 seconds\n",
      "epoch 261| loss: 0.12942 | val_0_accuracy: 0.53892 |  0:01:41s\n",
      "⏱️ Epoch 262 took 0.38 seconds\n",
      "epoch 262| loss: 0.1232  | val_0_accuracy: 0.53643 |  0:01:41s\n",
      "⏱️ Epoch 263 took 0.37 seconds\n",
      "epoch 263| loss: 0.12422 | val_0_accuracy: 0.53244 |  0:01:42s\n",
      "⏱️ Epoch 264 took 0.43 seconds\n",
      "epoch 264| loss: 0.12804 | val_0_accuracy: 0.53543 |  0:01:42s\n",
      "⏱️ Epoch 265 took 0.37 seconds\n",
      "epoch 265| loss: 0.13226 | val_0_accuracy: 0.53593 |  0:01:43s\n",
      "⏱️ Epoch 266 took 0.36 seconds\n",
      "epoch 266| loss: 0.12827 | val_0_accuracy: 0.53443 |  0:01:43s\n",
      "⏱️ Epoch 267 took 0.37 seconds\n",
      "epoch 267| loss: 0.12759 | val_0_accuracy: 0.53693 |  0:01:43s\n",
      "⏱️ Epoch 268 took 0.40 seconds\n",
      "epoch 268| loss: 0.12407 | val_0_accuracy: 0.53892 |  0:01:44s\n",
      "⏱️ Epoch 269 took 0.39 seconds\n",
      "epoch 269| loss: 0.12877 | val_0_accuracy: 0.54391 |  0:01:44s\n",
      "⏱️ Epoch 270 took 0.39 seconds\n",
      "epoch 270| loss: 0.12195 | val_0_accuracy: 0.53593 |  0:01:44s\n",
      "⏱️ Epoch 271 took 0.37 seconds\n",
      "epoch 271| loss: 0.12217 | val_0_accuracy: 0.54291 |  0:01:45s\n",
      "⏱️ Epoch 272 took 0.39 seconds\n",
      "epoch 272| loss: 0.11651 | val_0_accuracy: 0.54142 |  0:01:45s\n",
      "⏱️ Epoch 273 took 0.41 seconds\n",
      "epoch 273| loss: 0.11767 | val_0_accuracy: 0.53543 |  0:01:46s\n",
      "⏱️ Epoch 274 took 0.37 seconds\n",
      "epoch 274| loss: 0.12985 | val_0_accuracy: 0.53643 |  0:01:46s\n",
      "⏱️ Epoch 275 took 0.36 seconds\n",
      "epoch 275| loss: 0.11459 | val_0_accuracy: 0.53942 |  0:01:46s\n",
      "⏱️ Epoch 276 took 0.40 seconds\n",
      "epoch 276| loss: 0.12189 | val_0_accuracy: 0.54142 |  0:01:47s\n",
      "⏱️ Epoch 277 took 0.39 seconds\n",
      "epoch 277| loss: 0.12108 | val_0_accuracy: 0.53593 |  0:01:47s\n",
      "⏱️ Epoch 278 took 0.37 seconds\n",
      "epoch 278| loss: 0.12361 | val_0_accuracy: 0.53792 |  0:01:48s\n",
      "⏱️ Epoch 279 took 0.37 seconds\n",
      "epoch 279| loss: 0.1296  | val_0_accuracy: 0.53593 |  0:01:48s\n",
      "⏱️ Epoch 280 took 0.38 seconds\n",
      "epoch 280| loss: 0.1167  | val_0_accuracy: 0.53443 |  0:01:48s\n",
      "⏱️ Epoch 281 took 0.43 seconds\n",
      "epoch 281| loss: 0.11761 | val_0_accuracy: 0.53443 |  0:01:49s\n",
      "⏱️ Epoch 282 took 0.53 seconds\n",
      "epoch 282| loss: 0.12121 | val_0_accuracy: 0.53892 |  0:01:49s\n",
      "⏱️ Epoch 283 took 0.39 seconds\n",
      "epoch 283| loss: 0.11311 | val_0_accuracy: 0.54192 |  0:01:50s\n",
      "⏱️ Epoch 284 took 0.40 seconds\n",
      "epoch 284| loss: 0.10249 | val_0_accuracy: 0.53892 |  0:01:50s\n",
      "⏱️ Epoch 285 took 0.40 seconds\n",
      "epoch 285| loss: 0.10746 | val_0_accuracy: 0.54391 |  0:01:50s\n",
      "⏱️ Epoch 286 took 0.40 seconds\n",
      "epoch 286| loss: 0.10375 | val_0_accuracy: 0.5484  |  0:01:51s\n",
      "⏱️ Epoch 287 took 0.37 seconds\n",
      "epoch 287| loss: 0.11127 | val_0_accuracy: 0.54741 |  0:01:51s\n",
      "⏱️ Epoch 288 took 0.43 seconds\n",
      "epoch 288| loss: 0.1091  | val_0_accuracy: 0.5504  |  0:01:52s\n",
      "⏱️ Epoch 289 took 0.37 seconds\n",
      "epoch 289| loss: 0.11307 | val_0_accuracy: 0.54491 |  0:01:52s\n",
      "⏱️ Epoch 290 took 0.37 seconds\n",
      "epoch 290| loss: 0.11297 | val_0_accuracy: 0.54541 |  0:01:52s\n",
      "⏱️ Epoch 291 took 0.36 seconds\n",
      "epoch 291| loss: 0.1138  | val_0_accuracy: 0.53792 |  0:01:53s\n",
      "⏱️ Epoch 292 took 0.37 seconds\n",
      "epoch 292| loss: 0.11112 | val_0_accuracy: 0.54391 |  0:01:53s\n",
      "⏱️ Epoch 293 took 0.38 seconds\n",
      "epoch 293| loss: 0.11078 | val_0_accuracy: 0.54291 |  0:01:54s\n",
      "⏱️ Epoch 294 took 0.39 seconds\n",
      "epoch 294| loss: 0.10832 | val_0_accuracy: 0.54291 |  0:01:54s\n",
      "⏱️ Epoch 295 took 0.42 seconds\n",
      "\n",
      "Early stopping occurred at epoch 294 with best_epoch = 94 and best_val_0_accuracy = 0.5514\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest, Ytest_decoded, le = split_data(traindata, testdata)\n",
    "best_tabnet_params = find_best_tabnet(Xtrain, Ytrain)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split 80% train, 20% validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    Xtrain, Ytrain,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=Ytrain\n",
    ")\n",
    "\n",
    "# Re-initialize TabNet with best params\n",
    "final_model = TabNetClassifier(\n",
    "    **{k: v for k, v in best_tabnet_params.items()},\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "epoch_timer = EpochTimer()\n",
    "\n",
    "# Retrain on full training data\n",
    "final_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    max_epochs=1000,\n",
    "    patience=200,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    callbacks=[epoch_timer]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, _, le = split_data(traindata, testdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "file_path = os.path.join(\"results\", f\"gtd{partition}.txt\")\n",
    "\n",
    "# Predict class indices for test set\n",
    "y_pred = final_model.predict(Xtest)\n",
    "y_proba = final_model.predict_proba(Xtest)\n",
    "y_pred_decoded = le.inverse_transform(y_pred)\n",
    "y_true_decoded = le.inverse_transform(Ytest)\n",
    "\n",
    "# Make sure the directory exists\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Compute accuracy from decoded labels\n",
    "acc = accuracy_score(y_true_decoded, y_pred_decoded)\n",
    "\n",
    "# Write metrics to file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "    file.write(f\"Precision weighted: {precision_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"Recall weighted: {recall_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score weighted: {f1_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"Precision micro: {precision_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"Recall micro: {recall_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score micro: {f1_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"Precision macro: {precision_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"Recall macro: {recall_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score macro: {f1_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"roc auc weighted: {roc_auc_score(y_true_decoded, y_proba, multi_class='ovr', average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"roc auc macro: {roc_auc_score(y_true_decoded, y_proba, multi_class='ovr', average='macro'):.4f}\\n\")\n",
    "    file.write(f\"roc auc micro: {roc_auc_score(y_true_decoded, y_proba, multi_class='ovr', average='micro'):.4f}\\n\")\n",
    "\n",
    "with open(f\"results/epoch_time_gtd{partition}.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(str(x) for x in epoch_timer.epoch_times))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "                          Abu Sayyaf Group (ASG)       0.65      0.45      0.53       144\n",
      "        African National Congress (South Africa)       0.88      0.76      0.81       144\n",
      "                                Al-Qaida in Iraq       0.68      0.73      0.70       144\n",
      "        Al-Qaida in the Arabian Peninsula (AQAP)       0.57      0.64      0.61       144\n",
      "                                      Al-Shabaab       0.68      0.74      0.70       144\n",
      "             Basque Fatherland and Freedom (ETA)       0.90      0.72      0.80       144\n",
      "                                      Boko Haram       0.62      0.44      0.52       144\n",
      "  Communist Party of India - Maoist (CPI-Maoist)       0.32      0.36      0.34       144\n",
      "       Corsican National Liberation Front (FLNC)       0.93      0.90      0.92       144\n",
      "                       Donetsk People's Republic       0.77      0.81      0.79       144\n",
      "Farabundo Marti National Liberation Front (FMLN)       0.68      0.75      0.72       144\n",
      "                               Fulani extremists       0.17      0.32      0.22       144\n",
      "                 Houthi extremists (Ansar Allah)       0.61      0.42      0.50       144\n",
      "                     Irish Republican Army (IRA)       0.84      0.85      0.84       144\n",
      "     Islamic State of Iraq and the Levant (ISIL)       0.45      0.39      0.42       144\n",
      "                  Kurdistan Workers' Party (PKK)       0.54      0.37      0.44       144\n",
      "         Liberation Tigers of Tamil Eelam (LTTE)       0.64      0.71      0.67       144\n",
      "         Manuel Rodriguez Patriotic Front (FPMR)       0.93      0.90      0.92       144\n",
      "                                         Maoists       0.17      0.25      0.20       144\n",
      "                               Muslim extremists       0.39      0.47      0.42       144\n",
      "      National Liberation Army of Colombia (ELN)       0.54      0.55      0.55       144\n",
      "                         New People's Army (NPA)       0.25      0.24      0.24       144\n",
      "               Nicaraguan Democratic Force (FDN)       0.64      0.53      0.58       144\n",
      "                                    Palestinians       0.71      0.81      0.76       144\n",
      "   Revolutionary Armed Forces of Colombia (FARC)       0.25      0.24      0.24       144\n",
      "                               Shining Path (SL)       0.49      0.28      0.36       144\n",
      "                                 Sikh Extremists       0.77      0.69      0.73       144\n",
      "                                         Taliban       0.36      0.31      0.34       144\n",
      "                 Tehrik-i-Taliban Pakistan (TTP)       0.64      0.60      0.62       144\n",
      "       Tupac Amaru Revolutionary Movement (MRTA)       0.63      0.83      0.71       144\n",
      "\n",
      "                                        accuracy                           0.57      4320\n",
      "                                       macro avg       0.59      0.57      0.57      4320\n",
      "                                    weighted avg       0.59      0.57      0.57      4320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Ytest_decoded, y_pred_decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(cm_normalized,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels,\n",
    "                cmap=\"viridis\",\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "    plt.title(f\"Normalized Confusion Matrix\", fontsize=18)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "    plt.ylabel(\"True Label\", fontsize=14)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = f\"results/confusion_matrix_partition_{partition}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved confusion matrix for partition {partition} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved confusion matrix for partition 478 to results/confusion_matrix_partition_478.png\n"
     ]
    }
   ],
   "source": [
    "# Get all unique class labels from the truths\n",
    "class_labels = np.unique(Ytest_decoded)\n",
    "\n",
    "plot_confusion_matrix(Ytest_decoded, y_pred_decoded, labels=class_labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TabNet)",
   "language": "python",
   "name": "tabnet-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
