{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-tabnet\n",
      "  Using cached pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (1.5.0)\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (2.3.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (4.66.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->pytorch-tabnet) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Using cached pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: pytorch-tabnet\n",
      "Successfully installed pytorch-tabnet-4.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install pytorch-tabnet --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = f'../../../../data/top30groups/LongLatCombined/train1/train{partition}.csv'\n",
    "testpath = f'../../../../data/top30groups/LongLatCombined/test1/test{partition}.csv'\n",
    "\n",
    "traindata = pd.read_csv(trainpath, encoding='ISO-8859-1')\n",
    "testdata = pd.read_csv(testpath, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def split_data(dftrain, dftest):\n",
    "    Xtrain = dftrain.drop(columns=['gname']).values.astype(float)\n",
    "    Ytrain = dftrain['gname'].values\n",
    "    Xtest = dftest.drop(columns=['gname']).values.astype(float)\n",
    "    Ytest = dftest['gname'].values\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    Ytrain = le.fit_transform(Ytrain)\n",
    "    Ytest = le.transform(Ytest)\n",
    "\n",
    "    #y_pred_decoded = model.label_encoder.inverse_transform(y_pred)\n",
    "    y_true_decoded = le.inverse_transform(Ytest)\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest, y_true_decoded, le\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "class TabNetClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, lambda_sparse=1e-3, optimizer_params=None):\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.optimizer_params = optimizer_params or {'lr': 0.01}\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = TabNetClassifier(\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            lambda_sparse=self.lambda_sparse,\n",
    "            optimizer_params=self.optimizer_params,\n",
    "            seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        self.model.fit(\n",
    "            X, y,\n",
    "            eval_set=[(X, y)],\n",
    "            max_epochs=500,\n",
    "            patience=120,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            eval_metric=['accuracy']\n",
    "        )\n",
    "\n",
    "        self.classes_ = np.unique(y)  \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return (preds == y).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_tabnet.sklearn import TabNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "\n",
    "def find_best_tabnet(Xtrain, Ytrain, n_iter=20):\n",
    "    print(\"Starting TabNet grid search\")\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "    param_dist = {\n",
    "        'n_d': [8, 16, 24],\n",
    "        'n_a': [8, 16, 24],\n",
    "        'n_steps': [3, 4, 5],\n",
    "        'gamma': [1.0, 1.3, 1.5],\n",
    "        'lambda_sparse': [1e-4, 1e-3, 1e-2],\n",
    "        'optimizer_params': [{'lr': 0.01}]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=TabNetClassifierWrapper(),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='accuracy',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(Xtrain, Ytrain)\n",
    "    print(\"Best parameters:\", random_search.best_params_)\n",
    "    print(\"Best accuracy:\", random_search.best_score_)\n",
    "\n",
    "    return random_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.callbacks import Callback\n",
    "import time\n",
    "\n",
    "class EpochTimer(Callback):\n",
    "    def __init__(self):\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch_idx, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch_idx, logs=None):\n",
    "        duration = time.time() - self.start_time\n",
    "        self.epoch_times.append(duration)\n",
    "        print(f\" Epoch {epoch_idx + 1} took {duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TabNet grid search\n",
      "CUDA available: True\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 459 with best_epoch = 339 and best_val_0_accuracy = 0.95634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 380 with best_epoch = 260 and best_val_0_accuracy = 0.94511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 499 and best_val_0_accuracy = 0.96045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 460 and best_val_0_accuracy = 0.96919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 499 and best_val_0_accuracy = 0.96607\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 499 and best_val_0_accuracy = 0.96382\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 492 and best_val_0_accuracy = 0.96881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 390 and best_val_0_accuracy = 0.97081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 478 and best_val_0_accuracy = 0.96083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 484 and best_val_0_accuracy = 0.97168\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 462 and best_val_0_accuracy = 0.9602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 425 with best_epoch = 305 and best_val_0_accuracy = 0.936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 458 with best_epoch = 338 and best_val_0_accuracy = 0.9491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 467 with best_epoch = 347 and best_val_0_accuracy = 0.94449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 496 with best_epoch = 376 and best_val_0_accuracy = 0.96519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 499 and best_val_0_accuracy = 0.96095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 409 and best_val_0_accuracy = 0.95646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 490 and best_val_0_accuracy = 0.96158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 454 and best_val_0_accuracy = 0.96307\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 471 and best_val_0_accuracy = 0.95971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 466 and best_val_0_accuracy = 0.96844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 417 and best_val_0_accuracy = 0.9486\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 463 and best_val_0_accuracy = 0.95709\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 494 and best_val_0_accuracy = 0.95908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 473 and best_val_0_accuracy = 0.95322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 495 and best_val_0_accuracy = 0.95971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 469 and best_val_0_accuracy = 0.96295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 486 and best_val_0_accuracy = 0.95072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 493 and best_val_0_accuracy = 0.96095\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 484 and best_val_0_accuracy = 0.95821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 471 and best_val_0_accuracy = 0.95609\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 462 and best_val_0_accuracy = 0.94561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 485 and best_val_0_accuracy = 0.96682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 448 and best_val_0_accuracy = 0.96357\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 480 and best_val_0_accuracy = 0.96582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 478 and best_val_0_accuracy = 0.97156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 465 and best_val_0_accuracy = 0.96719\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 456 and best_val_0_accuracy = 0.96332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 474 and best_val_0_accuracy = 0.96332\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 425 and best_val_0_accuracy = 0.95721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 491 and best_val_0_accuracy = 0.95958\n",
      "\n",
      "Early stopping occurred at epoch 420 with best_epoch = 300 and best_val_0_accuracy = 0.94548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 443 and best_val_0_accuracy = 0.96245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 425 and best_val_0_accuracy = 0.96245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 494 and best_val_0_accuracy = 0.95983\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 480 and best_val_0_accuracy = 0.96083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 449 and best_val_0_accuracy = 0.95908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 487 and best_val_0_accuracy = 0.95871\n",
      "\n",
      "Early stopping occurred at epoch 467 with best_epoch = 347 and best_val_0_accuracy = 0.95472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 483 and best_val_0_accuracy = 0.94973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 484 and best_val_0_accuracy = 0.95185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 408 with best_epoch = 288 and best_val_0_accuracy = 0.936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 491 and best_val_0_accuracy = 0.95197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 465 and best_val_0_accuracy = 0.94898\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 452 and best_val_0_accuracy = 0.95534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 456 and best_val_0_accuracy = 0.95384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 422 and best_val_0_accuracy = 0.94673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 470 and best_val_0_accuracy = 0.94536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 488 with best_epoch = 368 and best_val_0_accuracy = 0.93263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 465 and best_val_0_accuracy = 0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 484 and best_val_0_accuracy = 0.95147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 495 and best_val_0_accuracy = 0.92303\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 458 and best_val_0_accuracy = 0.94923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 489 and best_val_0_accuracy = 0.94885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 425 with best_epoch = 305 and best_val_0_accuracy = 0.92777\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 471 and best_val_0_accuracy = 0.93538\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 388 and best_val_0_accuracy = 0.94598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 494 and best_val_0_accuracy = 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 374 with best_epoch = 254 and best_val_0_accuracy = 0.9496\n",
      "\n",
      "Early stopping occurred at epoch 460 with best_epoch = 340 and best_val_0_accuracy = 0.94112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 467 with best_epoch = 347 and best_val_0_accuracy = 0.94573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 456 and best_val_0_accuracy = 0.94162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 490 and best_val_0_accuracy = 0.95035\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 429 and best_val_0_accuracy = 0.95085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 463 and best_val_0_accuracy = 0.9607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 498 and best_val_0_accuracy = 0.94698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 470 and best_val_0_accuracy = 0.97293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 486 and best_val_0_accuracy = 0.96919\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 497 and best_val_0_accuracy = 0.95372\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 499 and best_val_0_accuracy = 0.95097\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 495 and best_val_0_accuracy = 0.96994\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 474 and best_val_0_accuracy = 0.95272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 468 and best_val_0_accuracy = 0.95235\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 492 and best_val_0_accuracy = 0.97368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 470 and best_val_0_accuracy = 0.96794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 426 and best_val_0_accuracy = 0.94124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 385 with best_epoch = 265 and best_val_0_accuracy = 0.92927\n",
      "\n",
      "Early stopping occurred at epoch 488 with best_epoch = 368 and best_val_0_accuracy = 0.95883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 451 and best_val_0_accuracy = 0.93787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 490 and best_val_0_accuracy = 0.96582\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 483 and best_val_0_accuracy = 0.96108\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 450 and best_val_0_accuracy = 0.96482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 417 and best_val_0_accuracy = 0.9511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 490 and best_val_0_accuracy = 0.93513\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 485 and best_val_0_accuracy = 0.94311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 489 and best_val_0_accuracy = 0.95684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 405 and best_val_0_accuracy = 0.94548\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 419 and best_val_0_accuracy = 0.95447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 495 and best_val_0_accuracy = 0.95671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 496 and best_val_0_accuracy = 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 478 and best_val_0_accuracy = 0.96118\n",
      "Best parameters: {'optimizer_params': {'lr': 0.01}, 'n_steps': 3, 'n_d': 8, 'n_a': 16, 'lambda_sparse': 0.0001, 'gamma': 1.0}\n",
      "Best accuracy: 0.9315369261477044\n",
      "epoch 0  | loss: 3.80875 | val_0_accuracy: 0.07086 |  0:00:00s\n",
      " Epoch 1 took 0.17 seconds\n",
      "epoch 1  | loss: 3.35883 | val_0_accuracy: 0.05439 |  0:00:00s\n",
      " Epoch 2 took 0.16 seconds\n",
      "epoch 2  | loss: 3.18871 | val_0_accuracy: 0.07984 |  0:00:00s\n",
      " Epoch 3 took 0.17 seconds\n",
      "epoch 3  | loss: 3.0548  | val_0_accuracy: 0.10729 |  0:00:00s\n",
      " Epoch 4 took 0.17 seconds\n",
      "epoch 4  | loss: 2.9076  | val_0_accuracy: 0.10429 |  0:00:00s\n",
      " Epoch 5 took 0.16 seconds\n",
      "epoch 5  | loss: 2.7534  | val_0_accuracy: 0.09431 |  0:00:00s\n",
      " Epoch 6 took 0.16 seconds\n",
      "epoch 6  | loss: 2.60771 | val_0_accuracy: 0.13273 |  0:00:01s\n",
      " Epoch 7 took 0.17 seconds\n",
      "epoch 7  | loss: 2.44963 | val_0_accuracy: 0.15519 |  0:00:01s\n",
      " Epoch 8 took 0.16 seconds\n",
      "epoch 8  | loss: 2.28476 | val_0_accuracy: 0.16367 |  0:00:01s\n",
      " Epoch 9 took 0.17 seconds\n",
      "epoch 9  | loss: 2.09951 | val_0_accuracy: 0.20509 |  0:00:01s\n",
      " Epoch 10 took 0.16 seconds\n",
      "epoch 10 | loss: 1.94304 | val_0_accuracy: 0.22405 |  0:00:01s\n",
      " Epoch 11 took 0.16 seconds\n",
      "epoch 11 | loss: 1.79744 | val_0_accuracy: 0.25599 |  0:00:01s\n",
      " Epoch 12 took 0.17 seconds\n",
      "epoch 12 | loss: 1.66865 | val_0_accuracy: 0.29142 |  0:00:02s\n",
      " Epoch 13 took 0.17 seconds\n",
      "epoch 13 | loss: 1.53941 | val_0_accuracy: 0.32535 |  0:00:02s\n",
      " Epoch 14 took 0.20 seconds\n",
      "epoch 14 | loss: 1.43367 | val_0_accuracy: 0.31737 |  0:00:02s\n",
      " Epoch 15 took 0.16 seconds\n",
      "epoch 15 | loss: 1.34095 | val_0_accuracy: 0.32884 |  0:00:02s\n",
      " Epoch 16 took 0.30 seconds\n",
      "epoch 16 | loss: 1.21404 | val_0_accuracy: 0.36677 |  0:00:02s\n",
      " Epoch 17 took 0.18 seconds\n",
      "epoch 17 | loss: 1.16422 | val_0_accuracy: 0.40619 |  0:00:03s\n",
      " Epoch 18 took 0.20 seconds\n",
      "epoch 18 | loss: 1.10028 | val_0_accuracy: 0.41018 |  0:00:03s\n",
      " Epoch 19 took 0.17 seconds\n",
      "epoch 19 | loss: 1.01296 | val_0_accuracy: 0.40719 |  0:00:03s\n",
      " Epoch 20 took 0.16 seconds\n",
      "epoch 20 | loss: 0.96214 | val_0_accuracy: 0.44611 |  0:00:03s\n",
      " Epoch 21 took 0.18 seconds\n",
      "epoch 21 | loss: 0.8616  | val_0_accuracy: 0.49701 |  0:00:03s\n",
      " Epoch 22 took 0.18 seconds\n",
      "epoch 22 | loss: 0.82493 | val_0_accuracy: 0.42814 |  0:00:04s\n",
      " Epoch 23 took 0.16 seconds\n",
      "epoch 23 | loss: 0.83456 | val_0_accuracy: 0.54142 |  0:00:04s\n",
      " Epoch 24 took 0.17 seconds\n",
      "epoch 24 | loss: 0.77326 | val_0_accuracy: 0.54242 |  0:00:04s\n",
      " Epoch 25 took 0.16 seconds\n",
      "epoch 25 | loss: 0.72577 | val_0_accuracy: 0.55888 |  0:00:04s\n",
      " Epoch 26 took 0.16 seconds\n",
      "epoch 26 | loss: 0.7168  | val_0_accuracy: 0.54691 |  0:00:04s\n",
      " Epoch 27 took 0.16 seconds\n",
      "epoch 27 | loss: 0.68893 | val_0_accuracy: 0.5998  |  0:00:04s\n",
      " Epoch 28 took 0.17 seconds\n",
      "epoch 28 | loss: 0.67163 | val_0_accuracy: 0.6487  |  0:00:05s\n",
      " Epoch 29 took 0.16 seconds\n",
      "epoch 29 | loss: 0.69646 | val_0_accuracy: 0.58084 |  0:00:05s\n",
      " Epoch 30 took 0.16 seconds\n",
      "epoch 30 | loss: 0.66038 | val_0_accuracy: 0.67365 |  0:00:05s\n",
      " Epoch 31 took 0.16 seconds\n",
      "epoch 31 | loss: 0.64055 | val_0_accuracy: 0.68613 |  0:00:05s\n",
      " Epoch 32 took 0.17 seconds\n",
      "epoch 32 | loss: 0.61351 | val_0_accuracy: 0.75    |  0:00:05s\n",
      " Epoch 33 took 0.17 seconds\n",
      "epoch 33 | loss: 0.5971  | val_0_accuracy: 0.71357 |  0:00:05s\n",
      " Epoch 34 took 0.16 seconds\n",
      "epoch 34 | loss: 0.60285 | val_0_accuracy: 0.74601 |  0:00:05s\n",
      " Epoch 35 took 0.17 seconds\n",
      "epoch 35 | loss: 0.6052  | val_0_accuracy: 0.76198 |  0:00:06s\n",
      " Epoch 36 took 0.17 seconds\n",
      "epoch 36 | loss: 0.56346 | val_0_accuracy: 0.72206 |  0:00:06s\n",
      " Epoch 37 took 0.16 seconds\n",
      "epoch 37 | loss: 0.56897 | val_0_accuracy: 0.76647 |  0:00:06s\n",
      " Epoch 38 took 0.16 seconds\n",
      "epoch 38 | loss: 0.58893 | val_0_accuracy: 0.79591 |  0:00:06s\n",
      " Epoch 39 took 0.17 seconds\n",
      "epoch 39 | loss: 0.54905 | val_0_accuracy: 0.78293 |  0:00:06s\n",
      " Epoch 40 took 0.16 seconds\n",
      "epoch 40 | loss: 0.54815 | val_0_accuracy: 0.77994 |  0:00:06s\n",
      " Epoch 41 took 0.16 seconds\n",
      "epoch 41 | loss: 0.54196 | val_0_accuracy: 0.79092 |  0:00:07s\n",
      " Epoch 42 took 0.17 seconds\n",
      "epoch 42 | loss: 0.55745 | val_0_accuracy: 0.80639 |  0:00:07s\n",
      " Epoch 43 took 0.16 seconds\n",
      "epoch 43 | loss: 0.55788 | val_0_accuracy: 0.83583 |  0:00:07s\n",
      " Epoch 44 took 0.17 seconds\n",
      "epoch 44 | loss: 0.53842 | val_0_accuracy: 0.79691 |  0:00:07s\n",
      " Epoch 45 took 0.16 seconds\n",
      "epoch 45 | loss: 0.51289 | val_0_accuracy: 0.82136 |  0:00:07s\n",
      " Epoch 46 took 0.17 seconds\n",
      "epoch 46 | loss: 0.50149 | val_0_accuracy: 0.83832 |  0:00:07s\n",
      " Epoch 47 took 0.17 seconds\n",
      "epoch 47 | loss: 0.50654 | val_0_accuracy: 0.83034 |  0:00:08s\n",
      " Epoch 48 took 0.19 seconds\n",
      "epoch 48 | loss: 0.48324 | val_0_accuracy: 0.82834 |  0:00:08s\n",
      " Epoch 49 took 0.16 seconds\n",
      "epoch 49 | loss: 0.50362 | val_0_accuracy: 0.83633 |  0:00:08s\n",
      " Epoch 50 took 0.17 seconds\n",
      "epoch 50 | loss: 0.57156 | val_0_accuracy: 0.83683 |  0:00:08s\n",
      " Epoch 51 took 0.16 seconds\n",
      "epoch 51 | loss: 0.51398 | val_0_accuracy: 0.83533 |  0:00:08s\n",
      " Epoch 52 took 0.16 seconds\n",
      "epoch 52 | loss: 0.50426 | val_0_accuracy: 0.81487 |  0:00:08s\n",
      " Epoch 53 took 0.16 seconds\n",
      "epoch 53 | loss: 0.48171 | val_0_accuracy: 0.83483 |  0:00:09s\n",
      " Epoch 54 took 0.17 seconds\n",
      "epoch 54 | loss: 0.47302 | val_0_accuracy: 0.8523  |  0:00:09s\n",
      " Epoch 55 took 0.17 seconds\n",
      "epoch 55 | loss: 0.43869 | val_0_accuracy: 0.86627 |  0:00:09s\n",
      " Epoch 56 took 0.17 seconds\n",
      "epoch 56 | loss: 0.46496 | val_0_accuracy: 0.84381 |  0:00:09s\n",
      " Epoch 57 took 0.16 seconds\n",
      "epoch 57 | loss: 0.46539 | val_0_accuracy: 0.85978 |  0:00:09s\n",
      " Epoch 58 took 0.15 seconds\n",
      "epoch 58 | loss: 0.43989 | val_0_accuracy: 0.86327 |  0:00:09s\n",
      " Epoch 59 took 0.18 seconds\n",
      "epoch 59 | loss: 0.43677 | val_0_accuracy: 0.81936 |  0:00:10s\n",
      " Epoch 60 took 0.17 seconds\n",
      "epoch 60 | loss: 0.42936 | val_0_accuracy: 0.85379 |  0:00:10s\n",
      " Epoch 61 took 0.16 seconds\n",
      "epoch 61 | loss: 0.41337 | val_0_accuracy: 0.86228 |  0:00:10s\n",
      " Epoch 62 took 0.17 seconds\n",
      "epoch 62 | loss: 0.396   | val_0_accuracy: 0.8478  |  0:00:10s\n",
      " Epoch 63 took 0.16 seconds\n",
      "epoch 63 | loss: 0.40889 | val_0_accuracy: 0.87874 |  0:00:10s\n",
      " Epoch 64 took 0.17 seconds\n",
      "epoch 64 | loss: 0.43046 | val_0_accuracy: 0.87874 |  0:00:10s\n",
      " Epoch 65 took 0.16 seconds\n",
      "epoch 65 | loss: 0.40493 | val_0_accuracy: 0.88124 |  0:00:11s\n",
      " Epoch 66 took 0.16 seconds\n",
      "epoch 66 | loss: 0.39343 | val_0_accuracy: 0.86976 |  0:00:11s\n",
      " Epoch 67 took 0.16 seconds\n",
      "epoch 67 | loss: 0.36089 | val_0_accuracy: 0.87725 |  0:00:11s\n",
      " Epoch 68 took 0.16 seconds\n",
      "epoch 68 | loss: 0.37197 | val_0_accuracy: 0.88822 |  0:00:11s\n",
      " Epoch 69 took 0.16 seconds\n",
      "epoch 69 | loss: 0.37224 | val_0_accuracy: 0.88423 |  0:00:11s\n",
      " Epoch 70 took 0.17 seconds\n",
      "epoch 70 | loss: 0.36361 | val_0_accuracy: 0.88772 |  0:00:11s\n",
      " Epoch 71 took 0.16 seconds\n",
      "epoch 71 | loss: 0.36016 | val_0_accuracy: 0.91068 |  0:00:12s\n",
      " Epoch 72 took 0.16 seconds\n",
      "epoch 72 | loss: 0.3593  | val_0_accuracy: 0.89571 |  0:00:12s\n",
      " Epoch 73 took 0.16 seconds\n",
      "epoch 73 | loss: 0.36807 | val_0_accuracy: 0.87675 |  0:00:12s\n",
      " Epoch 74 took 0.16 seconds\n",
      "epoch 74 | loss: 0.35349 | val_0_accuracy: 0.88723 |  0:00:12s\n",
      " Epoch 75 took 0.17 seconds\n",
      "epoch 75 | loss: 0.37461 | val_0_accuracy: 0.87176 |  0:00:12s\n",
      " Epoch 76 took 0.16 seconds\n",
      "epoch 76 | loss: 0.33687 | val_0_accuracy: 0.86627 |  0:00:12s\n",
      " Epoch 77 took 0.16 seconds\n",
      "epoch 77 | loss: 0.35493 | val_0_accuracy: 0.89671 |  0:00:13s\n",
      " Epoch 78 took 0.17 seconds\n",
      "epoch 78 | loss: 0.34498 | val_0_accuracy: 0.90419 |  0:00:13s\n",
      " Epoch 79 took 0.17 seconds\n",
      "epoch 79 | loss: 0.33432 | val_0_accuracy: 0.90319 |  0:00:13s\n",
      " Epoch 80 took 0.16 seconds\n",
      "epoch 80 | loss: 0.34134 | val_0_accuracy: 0.85429 |  0:00:13s\n",
      " Epoch 81 took 0.17 seconds\n",
      "epoch 81 | loss: 0.33595 | val_0_accuracy: 0.90519 |  0:00:13s\n",
      " Epoch 82 took 0.16 seconds\n",
      "epoch 82 | loss: 0.34256 | val_0_accuracy: 0.87625 |  0:00:13s\n",
      " Epoch 83 took 0.16 seconds\n",
      "epoch 83 | loss: 0.32445 | val_0_accuracy: 0.84581 |  0:00:14s\n",
      " Epoch 84 took 0.17 seconds\n",
      "epoch 84 | loss: 0.34233 | val_0_accuracy: 0.87275 |  0:00:14s\n",
      " Epoch 85 took 0.18 seconds\n",
      "epoch 85 | loss: 0.34557 | val_0_accuracy: 0.89172 |  0:00:14s\n",
      " Epoch 86 took 0.17 seconds\n",
      "epoch 86 | loss: 0.32411 | val_0_accuracy: 0.9017  |  0:00:14s\n",
      " Epoch 87 took 0.16 seconds\n",
      "epoch 87 | loss: 0.32064 | val_0_accuracy: 0.88872 |  0:00:14s\n",
      " Epoch 88 took 0.16 seconds\n",
      "epoch 88 | loss: 0.32647 | val_0_accuracy: 0.90818 |  0:00:14s\n",
      " Epoch 89 took 0.18 seconds\n",
      "epoch 89 | loss: 0.30985 | val_0_accuracy: 0.89621 |  0:00:15s\n",
      " Epoch 90 took 0.16 seconds\n",
      "epoch 90 | loss: 0.33651 | val_0_accuracy: 0.89072 |  0:00:15s\n",
      " Epoch 91 took 0.16 seconds\n",
      "epoch 91 | loss: 0.33129 | val_0_accuracy: 0.88872 |  0:00:15s\n",
      " Epoch 92 took 0.29 seconds\n",
      "epoch 92 | loss: 0.35466 | val_0_accuracy: 0.87226 |  0:00:15s\n",
      " Epoch 93 took 0.15 seconds\n",
      "epoch 93 | loss: 0.33786 | val_0_accuracy: 0.90519 |  0:00:15s\n",
      " Epoch 94 took 0.17 seconds\n",
      "epoch 94 | loss: 0.31112 | val_0_accuracy: 0.89222 |  0:00:16s\n",
      " Epoch 95 took 0.16 seconds\n",
      "epoch 95 | loss: 0.33253 | val_0_accuracy: 0.86876 |  0:00:16s\n",
      " Epoch 96 took 0.16 seconds\n",
      "epoch 96 | loss: 0.34565 | val_0_accuracy: 0.86527 |  0:00:16s\n",
      " Epoch 97 took 0.16 seconds\n",
      "epoch 97 | loss: 0.31857 | val_0_accuracy: 0.90369 |  0:00:16s\n",
      " Epoch 98 took 0.16 seconds\n",
      "epoch 98 | loss: 0.31326 | val_0_accuracy: 0.89621 |  0:00:16s\n",
      " Epoch 99 took 0.17 seconds\n",
      "epoch 99 | loss: 0.34777 | val_0_accuracy: 0.9002  |  0:00:16s\n",
      " Epoch 100 took 0.17 seconds\n",
      "epoch 100| loss: 0.31791 | val_0_accuracy: 0.8992  |  0:00:17s\n",
      " Epoch 101 took 0.16 seconds\n",
      "epoch 101| loss: 0.31127 | val_0_accuracy: 0.91018 |  0:00:17s\n",
      " Epoch 102 took 0.16 seconds\n",
      "epoch 102| loss: 0.30276 | val_0_accuracy: 0.89222 |  0:00:17s\n",
      " Epoch 103 took 0.15 seconds\n",
      "epoch 103| loss: 0.28206 | val_0_accuracy: 0.9002  |  0:00:17s\n",
      " Epoch 104 took 0.16 seconds\n",
      "epoch 104| loss: 0.27911 | val_0_accuracy: 0.90669 |  0:00:17s\n",
      " Epoch 105 took 0.16 seconds\n",
      "epoch 105| loss: 0.28354 | val_0_accuracy: 0.88673 |  0:00:17s\n",
      " Epoch 106 took 0.16 seconds\n",
      "epoch 106| loss: 0.30141 | val_0_accuracy: 0.88224 |  0:00:17s\n",
      " Epoch 107 took 0.16 seconds\n",
      "epoch 107| loss: 0.30883 | val_0_accuracy: 0.90868 |  0:00:18s\n",
      " Epoch 108 took 0.16 seconds\n",
      "epoch 108| loss: 0.32212 | val_0_accuracy: 0.89521 |  0:00:18s\n",
      " Epoch 109 took 0.17 seconds\n",
      "epoch 109| loss: 0.3058  | val_0_accuracy: 0.91018 |  0:00:18s\n",
      " Epoch 110 took 0.17 seconds\n",
      "epoch 110| loss: 0.31566 | val_0_accuracy: 0.8992  |  0:00:18s\n",
      " Epoch 111 took 0.18 seconds\n",
      "epoch 111| loss: 0.32646 | val_0_accuracy: 0.89521 |  0:00:18s\n",
      " Epoch 112 took 0.16 seconds\n",
      "epoch 112| loss: 0.31344 | val_0_accuracy: 0.87924 |  0:00:18s\n",
      " Epoch 113 took 0.17 seconds\n",
      "epoch 113| loss: 0.29249 | val_0_accuracy: 0.88623 |  0:00:19s\n",
      " Epoch 114 took 0.16 seconds\n",
      "epoch 114| loss: 0.29828 | val_0_accuracy: 0.8987  |  0:00:19s\n",
      " Epoch 115 took 0.16 seconds\n",
      "epoch 115| loss: 0.30745 | val_0_accuracy: 0.91018 |  0:00:19s\n",
      " Epoch 116 took 0.16 seconds\n",
      "epoch 116| loss: 0.30389 | val_0_accuracy: 0.89371 |  0:00:19s\n",
      " Epoch 117 took 0.16 seconds\n",
      "epoch 117| loss: 0.30151 | val_0_accuracy: 0.90519 |  0:00:19s\n",
      " Epoch 118 took 0.17 seconds\n",
      "epoch 118| loss: 0.28644 | val_0_accuracy: 0.89421 |  0:00:19s\n",
      " Epoch 119 took 0.16 seconds\n",
      "epoch 119| loss: 0.27801 | val_0_accuracy: 0.8987  |  0:00:20s\n",
      " Epoch 120 took 0.17 seconds\n",
      "epoch 120| loss: 0.28204 | val_0_accuracy: 0.92365 |  0:00:20s\n",
      " Epoch 121 took 0.17 seconds\n",
      "epoch 121| loss: 0.27759 | val_0_accuracy: 0.91966 |  0:00:20s\n",
      " Epoch 122 took 0.17 seconds\n",
      "epoch 122| loss: 0.27481 | val_0_accuracy: 0.89022 |  0:00:20s\n",
      " Epoch 123 took 0.16 seconds\n",
      "epoch 123| loss: 0.28775 | val_0_accuracy: 0.90719 |  0:00:20s\n",
      " Epoch 124 took 0.17 seconds\n",
      "epoch 124| loss: 0.28443 | val_0_accuracy: 0.90369 |  0:00:20s\n",
      " Epoch 125 took 0.17 seconds\n",
      "epoch 125| loss: 0.26661 | val_0_accuracy: 0.90669 |  0:00:21s\n",
      " Epoch 126 took 0.16 seconds\n",
      "epoch 126| loss: 0.27201 | val_0_accuracy: 0.87924 |  0:00:21s\n",
      " Epoch 127 took 0.16 seconds\n",
      "epoch 127| loss: 0.30844 | val_0_accuracy: 0.89122 |  0:00:21s\n",
      " Epoch 128 took 0.16 seconds\n",
      "epoch 128| loss: 0.28308 | val_0_accuracy: 0.9022  |  0:00:21s\n",
      " Epoch 129 took 0.17 seconds\n",
      "epoch 129| loss: 0.27241 | val_0_accuracy: 0.9007  |  0:00:21s\n",
      " Epoch 130 took 0.17 seconds\n",
      "epoch 130| loss: 0.33211 | val_0_accuracy: 0.89621 |  0:00:21s\n",
      " Epoch 131 took 0.16 seconds\n",
      "epoch 131| loss: 0.29271 | val_0_accuracy: 0.91517 |  0:00:22s\n",
      " Epoch 132 took 0.16 seconds\n",
      "epoch 132| loss: 0.27683 | val_0_accuracy: 0.89321 |  0:00:22s\n",
      " Epoch 133 took 0.16 seconds\n",
      "epoch 133| loss: 0.27015 | val_0_accuracy: 0.91168 |  0:00:22s\n",
      " Epoch 134 took 0.16 seconds\n",
      "epoch 134| loss: 0.28866 | val_0_accuracy: 0.89421 |  0:00:22s\n",
      " Epoch 135 took 0.17 seconds\n",
      "epoch 135| loss: 0.28698 | val_0_accuracy: 0.88623 |  0:00:22s\n",
      " Epoch 136 took 0.17 seconds\n",
      "epoch 136| loss: 0.29643 | val_0_accuracy: 0.86926 |  0:00:22s\n",
      " Epoch 137 took 0.16 seconds\n",
      "epoch 137| loss: 0.27383 | val_0_accuracy: 0.90818 |  0:00:23s\n",
      " Epoch 138 took 0.17 seconds\n",
      "epoch 138| loss: 0.27096 | val_0_accuracy: 0.90719 |  0:00:23s\n",
      " Epoch 139 took 0.17 seconds\n",
      "epoch 139| loss: 0.2874  | val_0_accuracy: 0.91367 |  0:00:23s\n",
      " Epoch 140 took 0.16 seconds\n",
      "epoch 140| loss: 0.25701 | val_0_accuracy: 0.91018 |  0:00:23s\n",
      " Epoch 141 took 0.16 seconds\n",
      "epoch 141| loss: 0.27071 | val_0_accuracy: 0.90669 |  0:00:23s\n",
      " Epoch 142 took 0.16 seconds\n",
      "epoch 142| loss: 0.26976 | val_0_accuracy: 0.91317 |  0:00:23s\n",
      " Epoch 143 took 0.16 seconds\n",
      "epoch 143| loss: 0.29299 | val_0_accuracy: 0.91168 |  0:00:24s\n",
      " Epoch 144 took 0.16 seconds\n",
      "epoch 144| loss: 0.26817 | val_0_accuracy: 0.90968 |  0:00:24s\n",
      " Epoch 145 took 0.16 seconds\n",
      "epoch 145| loss: 0.26558 | val_0_accuracy: 0.92066 |  0:00:24s\n",
      " Epoch 146 took 0.16 seconds\n",
      "epoch 146| loss: 0.2585  | val_0_accuracy: 0.87525 |  0:00:24s\n",
      " Epoch 147 took 0.17 seconds\n",
      "epoch 147| loss: 0.25347 | val_0_accuracy: 0.88074 |  0:00:24s\n",
      " Epoch 148 took 0.16 seconds\n",
      "epoch 148| loss: 0.23849 | val_0_accuracy: 0.91317 |  0:00:24s\n",
      " Epoch 149 took 0.17 seconds\n",
      "epoch 149| loss: 0.27617 | val_0_accuracy: 0.91517 |  0:00:25s\n",
      " Epoch 150 took 0.16 seconds\n",
      "epoch 150| loss: 0.26326 | val_0_accuracy: 0.91118 |  0:00:25s\n",
      " Epoch 151 took 0.16 seconds\n",
      "epoch 151| loss: 0.26689 | val_0_accuracy: 0.8982  |  0:00:25s\n",
      " Epoch 152 took 0.16 seconds\n",
      "epoch 152| loss: 0.25247 | val_0_accuracy: 0.90319 |  0:00:25s\n",
      " Epoch 153 took 0.16 seconds\n",
      "epoch 153| loss: 0.26543 | val_0_accuracy: 0.8992  |  0:00:25s\n",
      " Epoch 154 took 0.17 seconds\n",
      "epoch 154| loss: 0.26977 | val_0_accuracy: 0.91218 |  0:00:25s\n",
      " Epoch 155 took 0.16 seconds\n",
      "epoch 155| loss: 0.26631 | val_0_accuracy: 0.92265 |  0:00:26s\n",
      " Epoch 156 took 0.16 seconds\n",
      "epoch 156| loss: 0.25672 | val_0_accuracy: 0.90818 |  0:00:26s\n",
      " Epoch 157 took 0.17 seconds\n",
      "epoch 157| loss: 0.27973 | val_0_accuracy: 0.92066 |  0:00:26s\n",
      " Epoch 158 took 0.16 seconds\n",
      "epoch 158| loss: 0.25414 | val_0_accuracy: 0.9017  |  0:00:26s\n",
      " Epoch 159 took 0.16 seconds\n",
      "epoch 159| loss: 0.27658 | val_0_accuracy: 0.90469 |  0:00:26s\n",
      " Epoch 160 took 0.17 seconds\n",
      "epoch 160| loss: 0.25769 | val_0_accuracy: 0.90918 |  0:00:26s\n",
      " Epoch 161 took 0.15 seconds\n",
      "epoch 161| loss: 0.26756 | val_0_accuracy: 0.91966 |  0:00:26s\n",
      " Epoch 162 took 0.17 seconds\n",
      "epoch 162| loss: 0.25675 | val_0_accuracy: 0.8987  |  0:00:27s\n",
      " Epoch 163 took 0.17 seconds\n",
      "epoch 163| loss: 0.25291 | val_0_accuracy: 0.92415 |  0:00:27s\n",
      " Epoch 164 took 0.17 seconds\n",
      "epoch 164| loss: 0.23815 | val_0_accuracy: 0.90868 |  0:00:27s\n",
      " Epoch 165 took 0.17 seconds\n",
      "epoch 165| loss: 0.24982 | val_0_accuracy: 0.92116 |  0:00:27s\n",
      " Epoch 166 took 0.16 seconds\n",
      "epoch 166| loss: 0.23268 | val_0_accuracy: 0.92665 |  0:00:27s\n",
      " Epoch 167 took 0.16 seconds\n",
      "epoch 167| loss: 0.23739 | val_0_accuracy: 0.9012  |  0:00:27s\n",
      " Epoch 168 took 0.17 seconds\n",
      "epoch 168| loss: 0.23533 | val_0_accuracy: 0.90768 |  0:00:28s\n",
      " Epoch 169 took 0.16 seconds\n",
      "epoch 169| loss: 0.26512 | val_0_accuracy: 0.91317 |  0:00:28s\n",
      " Epoch 170 took 0.16 seconds\n",
      "epoch 170| loss: 0.2522  | val_0_accuracy: 0.9017  |  0:00:28s\n",
      " Epoch 171 took 0.17 seconds\n",
      "epoch 171| loss: 0.25724 | val_0_accuracy: 0.90719 |  0:00:28s\n",
      " Epoch 172 took 0.30 seconds\n",
      "epoch 172| loss: 0.24784 | val_0_accuracy: 0.88972 |  0:00:28s\n",
      " Epoch 173 took 0.16 seconds\n",
      "epoch 173| loss: 0.23939 | val_0_accuracy: 0.90918 |  0:00:29s\n",
      " Epoch 174 took 0.16 seconds\n",
      "epoch 174| loss: 0.25176 | val_0_accuracy: 0.90669 |  0:00:29s\n",
      " Epoch 175 took 0.16 seconds\n",
      "epoch 175| loss: 0.23548 | val_0_accuracy: 0.90619 |  0:00:29s\n",
      " Epoch 176 took 0.17 seconds\n",
      "epoch 176| loss: 0.25046 | val_0_accuracy: 0.91667 |  0:00:29s\n",
      " Epoch 177 took 0.16 seconds\n",
      "epoch 177| loss: 0.23685 | val_0_accuracy: 0.9017  |  0:00:29s\n",
      " Epoch 178 took 0.16 seconds\n",
      "epoch 178| loss: 0.2325  | val_0_accuracy: 0.90918 |  0:00:29s\n",
      " Epoch 179 took 0.16 seconds\n",
      "epoch 179| loss: 0.2513  | val_0_accuracy: 0.90719 |  0:00:30s\n",
      " Epoch 180 took 0.17 seconds\n",
      "epoch 180| loss: 0.26587 | val_0_accuracy: 0.88373 |  0:00:30s\n",
      " Epoch 181 took 0.16 seconds\n",
      "epoch 181| loss: 0.24153 | val_0_accuracy: 0.87475 |  0:00:30s\n",
      " Epoch 182 took 0.16 seconds\n",
      "epoch 182| loss: 0.24024 | val_0_accuracy: 0.90569 |  0:00:30s\n",
      " Epoch 183 took 0.17 seconds\n",
      "epoch 183| loss: 0.2229  | val_0_accuracy: 0.92066 |  0:00:30s\n",
      " Epoch 184 took 0.16 seconds\n",
      "epoch 184| loss: 0.24011 | val_0_accuracy: 0.90519 |  0:00:30s\n",
      " Epoch 185 took 0.16 seconds\n",
      "epoch 185| loss: 0.2402  | val_0_accuracy: 0.9017  |  0:00:31s\n",
      " Epoch 186 took 0.16 seconds\n",
      "epoch 186| loss: 0.22799 | val_0_accuracy: 0.92515 |  0:00:31s\n",
      " Epoch 187 took 0.16 seconds\n",
      "epoch 187| loss: 0.23108 | val_0_accuracy: 0.91267 |  0:00:31s\n",
      " Epoch 188 took 0.17 seconds\n",
      "epoch 188| loss: 0.24723 | val_0_accuracy: 0.89621 |  0:00:31s\n",
      " Epoch 189 took 0.17 seconds\n",
      "epoch 189| loss: 0.22764 | val_0_accuracy: 0.91916 |  0:00:31s\n",
      " Epoch 190 took 0.16 seconds\n",
      "epoch 190| loss: 0.22976 | val_0_accuracy: 0.9002  |  0:00:31s\n",
      " Epoch 191 took 0.16 seconds\n",
      "epoch 191| loss: 0.23667 | val_0_accuracy: 0.89521 |  0:00:32s\n",
      " Epoch 192 took 0.16 seconds\n",
      "epoch 192| loss: 0.21203 | val_0_accuracy: 0.91068 |  0:00:32s\n",
      " Epoch 193 took 0.16 seconds\n",
      "epoch 193| loss: 0.24829 | val_0_accuracy: 0.88024 |  0:00:32s\n",
      " Epoch 194 took 0.16 seconds\n",
      "epoch 194| loss: 0.23639 | val_0_accuracy: 0.92315 |  0:00:32s\n",
      " Epoch 195 took 0.17 seconds\n",
      "epoch 195| loss: 0.24042 | val_0_accuracy: 0.89172 |  0:00:32s\n",
      " Epoch 196 took 0.16 seconds\n",
      "epoch 196| loss: 0.24029 | val_0_accuracy: 0.91567 |  0:00:32s\n",
      " Epoch 197 took 0.17 seconds\n",
      "epoch 197| loss: 0.22675 | val_0_accuracy: 0.91317 |  0:00:33s\n",
      " Epoch 198 took 0.17 seconds\n",
      "epoch 198| loss: 0.22237 | val_0_accuracy: 0.92515 |  0:00:33s\n",
      " Epoch 199 took 0.16 seconds\n",
      "epoch 199| loss: 0.24196 | val_0_accuracy: 0.9002  |  0:00:33s\n",
      " Epoch 200 took 0.17 seconds\n",
      "epoch 200| loss: 0.25925 | val_0_accuracy: 0.89321 |  0:00:33s\n",
      " Epoch 201 took 0.16 seconds\n",
      "epoch 201| loss: 0.24512 | val_0_accuracy: 0.91717 |  0:00:33s\n",
      " Epoch 202 took 0.17 seconds\n",
      "epoch 202| loss: 0.2428  | val_0_accuracy: 0.9022  |  0:00:33s\n",
      " Epoch 203 took 0.16 seconds\n",
      "epoch 203| loss: 0.21846 | val_0_accuracy: 0.91567 |  0:00:34s\n",
      " Epoch 204 took 0.16 seconds\n",
      "epoch 204| loss: 0.22323 | val_0_accuracy: 0.91168 |  0:00:34s\n",
      " Epoch 205 took 0.17 seconds\n",
      "epoch 205| loss: 0.23523 | val_0_accuracy: 0.8997  |  0:00:34s\n",
      " Epoch 206 took 0.16 seconds\n",
      "epoch 206| loss: 0.25957 | val_0_accuracy: 0.88972 |  0:00:34s\n",
      " Epoch 207 took 0.16 seconds\n",
      "epoch 207| loss: 0.21939 | val_0_accuracy: 0.90719 |  0:00:34s\n",
      " Epoch 208 took 0.16 seconds\n",
      "epoch 208| loss: 0.2299  | val_0_accuracy: 0.90669 |  0:00:34s\n",
      " Epoch 209 took 0.16 seconds\n",
      "epoch 209| loss: 0.23165 | val_0_accuracy: 0.92914 |  0:00:34s\n",
      " Epoch 210 took 0.17 seconds\n",
      "epoch 210| loss: 0.21565 | val_0_accuracy: 0.92016 |  0:00:35s\n",
      " Epoch 211 took 0.17 seconds\n",
      "epoch 211| loss: 0.22307 | val_0_accuracy: 0.89122 |  0:00:35s\n",
      " Epoch 212 took 0.16 seconds\n",
      "epoch 212| loss: 0.23454 | val_0_accuracy: 0.8987  |  0:00:35s\n",
      " Epoch 213 took 0.17 seconds\n",
      "epoch 213| loss: 0.21958 | val_0_accuracy: 0.9002  |  0:00:35s\n",
      " Epoch 214 took 0.16 seconds\n",
      "epoch 214| loss: 0.21383 | val_0_accuracy: 0.92665 |  0:00:35s\n",
      " Epoch 215 took 0.16 seconds\n",
      "epoch 215| loss: 0.22265 | val_0_accuracy: 0.92166 |  0:00:35s\n",
      " Epoch 216 took 0.16 seconds\n",
      "epoch 216| loss: 0.20881 | val_0_accuracy: 0.9017  |  0:00:36s\n",
      " Epoch 217 took 0.16 seconds\n",
      "epoch 217| loss: 0.21858 | val_0_accuracy: 0.91866 |  0:00:36s\n",
      " Epoch 218 took 0.16 seconds\n",
      "epoch 218| loss: 0.22375 | val_0_accuracy: 0.91866 |  0:00:36s\n",
      " Epoch 219 took 0.16 seconds\n",
      "epoch 219| loss: 0.2324  | val_0_accuracy: 0.93114 |  0:00:36s\n",
      " Epoch 220 took 0.16 seconds\n",
      "epoch 220| loss: 0.22335 | val_0_accuracy: 0.90719 |  0:00:36s\n",
      " Epoch 221 took 0.16 seconds\n",
      "epoch 221| loss: 0.24331 | val_0_accuracy: 0.91267 |  0:00:36s\n",
      " Epoch 222 took 0.16 seconds\n",
      "epoch 222| loss: 0.22846 | val_0_accuracy: 0.91567 |  0:00:37s\n",
      " Epoch 223 took 0.16 seconds\n",
      "epoch 223| loss: 0.21758 | val_0_accuracy: 0.90619 |  0:00:37s\n",
      " Epoch 224 took 0.16 seconds\n",
      "epoch 224| loss: 0.22415 | val_0_accuracy: 0.91218 |  0:00:37s\n",
      " Epoch 225 took 0.16 seconds\n",
      "epoch 225| loss: 0.21489 | val_0_accuracy: 0.9012  |  0:00:37s\n",
      " Epoch 226 took 0.17 seconds\n",
      "epoch 226| loss: 0.22728 | val_0_accuracy: 0.87974 |  0:00:37s\n",
      " Epoch 227 took 0.16 seconds\n",
      "epoch 227| loss: 0.22171 | val_0_accuracy: 0.89172 |  0:00:37s\n",
      " Epoch 228 took 0.16 seconds\n",
      "epoch 228| loss: 0.23998 | val_0_accuracy: 0.91068 |  0:00:38s\n",
      " Epoch 229 took 0.16 seconds\n",
      "epoch 229| loss: 0.22654 | val_0_accuracy: 0.92715 |  0:00:38s\n",
      " Epoch 230 took 0.16 seconds\n",
      "epoch 230| loss: 0.22998 | val_0_accuracy: 0.91617 |  0:00:38s\n",
      " Epoch 231 took 0.17 seconds\n",
      "epoch 231| loss: 0.23151 | val_0_accuracy: 0.91317 |  0:00:38s\n",
      " Epoch 232 took 0.17 seconds\n",
      "epoch 232| loss: 0.20822 | val_0_accuracy: 0.90569 |  0:00:38s\n",
      " Epoch 233 took 0.17 seconds\n",
      "epoch 233| loss: 0.21296 | val_0_accuracy: 0.92116 |  0:00:38s\n",
      " Epoch 234 took 0.16 seconds\n",
      "epoch 234| loss: 0.21938 | val_0_accuracy: 0.92415 |  0:00:39s\n",
      " Epoch 235 took 0.16 seconds\n",
      "epoch 235| loss: 0.20784 | val_0_accuracy: 0.9007  |  0:00:39s\n",
      " Epoch 236 took 0.16 seconds\n",
      "epoch 236| loss: 0.21002 | val_0_accuracy: 0.8997  |  0:00:39s\n",
      " Epoch 237 took 0.16 seconds\n",
      "epoch 237| loss: 0.22064 | val_0_accuracy: 0.90419 |  0:00:39s\n",
      " Epoch 238 took 0.16 seconds\n",
      "epoch 238| loss: 0.23083 | val_0_accuracy: 0.92116 |  0:00:39s\n",
      " Epoch 239 took 0.16 seconds\n",
      "epoch 239| loss: 0.21599 | val_0_accuracy: 0.90269 |  0:00:39s\n",
      " Epoch 240 took 0.17 seconds\n",
      "epoch 240| loss: 0.21477 | val_0_accuracy: 0.91766 |  0:00:40s\n",
      " Epoch 241 took 0.18 seconds\n",
      "epoch 241| loss: 0.21908 | val_0_accuracy: 0.91517 |  0:00:40s\n",
      " Epoch 242 took 0.18 seconds\n",
      "epoch 242| loss: 0.2177  | val_0_accuracy: 0.89721 |  0:00:40s\n",
      " Epoch 243 took 0.17 seconds\n",
      "epoch 243| loss: 0.20981 | val_0_accuracy: 0.88822 |  0:00:40s\n",
      " Epoch 244 took 0.18 seconds\n",
      "epoch 244| loss: 0.22168 | val_0_accuracy: 0.90569 |  0:00:40s\n",
      " Epoch 245 took 0.16 seconds\n",
      "epoch 245| loss: 0.21529 | val_0_accuracy: 0.91168 |  0:00:40s\n",
      " Epoch 246 took 0.16 seconds\n",
      "epoch 246| loss: 0.21489 | val_0_accuracy: 0.92715 |  0:00:41s\n",
      " Epoch 247 took 0.16 seconds\n",
      "epoch 247| loss: 0.21706 | val_0_accuracy: 0.92964 |  0:00:41s\n",
      " Epoch 248 took 0.16 seconds\n",
      "epoch 248| loss: 0.21626 | val_0_accuracy: 0.91916 |  0:00:41s\n",
      " Epoch 249 took 0.16 seconds\n",
      "epoch 249| loss: 0.20701 | val_0_accuracy: 0.91517 |  0:00:41s\n",
      " Epoch 250 took 0.17 seconds\n",
      "epoch 250| loss: 0.19794 | val_0_accuracy: 0.91068 |  0:00:41s\n",
      " Epoch 251 took 0.29 seconds\n",
      "epoch 251| loss: 0.19888 | val_0_accuracy: 0.91018 |  0:00:42s\n",
      " Epoch 252 took 0.19 seconds\n",
      "epoch 252| loss: 0.2157  | val_0_accuracy: 0.90419 |  0:00:42s\n",
      " Epoch 253 took 0.16 seconds\n",
      "epoch 253| loss: 0.21097 | val_0_accuracy: 0.9012  |  0:00:42s\n",
      " Epoch 254 took 0.17 seconds\n",
      "epoch 254| loss: 0.21434 | val_0_accuracy: 0.91667 |  0:00:42s\n",
      " Epoch 255 took 0.16 seconds\n",
      "epoch 255| loss: 0.22243 | val_0_accuracy: 0.90968 |  0:00:42s\n",
      " Epoch 256 took 0.16 seconds\n",
      "epoch 256| loss: 0.22288 | val_0_accuracy: 0.90369 |  0:00:42s\n",
      " Epoch 257 took 0.17 seconds\n",
      "epoch 257| loss: 0.21575 | val_0_accuracy: 0.89721 |  0:00:43s\n",
      " Epoch 258 took 0.16 seconds\n",
      "epoch 258| loss: 0.21484 | val_0_accuracy: 0.90968 |  0:00:43s\n",
      " Epoch 259 took 0.17 seconds\n",
      "epoch 259| loss: 0.20706 | val_0_accuracy: 0.91068 |  0:00:43s\n",
      " Epoch 260 took 0.17 seconds\n",
      "epoch 260| loss: 0.19764 | val_0_accuracy: 0.90619 |  0:00:43s\n",
      " Epoch 261 took 0.16 seconds\n",
      "epoch 261| loss: 0.20306 | val_0_accuracy: 0.92066 |  0:00:43s\n",
      " Epoch 262 took 0.17 seconds\n",
      "epoch 262| loss: 0.2058  | val_0_accuracy: 0.92116 |  0:00:43s\n",
      " Epoch 263 took 0.16 seconds\n",
      "epoch 263| loss: 0.2097  | val_0_accuracy: 0.91068 |  0:00:44s\n",
      " Epoch 264 took 0.16 seconds\n",
      "epoch 264| loss: 0.1911  | val_0_accuracy: 0.92265 |  0:00:44s\n",
      " Epoch 265 took 0.19 seconds\n",
      "epoch 265| loss: 0.19972 | val_0_accuracy: 0.9017  |  0:00:44s\n",
      " Epoch 266 took 0.18 seconds\n",
      "epoch 266| loss: 0.19755 | val_0_accuracy: 0.90768 |  0:00:44s\n",
      " Epoch 267 took 0.18 seconds\n",
      "epoch 267| loss: 0.20027 | val_0_accuracy: 0.91667 |  0:00:44s\n",
      " Epoch 268 took 0.20 seconds\n",
      "epoch 268| loss: 0.2009  | val_0_accuracy: 0.90369 |  0:00:44s\n",
      " Epoch 269 took 0.18 seconds\n",
      "epoch 269| loss: 0.2097  | val_0_accuracy: 0.92066 |  0:00:45s\n",
      " Epoch 270 took 0.17 seconds\n",
      "epoch 270| loss: 0.20749 | val_0_accuracy: 0.90818 |  0:00:45s\n",
      " Epoch 271 took 0.18 seconds\n",
      "epoch 271| loss: 0.20721 | val_0_accuracy: 0.92665 |  0:00:45s\n",
      " Epoch 272 took 0.17 seconds\n",
      "epoch 272| loss: 0.21781 | val_0_accuracy: 0.90469 |  0:00:45s\n",
      " Epoch 273 took 0.16 seconds\n",
      "epoch 273| loss: 0.21925 | val_0_accuracy: 0.90419 |  0:00:45s\n",
      " Epoch 274 took 0.17 seconds\n",
      "epoch 274| loss: 0.21714 | val_0_accuracy: 0.8992  |  0:00:45s\n",
      " Epoch 275 took 0.16 seconds\n",
      "epoch 275| loss: 0.20452 | val_0_accuracy: 0.91816 |  0:00:46s\n",
      " Epoch 276 took 0.16 seconds\n",
      "epoch 276| loss: 0.21228 | val_0_accuracy: 0.92515 |  0:00:46s\n",
      " Epoch 277 took 0.16 seconds\n",
      "epoch 277| loss: 0.20395 | val_0_accuracy: 0.91317 |  0:00:46s\n",
      " Epoch 278 took 0.16 seconds\n",
      "epoch 278| loss: 0.21244 | val_0_accuracy: 0.92715 |  0:00:46s\n",
      " Epoch 279 took 0.16 seconds\n",
      "epoch 279| loss: 0.19619 | val_0_accuracy: 0.92116 |  0:00:46s\n",
      " Epoch 280 took 0.17 seconds\n",
      "epoch 280| loss: 0.195   | val_0_accuracy: 0.91168 |  0:00:46s\n",
      " Epoch 281 took 0.17 seconds\n",
      "epoch 281| loss: 0.20587 | val_0_accuracy: 0.92415 |  0:00:47s\n",
      " Epoch 282 took 0.16 seconds\n",
      "epoch 282| loss: 0.20096 | val_0_accuracy: 0.92715 |  0:00:47s\n",
      " Epoch 283 took 0.17 seconds\n",
      "epoch 283| loss: 0.1945  | val_0_accuracy: 0.92315 |  0:00:47s\n",
      " Epoch 284 took 0.16 seconds\n",
      "epoch 284| loss: 0.18977 | val_0_accuracy: 0.90369 |  0:00:47s\n",
      " Epoch 285 took 0.16 seconds\n",
      "epoch 285| loss: 0.1994  | val_0_accuracy: 0.8982  |  0:00:47s\n",
      " Epoch 286 took 0.17 seconds\n",
      "epoch 286| loss: 0.20966 | val_0_accuracy: 0.92415 |  0:00:47s\n",
      " Epoch 287 took 0.16 seconds\n",
      "epoch 287| loss: 0.1959  | val_0_accuracy: 0.92665 |  0:00:48s\n",
      " Epoch 288 took 0.16 seconds\n",
      "epoch 288| loss: 0.19611 | val_0_accuracy: 0.92715 |  0:00:48s\n",
      " Epoch 289 took 0.16 seconds\n",
      "epoch 289| loss: 0.2015  | val_0_accuracy: 0.92365 |  0:00:48s\n",
      " Epoch 290 took 0.17 seconds\n",
      "epoch 290| loss: 0.21378 | val_0_accuracy: 0.89321 |  0:00:48s\n",
      " Epoch 291 took 0.17 seconds\n",
      "epoch 291| loss: 0.19908 | val_0_accuracy: 0.90419 |  0:00:48s\n",
      " Epoch 292 took 0.16 seconds\n",
      "epoch 292| loss: 0.20125 | val_0_accuracy: 0.8997  |  0:00:48s\n",
      " Epoch 293 took 0.16 seconds\n",
      "epoch 293| loss: 0.21039 | val_0_accuracy: 0.92415 |  0:00:49s\n",
      " Epoch 294 took 0.16 seconds\n",
      "epoch 294| loss: 0.20473 | val_0_accuracy: 0.90669 |  0:00:49s\n",
      " Epoch 295 took 0.16 seconds\n",
      "epoch 295| loss: 0.19898 | val_0_accuracy: 0.92764 |  0:00:49s\n",
      " Epoch 296 took 0.17 seconds\n",
      "epoch 296| loss: 0.1848  | val_0_accuracy: 0.92914 |  0:00:49s\n",
      " Epoch 297 took 0.17 seconds\n",
      "epoch 297| loss: 0.20099 | val_0_accuracy: 0.91367 |  0:00:49s\n",
      " Epoch 298 took 0.16 seconds\n",
      "epoch 298| loss: 0.19006 | val_0_accuracy: 0.89222 |  0:00:49s\n",
      " Epoch 299 took 0.16 seconds\n",
      "epoch 299| loss: 0.18355 | val_0_accuracy: 0.91617 |  0:00:50s\n",
      " Epoch 300 took 0.16 seconds\n",
      "epoch 300| loss: 0.21213 | val_0_accuracy: 0.92814 |  0:00:50s\n",
      " Epoch 301 took 0.16 seconds\n",
      "epoch 301| loss: 0.21521 | val_0_accuracy: 0.92116 |  0:00:50s\n",
      " Epoch 302 took 0.17 seconds\n",
      "epoch 302| loss: 0.20723 | val_0_accuracy: 0.91218 |  0:00:50s\n",
      " Epoch 303 took 0.16 seconds\n",
      "epoch 303| loss: 0.19514 | val_0_accuracy: 0.90719 |  0:00:50s\n",
      " Epoch 304 took 0.17 seconds\n",
      "epoch 304| loss: 0.18973 | val_0_accuracy: 0.92166 |  0:00:50s\n",
      " Epoch 305 took 0.17 seconds\n",
      "epoch 305| loss: 0.1908  | val_0_accuracy: 0.92066 |  0:00:51s\n",
      " Epoch 306 took 0.16 seconds\n",
      "epoch 306| loss: 0.18665 | val_0_accuracy: 0.93363 |  0:00:51s\n",
      " Epoch 307 took 0.17 seconds\n",
      "epoch 307| loss: 0.19068 | val_0_accuracy: 0.91517 |  0:00:51s\n",
      " Epoch 308 took 0.16 seconds\n",
      "epoch 308| loss: 0.19073 | val_0_accuracy: 0.91517 |  0:00:51s\n",
      " Epoch 309 took 0.16 seconds\n",
      "epoch 309| loss: 0.19378 | val_0_accuracy: 0.92365 |  0:00:51s\n",
      " Epoch 310 took 0.16 seconds\n",
      "epoch 310| loss: 0.18243 | val_0_accuracy: 0.9002  |  0:00:51s\n",
      " Epoch 311 took 0.16 seconds\n",
      "epoch 311| loss: 0.20762 | val_0_accuracy: 0.92465 |  0:00:51s\n",
      " Epoch 312 took 0.17 seconds\n",
      "epoch 312| loss: 0.19108 | val_0_accuracy: 0.93014 |  0:00:52s\n",
      " Epoch 313 took 0.16 seconds\n",
      "epoch 313| loss: 0.19723 | val_0_accuracy: 0.91766 |  0:00:52s\n",
      " Epoch 314 took 0.16 seconds\n",
      "epoch 314| loss: 0.19194 | val_0_accuracy: 0.9007  |  0:00:52s\n",
      " Epoch 315 took 0.17 seconds\n",
      "epoch 315| loss: 0.19267 | val_0_accuracy: 0.91916 |  0:00:52s\n",
      " Epoch 316 took 0.16 seconds\n",
      "epoch 316| loss: 0.19427 | val_0_accuracy: 0.90669 |  0:00:52s\n",
      " Epoch 317 took 0.16 seconds\n",
      "epoch 317| loss: 0.18595 | val_0_accuracy: 0.90619 |  0:00:52s\n",
      " Epoch 318 took 0.16 seconds\n",
      "epoch 318| loss: 0.18775 | val_0_accuracy: 0.89222 |  0:00:53s\n",
      " Epoch 319 took 0.16 seconds\n",
      "epoch 319| loss: 0.20023 | val_0_accuracy: 0.92116 |  0:00:53s\n",
      " Epoch 320 took 0.16 seconds\n",
      "epoch 320| loss: 0.19358 | val_0_accuracy: 0.93313 |  0:00:53s\n",
      " Epoch 321 took 0.17 seconds\n",
      "epoch 321| loss: 0.20506 | val_0_accuracy: 0.90369 |  0:00:53s\n",
      " Epoch 322 took 0.16 seconds\n",
      "epoch 322| loss: 0.20225 | val_0_accuracy: 0.91866 |  0:00:53s\n",
      " Epoch 323 took 0.16 seconds\n",
      "epoch 323| loss: 0.18242 | val_0_accuracy: 0.93114 |  0:00:53s\n",
      " Epoch 324 took 0.16 seconds\n",
      "epoch 324| loss: 0.17618 | val_0_accuracy: 0.92764 |  0:00:54s\n",
      " Epoch 325 took 0.17 seconds\n",
      "epoch 325| loss: 0.19322 | val_0_accuracy: 0.89471 |  0:00:54s\n",
      " Epoch 326 took 0.16 seconds\n",
      "epoch 326| loss: 0.18906 | val_0_accuracy: 0.93214 |  0:00:54s\n",
      " Epoch 327 took 0.16 seconds\n",
      "epoch 327| loss: 0.19409 | val_0_accuracy: 0.93014 |  0:00:54s\n",
      " Epoch 328 took 0.16 seconds\n",
      "epoch 328| loss: 0.18886 | val_0_accuracy: 0.93313 |  0:00:54s\n",
      " Epoch 329 took 0.18 seconds\n",
      "epoch 329| loss: 0.19602 | val_0_accuracy: 0.91816 |  0:00:54s\n",
      " Epoch 330 took 0.17 seconds\n",
      "epoch 330| loss: 0.1824  | val_0_accuracy: 0.90319 |  0:00:55s\n",
      " Epoch 331 took 0.17 seconds\n",
      "epoch 331| loss: 0.18507 | val_0_accuracy: 0.92116 |  0:00:55s\n",
      " Epoch 332 took 0.17 seconds\n",
      "epoch 332| loss: 0.18673 | val_0_accuracy: 0.91218 |  0:00:55s\n",
      " Epoch 333 took 0.16 seconds\n",
      "epoch 333| loss: 0.20391 | val_0_accuracy: 0.92764 |  0:00:55s\n",
      " Epoch 334 took 0.30 seconds\n",
      "epoch 334| loss: 0.20672 | val_0_accuracy: 0.93263 |  0:00:55s\n",
      " Epoch 335 took 0.17 seconds\n",
      "epoch 335| loss: 0.24785 | val_0_accuracy: 0.90768 |  0:00:56s\n",
      " Epoch 336 took 0.17 seconds\n",
      "epoch 336| loss: 0.19847 | val_0_accuracy: 0.89471 |  0:00:56s\n",
      " Epoch 337 took 0.16 seconds\n",
      "epoch 337| loss: 0.19642 | val_0_accuracy: 0.91417 |  0:00:56s\n",
      " Epoch 338 took 0.16 seconds\n",
      "epoch 338| loss: 0.18568 | val_0_accuracy: 0.89721 |  0:00:56s\n",
      " Epoch 339 took 0.17 seconds\n",
      "epoch 339| loss: 0.17945 | val_0_accuracy: 0.88872 |  0:00:56s\n",
      " Epoch 340 took 0.16 seconds\n",
      "epoch 340| loss: 0.17934 | val_0_accuracy: 0.90469 |  0:00:56s\n",
      " Epoch 341 took 0.17 seconds\n",
      "epoch 341| loss: 0.19463 | val_0_accuracy: 0.92864 |  0:00:57s\n",
      " Epoch 342 took 0.16 seconds\n",
      "epoch 342| loss: 0.17663 | val_0_accuracy: 0.8992  |  0:00:57s\n",
      " Epoch 343 took 0.17 seconds\n",
      "epoch 343| loss: 0.17259 | val_0_accuracy: 0.9017  |  0:00:57s\n",
      " Epoch 344 took 0.17 seconds\n",
      "epoch 344| loss: 0.17774 | val_0_accuracy: 0.91417 |  0:00:57s\n",
      " Epoch 345 took 0.16 seconds\n",
      "epoch 345| loss: 0.1777  | val_0_accuracy: 0.90818 |  0:00:57s\n",
      " Epoch 346 took 0.16 seconds\n",
      "epoch 346| loss: 0.1708  | val_0_accuracy: 0.91667 |  0:00:57s\n",
      " Epoch 347 took 0.16 seconds\n",
      "epoch 347| loss: 0.18383 | val_0_accuracy: 0.91916 |  0:00:58s\n",
      " Epoch 348 took 0.16 seconds\n",
      "epoch 348| loss: 0.18869 | val_0_accuracy: 0.92216 |  0:00:58s\n",
      " Epoch 349 took 0.16 seconds\n",
      "epoch 349| loss: 0.18712 | val_0_accuracy: 0.91667 |  0:00:58s\n",
      " Epoch 350 took 0.16 seconds\n",
      "epoch 350| loss: 0.19245 | val_0_accuracy: 0.90269 |  0:00:58s\n",
      " Epoch 351 took 0.16 seconds\n",
      "epoch 351| loss: 0.17954 | val_0_accuracy: 0.92465 |  0:00:58s\n",
      " Epoch 352 took 0.17 seconds\n",
      "epoch 352| loss: 0.18836 | val_0_accuracy: 0.92166 |  0:00:58s\n",
      " Epoch 353 took 0.16 seconds\n",
      "epoch 353| loss: 0.177   | val_0_accuracy: 0.90868 |  0:00:58s\n",
      " Epoch 354 took 0.16 seconds\n",
      "epoch 354| loss: 0.19546 | val_0_accuracy: 0.91667 |  0:00:59s\n",
      " Epoch 355 took 0.16 seconds\n",
      "epoch 355| loss: 0.1775  | val_0_accuracy: 0.93613 |  0:00:59s\n",
      " Epoch 356 took 0.17 seconds\n",
      "epoch 356| loss: 0.17114 | val_0_accuracy: 0.91168 |  0:00:59s\n",
      " Epoch 357 took 0.16 seconds\n",
      "epoch 357| loss: 0.1835  | val_0_accuracy: 0.91267 |  0:00:59s\n",
      " Epoch 358 took 0.16 seconds\n",
      "epoch 358| loss: 0.19794 | val_0_accuracy: 0.90369 |  0:00:59s\n",
      " Epoch 359 took 0.17 seconds\n",
      "epoch 359| loss: 0.19294 | val_0_accuracy: 0.92565 |  0:00:59s\n",
      " Epoch 360 took 0.17 seconds\n",
      "epoch 360| loss: 0.19506 | val_0_accuracy: 0.92864 |  0:01:00s\n",
      " Epoch 361 took 0.16 seconds\n",
      "epoch 361| loss: 0.17989 | val_0_accuracy: 0.92415 |  0:01:00s\n",
      " Epoch 362 took 0.17 seconds\n",
      "epoch 362| loss: 0.17173 | val_0_accuracy: 0.91816 |  0:01:00s\n",
      " Epoch 363 took 0.16 seconds\n",
      "epoch 363| loss: 0.17888 | val_0_accuracy: 0.91317 |  0:01:00s\n",
      " Epoch 364 took 0.16 seconds\n",
      "epoch 364| loss: 0.1807  | val_0_accuracy: 0.90818 |  0:01:00s\n",
      " Epoch 365 took 0.17 seconds\n",
      "epoch 365| loss: 0.16244 | val_0_accuracy: 0.90719 |  0:01:00s\n",
      " Epoch 366 took 0.16 seconds\n",
      "epoch 366| loss: 0.19334 | val_0_accuracy: 0.87375 |  0:01:01s\n",
      " Epoch 367 took 0.16 seconds\n",
      "epoch 367| loss: 0.19663 | val_0_accuracy: 0.91018 |  0:01:01s\n",
      " Epoch 368 took 0.16 seconds\n",
      "epoch 368| loss: 0.18292 | val_0_accuracy: 0.8997  |  0:01:01s\n",
      " Epoch 369 took 0.16 seconds\n",
      "epoch 369| loss: 0.18087 | val_0_accuracy: 0.91717 |  0:01:01s\n",
      " Epoch 370 took 0.17 seconds\n",
      "epoch 370| loss: 0.1605  | val_0_accuracy: 0.91218 |  0:01:01s\n",
      " Epoch 371 took 0.16 seconds\n",
      "epoch 371| loss: 0.16907 | val_0_accuracy: 0.92365 |  0:01:01s\n",
      " Epoch 372 took 0.16 seconds\n",
      "epoch 372| loss: 0.17393 | val_0_accuracy: 0.92565 |  0:01:02s\n",
      " Epoch 373 took 0.17 seconds\n",
      "epoch 373| loss: 0.18497 | val_0_accuracy: 0.91617 |  0:01:02s\n",
      " Epoch 374 took 0.16 seconds\n",
      "epoch 374| loss: 0.18885 | val_0_accuracy: 0.88623 |  0:01:02s\n",
      " Epoch 375 took 0.16 seconds\n",
      "epoch 375| loss: 0.19606 | val_0_accuracy: 0.87874 |  0:01:02s\n",
      " Epoch 376 took 0.16 seconds\n",
      "epoch 376| loss: 0.17378 | val_0_accuracy: 0.91617 |  0:01:02s\n",
      " Epoch 377 took 0.16 seconds\n",
      "epoch 377| loss: 0.1787  | val_0_accuracy: 0.92914 |  0:01:02s\n",
      " Epoch 378 took 0.17 seconds\n",
      "epoch 378| loss: 0.194   | val_0_accuracy: 0.93463 |  0:01:03s\n",
      " Epoch 379 took 0.16 seconds\n",
      "epoch 379| loss: 0.18056 | val_0_accuracy: 0.93064 |  0:01:03s\n",
      " Epoch 380 took 0.17 seconds\n",
      "epoch 380| loss: 0.17641 | val_0_accuracy: 0.91267 |  0:01:03s\n",
      " Epoch 381 took 0.16 seconds\n",
      "epoch 381| loss: 0.16539 | val_0_accuracy: 0.91916 |  0:01:03s\n",
      " Epoch 382 took 0.16 seconds\n",
      "epoch 382| loss: 0.18052 | val_0_accuracy: 0.92016 |  0:01:03s\n",
      " Epoch 383 took 0.17 seconds\n",
      "epoch 383| loss: 0.18224 | val_0_accuracy: 0.91467 |  0:01:03s\n",
      " Epoch 384 took 0.16 seconds\n",
      "epoch 384| loss: 0.17358 | val_0_accuracy: 0.89072 |  0:01:03s\n",
      " Epoch 385 took 0.16 seconds\n",
      "epoch 385| loss: 0.1728  | val_0_accuracy: 0.90818 |  0:01:04s\n",
      " Epoch 386 took 0.17 seconds\n",
      "epoch 386| loss: 0.18474 | val_0_accuracy: 0.92315 |  0:01:04s\n",
      " Epoch 387 took 0.16 seconds\n",
      "epoch 387| loss: 0.17394 | val_0_accuracy: 0.93363 |  0:01:04s\n",
      " Epoch 388 took 0.17 seconds\n",
      "epoch 388| loss: 0.16788 | val_0_accuracy: 0.92066 |  0:01:04s\n",
      " Epoch 389 took 0.16 seconds\n",
      "epoch 389| loss: 0.19783 | val_0_accuracy: 0.92365 |  0:01:04s\n",
      " Epoch 390 took 0.16 seconds\n",
      "epoch 390| loss: 0.1906  | val_0_accuracy: 0.89222 |  0:01:04s\n",
      " Epoch 391 took 0.17 seconds\n",
      "epoch 391| loss: 0.19652 | val_0_accuracy: 0.92116 |  0:01:05s\n",
      " Epoch 392 took 0.16 seconds\n",
      "epoch 392| loss: 0.19694 | val_0_accuracy: 0.91367 |  0:01:05s\n",
      " Epoch 393 took 0.17 seconds\n",
      "epoch 393| loss: 0.19323 | val_0_accuracy: 0.92565 |  0:01:05s\n",
      " Epoch 394 took 0.19 seconds\n",
      "epoch 394| loss: 0.19152 | val_0_accuracy: 0.92964 |  0:01:05s\n",
      " Epoch 395 took 0.16 seconds\n",
      "epoch 395| loss: 0.18515 | val_0_accuracy: 0.93064 |  0:01:05s\n",
      " Epoch 396 took 0.16 seconds\n",
      "epoch 396| loss: 0.1732  | val_0_accuracy: 0.92465 |  0:01:05s\n",
      " Epoch 397 took 0.16 seconds\n",
      "epoch 397| loss: 0.16175 | val_0_accuracy: 0.92615 |  0:01:06s\n",
      " Epoch 398 took 0.16 seconds\n",
      "epoch 398| loss: 0.1722  | val_0_accuracy: 0.91317 |  0:01:06s\n",
      " Epoch 399 took 0.16 seconds\n",
      "epoch 399| loss: 0.17406 | val_0_accuracy: 0.91467 |  0:01:06s\n",
      " Epoch 400 took 0.17 seconds\n",
      "epoch 400| loss: 0.15944 | val_0_accuracy: 0.90868 |  0:01:06s\n",
      " Epoch 401 took 0.16 seconds\n",
      "epoch 401| loss: 0.16019 | val_0_accuracy: 0.91567 |  0:01:06s\n",
      " Epoch 402 took 0.16 seconds\n",
      "epoch 402| loss: 0.16066 | val_0_accuracy: 0.9012  |  0:01:06s\n",
      " Epoch 403 took 0.16 seconds\n",
      "epoch 403| loss: 0.17275 | val_0_accuracy: 0.88872 |  0:01:07s\n",
      " Epoch 404 took 0.16 seconds\n",
      "epoch 404| loss: 0.17602 | val_0_accuracy: 0.89072 |  0:01:07s\n",
      " Epoch 405 took 0.16 seconds\n",
      "epoch 405| loss: 0.19308 | val_0_accuracy: 0.9002  |  0:01:07s\n",
      " Epoch 406 took 0.16 seconds\n",
      "epoch 406| loss: 0.19065 | val_0_accuracy: 0.92715 |  0:01:07s\n",
      " Epoch 407 took 0.16 seconds\n",
      "epoch 407| loss: 0.1675  | val_0_accuracy: 0.91717 |  0:01:07s\n",
      " Epoch 408 took 0.16 seconds\n",
      "epoch 408| loss: 0.16339 | val_0_accuracy: 0.93463 |  0:01:07s\n",
      " Epoch 409 took 0.16 seconds\n",
      "epoch 409| loss: 0.17589 | val_0_accuracy: 0.91068 |  0:01:08s\n",
      " Epoch 410 took 0.16 seconds\n",
      "epoch 410| loss: 0.16303 | val_0_accuracy: 0.90868 |  0:01:08s\n",
      " Epoch 411 took 0.16 seconds\n",
      "epoch 411| loss: 0.16305 | val_0_accuracy: 0.93064 |  0:01:08s\n",
      " Epoch 412 took 0.16 seconds\n",
      "epoch 412| loss: 0.16552 | val_0_accuracy: 0.90269 |  0:01:08s\n",
      " Epoch 413 took 0.16 seconds\n",
      "epoch 413| loss: 0.16945 | val_0_accuracy: 0.91866 |  0:01:08s\n",
      " Epoch 414 took 0.30 seconds\n",
      "epoch 414| loss: 0.16473 | val_0_accuracy: 0.91317 |  0:01:09s\n",
      " Epoch 415 took 0.16 seconds\n",
      "epoch 415| loss: 0.16517 | val_0_accuracy: 0.90519 |  0:01:09s\n",
      " Epoch 416 took 0.16 seconds\n",
      "epoch 416| loss: 0.16092 | val_0_accuracy: 0.91367 |  0:01:09s\n",
      " Epoch 417 took 0.17 seconds\n",
      "epoch 417| loss: 0.15847 | val_0_accuracy: 0.9017  |  0:01:09s\n",
      " Epoch 418 took 0.17 seconds\n",
      "epoch 418| loss: 0.17233 | val_0_accuracy: 0.91417 |  0:01:09s\n",
      " Epoch 419 took 0.17 seconds\n",
      "epoch 419| loss: 0.17642 | val_0_accuracy: 0.92964 |  0:01:09s\n",
      " Epoch 420 took 0.20 seconds\n",
      "epoch 420| loss: 0.16504 | val_0_accuracy: 0.90319 |  0:01:10s\n",
      " Epoch 421 took 0.17 seconds\n",
      "epoch 421| loss: 0.17039 | val_0_accuracy: 0.91168 |  0:01:10s\n",
      " Epoch 422 took 0.18 seconds\n",
      "epoch 422| loss: 0.17064 | val_0_accuracy: 0.93114 |  0:01:10s\n",
      " Epoch 423 took 0.21 seconds\n",
      "epoch 423| loss: 0.15563 | val_0_accuracy: 0.92964 |  0:01:10s\n",
      " Epoch 424 took 0.18 seconds\n",
      "epoch 424| loss: 0.18362 | val_0_accuracy: 0.93164 |  0:01:10s\n",
      " Epoch 425 took 0.16 seconds\n",
      "epoch 425| loss: 0.16977 | val_0_accuracy: 0.90319 |  0:01:10s\n",
      " Epoch 426 took 0.16 seconds\n",
      "epoch 426| loss: 0.18252 | val_0_accuracy: 0.92016 |  0:01:11s\n",
      " Epoch 427 took 0.16 seconds\n",
      "epoch 427| loss: 0.17056 | val_0_accuracy: 0.88872 |  0:01:11s\n",
      " Epoch 428 took 0.17 seconds\n",
      "epoch 428| loss: 0.16297 | val_0_accuracy: 0.90469 |  0:01:11s\n",
      " Epoch 429 took 0.16 seconds\n",
      "epoch 429| loss: 0.17239 | val_0_accuracy: 0.91966 |  0:01:11s\n",
      " Epoch 430 took 0.17 seconds\n",
      "epoch 430| loss: 0.16388 | val_0_accuracy: 0.9012  |  0:01:11s\n",
      " Epoch 431 took 0.16 seconds\n",
      "epoch 431| loss: 0.15758 | val_0_accuracy: 0.92864 |  0:01:11s\n",
      " Epoch 432 took 0.17 seconds\n",
      "epoch 432| loss: 0.17017 | val_0_accuracy: 0.91267 |  0:01:12s\n",
      " Epoch 433 took 0.17 seconds\n",
      "epoch 433| loss: 0.15699 | val_0_accuracy: 0.91467 |  0:01:12s\n",
      " Epoch 434 took 0.17 seconds\n",
      "epoch 434| loss: 0.16019 | val_0_accuracy: 0.92864 |  0:01:12s\n",
      " Epoch 435 took 0.16 seconds\n",
      "epoch 435| loss: 0.16228 | val_0_accuracy: 0.92814 |  0:01:12s\n",
      " Epoch 436 took 0.17 seconds\n",
      "epoch 436| loss: 0.18056 | val_0_accuracy: 0.89721 |  0:01:12s\n",
      " Epoch 437 took 0.16 seconds\n",
      "epoch 437| loss: 0.15143 | val_0_accuracy: 0.9022  |  0:01:12s\n",
      " Epoch 438 took 0.16 seconds\n",
      "epoch 438| loss: 0.16927 | val_0_accuracy: 0.92315 |  0:01:13s\n",
      " Epoch 439 took 0.17 seconds\n",
      "epoch 439| loss: 0.1691  | val_0_accuracy: 0.90619 |  0:01:13s\n",
      " Epoch 440 took 0.16 seconds\n",
      "epoch 440| loss: 0.15749 | val_0_accuracy: 0.91517 |  0:01:13s\n",
      " Epoch 441 took 0.16 seconds\n",
      "epoch 441| loss: 0.17139 | val_0_accuracy: 0.92565 |  0:01:13s\n",
      " Epoch 442 took 0.16 seconds\n",
      "epoch 442| loss: 0.16885 | val_0_accuracy: 0.90968 |  0:01:13s\n",
      " Epoch 443 took 0.16 seconds\n",
      "epoch 443| loss: 0.15162 | val_0_accuracy: 0.93613 |  0:01:13s\n",
      " Epoch 444 took 0.17 seconds\n",
      "epoch 444| loss: 0.1588  | val_0_accuracy: 0.90369 |  0:01:14s\n",
      " Epoch 445 took 0.17 seconds\n",
      "epoch 445| loss: 0.17163 | val_0_accuracy: 0.92715 |  0:01:14s\n",
      " Epoch 446 took 0.16 seconds\n",
      "epoch 446| loss: 0.15877 | val_0_accuracy: 0.89471 |  0:01:14s\n",
      " Epoch 447 took 0.17 seconds\n",
      "epoch 447| loss: 0.15919 | val_0_accuracy: 0.93313 |  0:01:14s\n",
      " Epoch 448 took 0.16 seconds\n",
      "epoch 448| loss: 0.16398 | val_0_accuracy: 0.91267 |  0:01:14s\n",
      " Epoch 449 took 0.17 seconds\n",
      "epoch 449| loss: 0.18058 | val_0_accuracy: 0.91567 |  0:01:14s\n",
      " Epoch 450 took 0.18 seconds\n",
      "epoch 450| loss: 0.15971 | val_0_accuracy: 0.89271 |  0:01:15s\n",
      " Epoch 451 took 0.16 seconds\n",
      "epoch 451| loss: 0.16183 | val_0_accuracy: 0.91567 |  0:01:15s\n",
      " Epoch 452 took 0.16 seconds\n",
      "epoch 452| loss: 0.17019 | val_0_accuracy: 0.92515 |  0:01:15s\n",
      " Epoch 453 took 0.18 seconds\n",
      "epoch 453| loss: 0.16031 | val_0_accuracy: 0.91068 |  0:01:15s\n",
      " Epoch 454 took 0.18 seconds\n",
      "epoch 454| loss: 0.15398 | val_0_accuracy: 0.90818 |  0:01:15s\n",
      " Epoch 455 took 0.19 seconds\n",
      "epoch 455| loss: 0.15219 | val_0_accuracy: 0.90469 |  0:01:15s\n",
      " Epoch 456 took 0.18 seconds\n",
      "epoch 456| loss: 0.16299 | val_0_accuracy: 0.90619 |  0:01:16s\n",
      " Epoch 457 took 0.17 seconds\n",
      "epoch 457| loss: 0.15791 | val_0_accuracy: 0.92415 |  0:01:16s\n",
      " Epoch 458 took 0.17 seconds\n",
      "epoch 458| loss: 0.1646  | val_0_accuracy: 0.93214 |  0:01:16s\n",
      " Epoch 459 took 0.19 seconds\n",
      "epoch 459| loss: 0.16594 | val_0_accuracy: 0.93164 |  0:01:16s\n",
      " Epoch 460 took 0.16 seconds\n",
      "epoch 460| loss: 0.1689  | val_0_accuracy: 0.88473 |  0:01:16s\n",
      " Epoch 461 took 0.16 seconds\n",
      "epoch 461| loss: 0.1562  | val_0_accuracy: 0.9012  |  0:01:16s\n",
      " Epoch 462 took 0.16 seconds\n",
      "epoch 462| loss: 0.15504 | val_0_accuracy: 0.88573 |  0:01:17s\n",
      " Epoch 463 took 0.16 seconds\n",
      "epoch 463| loss: 0.15442 | val_0_accuracy: 0.93014 |  0:01:17s\n",
      " Epoch 464 took 0.16 seconds\n",
      "epoch 464| loss: 0.15217 | val_0_accuracy: 0.91916 |  0:01:17s\n",
      " Epoch 465 took 0.16 seconds\n",
      "epoch 465| loss: 0.15655 | val_0_accuracy: 0.91816 |  0:01:17s\n",
      " Epoch 466 took 0.16 seconds\n",
      "epoch 466| loss: 0.15217 | val_0_accuracy: 0.90868 |  0:01:17s\n",
      " Epoch 467 took 0.16 seconds\n",
      "epoch 467| loss: 0.15679 | val_0_accuracy: 0.87126 |  0:01:17s\n",
      " Epoch 468 took 0.16 seconds\n",
      "epoch 468| loss: 0.16102 | val_0_accuracy: 0.88772 |  0:01:18s\n",
      " Epoch 469 took 0.16 seconds\n",
      "epoch 469| loss: 0.17024 | val_0_accuracy: 0.89321 |  0:01:18s\n",
      " Epoch 470 took 0.16 seconds\n",
      "epoch 470| loss: 0.18487 | val_0_accuracy: 0.88623 |  0:01:18s\n",
      " Epoch 471 took 0.16 seconds\n",
      "epoch 471| loss: 0.17537 | val_0_accuracy: 0.91267 |  0:01:18s\n",
      " Epoch 472 took 0.17 seconds\n",
      "epoch 472| loss: 0.16049 | val_0_accuracy: 0.91517 |  0:01:18s\n",
      " Epoch 473 took 0.18 seconds\n",
      "epoch 473| loss: 0.16389 | val_0_accuracy: 0.92116 |  0:01:18s\n",
      " Epoch 474 took 0.16 seconds\n",
      "epoch 474| loss: 0.15632 | val_0_accuracy: 0.93064 |  0:01:19s\n",
      " Epoch 475 took 0.16 seconds\n",
      "epoch 475| loss: 0.15404 | val_0_accuracy: 0.92265 |  0:01:19s\n",
      " Epoch 476 took 0.16 seconds\n",
      "epoch 476| loss: 0.15599 | val_0_accuracy: 0.90319 |  0:01:19s\n",
      " Epoch 477 took 0.16 seconds\n",
      "epoch 477| loss: 0.15449 | val_0_accuracy: 0.91218 |  0:01:19s\n",
      " Epoch 478 took 0.16 seconds\n",
      "epoch 478| loss: 0.1502  | val_0_accuracy: 0.89621 |  0:01:19s\n",
      " Epoch 479 took 0.16 seconds\n",
      "epoch 479| loss: 0.16087 | val_0_accuracy: 0.91367 |  0:01:19s\n",
      " Epoch 480 took 0.16 seconds\n",
      "epoch 480| loss: 0.16174 | val_0_accuracy: 0.91218 |  0:01:20s\n",
      " Epoch 481 took 0.16 seconds\n",
      "epoch 481| loss: 0.14732 | val_0_accuracy: 0.93214 |  0:01:20s\n",
      " Epoch 482 took 0.16 seconds\n",
      "epoch 482| loss: 0.15478 | val_0_accuracy: 0.91367 |  0:01:20s\n",
      " Epoch 483 took 0.17 seconds\n",
      "epoch 483| loss: 0.15561 | val_0_accuracy: 0.8977  |  0:01:20s\n",
      " Epoch 484 took 0.16 seconds\n",
      "epoch 484| loss: 0.15333 | val_0_accuracy: 0.92864 |  0:01:20s\n",
      " Epoch 485 took 0.16 seconds\n",
      "epoch 485| loss: 0.16114 | val_0_accuracy: 0.90619 |  0:01:20s\n",
      " Epoch 486 took 0.16 seconds\n",
      "epoch 486| loss: 0.16498 | val_0_accuracy: 0.89721 |  0:01:21s\n",
      " Epoch 487 took 0.16 seconds\n",
      "epoch 487| loss: 0.14868 | val_0_accuracy: 0.88673 |  0:01:21s\n",
      " Epoch 488 took 0.16 seconds\n",
      "epoch 488| loss: 0.14962 | val_0_accuracy: 0.88922 |  0:01:21s\n",
      " Epoch 489 took 0.16 seconds\n",
      "epoch 489| loss: 0.15667 | val_0_accuracy: 0.92016 |  0:01:21s\n",
      " Epoch 490 took 0.16 seconds\n",
      "epoch 490| loss: 0.16542 | val_0_accuracy: 0.92365 |  0:01:21s\n",
      " Epoch 491 took 0.17 seconds\n",
      "epoch 491| loss: 0.15958 | val_0_accuracy: 0.92764 |  0:01:21s\n",
      " Epoch 492 took 0.16 seconds\n",
      "epoch 492| loss: 0.15277 | val_0_accuracy: 0.90719 |  0:01:21s\n",
      " Epoch 493 took 0.16 seconds\n",
      "epoch 493| loss: 0.15045 | val_0_accuracy: 0.90619 |  0:01:22s\n",
      " Epoch 494 took 0.16 seconds\n",
      "epoch 494| loss: 0.15377 | val_0_accuracy: 0.92914 |  0:01:22s\n",
      " Epoch 495 took 0.16 seconds\n",
      "epoch 495| loss: 0.17197 | val_0_accuracy: 0.92764 |  0:01:22s\n",
      " Epoch 496 took 0.17 seconds\n",
      "epoch 496| loss: 0.16286 | val_0_accuracy: 0.90469 |  0:01:22s\n",
      " Epoch 497 took 0.15 seconds\n",
      "epoch 497| loss: 0.17141 | val_0_accuracy: 0.89072 |  0:01:22s\n",
      " Epoch 498 took 0.16 seconds\n",
      "epoch 498| loss: 0.15832 | val_0_accuracy: 0.92665 |  0:01:23s\n",
      " Epoch 499 took 0.30 seconds\n",
      "epoch 499| loss: 0.15625 | val_0_accuracy: 0.92565 |  0:01:23s\n",
      " Epoch 500 took 0.16 seconds\n",
      "epoch 500| loss: 0.14335 | val_0_accuracy: 0.88124 |  0:01:23s\n",
      " Epoch 501 took 0.16 seconds\n",
      "epoch 501| loss: 0.14806 | val_0_accuracy: 0.90369 |  0:01:23s\n",
      " Epoch 502 took 0.16 seconds\n",
      "epoch 502| loss: 0.14913 | val_0_accuracy: 0.91567 |  0:01:23s\n",
      " Epoch 503 took 0.16 seconds\n",
      "epoch 503| loss: 0.15456 | val_0_accuracy: 0.93363 |  0:01:23s\n",
      " Epoch 504 took 0.16 seconds\n",
      "epoch 504| loss: 0.15406 | val_0_accuracy: 0.93363 |  0:01:24s\n",
      " Epoch 505 took 0.16 seconds\n",
      "epoch 505| loss: 0.15426 | val_0_accuracy: 0.92665 |  0:01:24s\n",
      " Epoch 506 took 0.16 seconds\n",
      "epoch 506| loss: 0.15531 | val_0_accuracy: 0.90369 |  0:01:24s\n",
      " Epoch 507 took 0.18 seconds\n",
      "epoch 507| loss: 0.14424 | val_0_accuracy: 0.91317 |  0:01:24s\n",
      " Epoch 508 took 0.17 seconds\n",
      "epoch 508| loss: 0.15533 | val_0_accuracy: 0.91317 |  0:01:24s\n",
      " Epoch 509 took 0.18 seconds\n",
      "epoch 509| loss: 0.15862 | val_0_accuracy: 0.8997  |  0:01:24s\n",
      " Epoch 510 took 0.17 seconds\n",
      "epoch 510| loss: 0.15279 | val_0_accuracy: 0.89671 |  0:01:25s\n",
      " Epoch 511 took 0.19 seconds\n",
      "epoch 511| loss: 0.16617 | val_0_accuracy: 0.9022  |  0:01:25s\n",
      " Epoch 512 took 0.19 seconds\n",
      "epoch 512| loss: 0.15702 | val_0_accuracy: 0.8977  |  0:01:25s\n",
      " Epoch 513 took 0.18 seconds\n",
      "epoch 513| loss: 0.17196 | val_0_accuracy: 0.91816 |  0:01:25s\n",
      " Epoch 514 took 0.16 seconds\n",
      "epoch 514| loss: 0.16884 | val_0_accuracy: 0.92565 |  0:01:25s\n",
      " Epoch 515 took 0.17 seconds\n",
      "epoch 515| loss: 0.16324 | val_0_accuracy: 0.91068 |  0:01:25s\n",
      " Epoch 516 took 0.17 seconds\n",
      "epoch 516| loss: 0.15931 | val_0_accuracy: 0.87475 |  0:01:26s\n",
      " Epoch 517 took 0.16 seconds\n",
      "epoch 517| loss: 0.14758 | val_0_accuracy: 0.9022  |  0:01:26s\n",
      " Epoch 518 took 0.16 seconds\n",
      "epoch 518| loss: 0.15322 | val_0_accuracy: 0.90968 |  0:01:26s\n",
      " Epoch 519 took 0.17 seconds\n",
      "epoch 519| loss: 0.16604 | val_0_accuracy: 0.92315 |  0:01:26s\n",
      " Epoch 520 took 0.15 seconds\n",
      "epoch 520| loss: 0.15482 | val_0_accuracy: 0.92515 |  0:01:26s\n",
      " Epoch 521 took 0.16 seconds\n",
      "epoch 521| loss: 0.15464 | val_0_accuracy: 0.91966 |  0:01:26s\n",
      " Epoch 522 took 0.16 seconds\n",
      "epoch 522| loss: 0.14594 | val_0_accuracy: 0.91617 |  0:01:27s\n",
      " Epoch 523 took 0.18 seconds\n",
      "epoch 523| loss: 0.14669 | val_0_accuracy: 0.91417 |  0:01:27s\n",
      " Epoch 524 took 0.19 seconds\n",
      "epoch 524| loss: 0.15352 | val_0_accuracy: 0.8987  |  0:01:27s\n",
      " Epoch 525 took 0.19 seconds\n",
      "epoch 525| loss: 0.1461  | val_0_accuracy: 0.88972 |  0:01:27s\n",
      " Epoch 526 took 0.17 seconds\n",
      "epoch 526| loss: 0.1443  | val_0_accuracy: 0.90768 |  0:01:27s\n",
      " Epoch 527 took 0.16 seconds\n",
      "epoch 527| loss: 0.13935 | val_0_accuracy: 0.92964 |  0:01:27s\n",
      " Epoch 528 took 0.16 seconds\n",
      "epoch 528| loss: 0.14341 | val_0_accuracy: 0.91467 |  0:01:28s\n",
      " Epoch 529 took 0.16 seconds\n",
      "epoch 529| loss: 0.14885 | val_0_accuracy: 0.89271 |  0:01:28s\n",
      " Epoch 530 took 0.16 seconds\n",
      "epoch 530| loss: 0.14172 | val_0_accuracy: 0.91966 |  0:01:28s\n",
      " Epoch 531 took 0.17 seconds\n",
      "epoch 531| loss: 0.14721 | val_0_accuracy: 0.91367 |  0:01:28s\n",
      " Epoch 532 took 0.16 seconds\n",
      "epoch 532| loss: 0.14543 | val_0_accuracy: 0.93463 |  0:01:28s\n",
      " Epoch 533 took 0.18 seconds\n",
      "epoch 533| loss: 0.15952 | val_0_accuracy: 0.92166 |  0:01:28s\n",
      " Epoch 534 took 0.18 seconds\n",
      "epoch 534| loss: 0.16361 | val_0_accuracy: 0.93214 |  0:01:29s\n",
      " Epoch 535 took 0.18 seconds\n",
      "epoch 535| loss: 0.16405 | val_0_accuracy: 0.92315 |  0:01:29s\n",
      " Epoch 536 took 0.17 seconds\n",
      "epoch 536| loss: 0.16395 | val_0_accuracy: 0.92665 |  0:01:29s\n",
      " Epoch 537 took 0.17 seconds\n",
      "epoch 537| loss: 0.16188 | val_0_accuracy: 0.92315 |  0:01:29s\n",
      " Epoch 538 took 0.16 seconds\n",
      "epoch 538| loss: 0.15481 | val_0_accuracy: 0.92465 |  0:01:29s\n",
      " Epoch 539 took 0.16 seconds\n",
      "epoch 539| loss: 0.15145 | val_0_accuracy: 0.92515 |  0:01:29s\n",
      " Epoch 540 took 0.17 seconds\n",
      "epoch 540| loss: 0.15502 | val_0_accuracy: 0.92515 |  0:01:30s\n",
      " Epoch 541 took 0.17 seconds\n",
      "epoch 541| loss: 0.14564 | val_0_accuracy: 0.89671 |  0:01:30s\n",
      " Epoch 542 took 0.17 seconds\n",
      "epoch 542| loss: 0.1447  | val_0_accuracy: 0.9007  |  0:01:30s\n",
      " Epoch 543 took 0.19 seconds\n",
      "epoch 543| loss: 0.14545 | val_0_accuracy: 0.89471 |  0:01:30s\n",
      " Epoch 544 took 0.17 seconds\n",
      "epoch 544| loss: 0.15035 | val_0_accuracy: 0.92265 |  0:01:30s\n",
      " Epoch 545 took 0.18 seconds\n",
      "epoch 545| loss: 0.14661 | val_0_accuracy: 0.90918 |  0:01:31s\n",
      " Epoch 546 took 0.17 seconds\n",
      "epoch 546| loss: 0.14242 | val_0_accuracy: 0.89471 |  0:01:31s\n",
      " Epoch 547 took 0.18 seconds\n",
      "epoch 547| loss: 0.15139 | val_0_accuracy: 0.90818 |  0:01:31s\n",
      " Epoch 548 took 0.18 seconds\n",
      "epoch 548| loss: 0.14869 | val_0_accuracy: 0.92216 |  0:01:31s\n",
      " Epoch 549 took 0.18 seconds\n",
      "epoch 549| loss: 0.15257 | val_0_accuracy: 0.90768 |  0:01:31s\n",
      " Epoch 550 took 0.18 seconds\n",
      "epoch 550| loss: 0.13595 | val_0_accuracy: 0.89721 |  0:01:31s\n",
      " Epoch 551 took 0.17 seconds\n",
      "epoch 551| loss: 0.14812 | val_0_accuracy: 0.90619 |  0:01:32s\n",
      " Epoch 552 took 0.18 seconds\n",
      "epoch 552| loss: 0.14586 | val_0_accuracy: 0.92864 |  0:01:32s\n",
      " Epoch 553 took 0.18 seconds\n",
      "epoch 553| loss: 0.15636 | val_0_accuracy: 0.93263 |  0:01:32s\n",
      " Epoch 554 took 0.18 seconds\n",
      "epoch 554| loss: 0.15161 | val_0_accuracy: 0.90369 |  0:01:32s\n",
      " Epoch 555 took 0.17 seconds\n",
      "epoch 555| loss: 0.15044 | val_0_accuracy: 0.90968 |  0:01:32s\n",
      " Epoch 556 took 0.17 seconds\n",
      "\n",
      "Early stopping occurred at epoch 555 with best_epoch = 355 and best_val_0_accuracy = 0.93613\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest, Ytest_decoded, le = split_data(traindata, testdata)\n",
    "best_tabnet_params = find_best_tabnet(Xtrain, Ytrain)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split 80% train, 20% validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    Xtrain, Ytrain,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=Ytrain\n",
    ")\n",
    "\n",
    "# Re-initialize TabNet with best params\n",
    "final_model = TabNetClassifier(\n",
    "    **{k: v for k, v in best_tabnet_params.items()},\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "epoch_timer = EpochTimer()\n",
    "\n",
    "# Retrain on full training data\n",
    "final_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    max_epochs=1000,\n",
    "    patience=200,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    callbacks=[epoch_timer]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, _, le = split_data(traindata, testdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "file_path = os.path.join(\"results\", f\"gtd{partition}.txt\")\n",
    "\n",
    "# Predict class indices for test set\n",
    "y_pred = final_model.predict(Xtest)\n",
    "y_proba = final_model.predict_proba(Xtest)\n",
    "y_pred_decoded = le.inverse_transform(y_pred)\n",
    "y_true_decoded = le.inverse_transform(Ytest)\n",
    "\n",
    "# Make sure the directory exists\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Compute accuracy from decoded labels\n",
    "acc = accuracy_score(y_true_decoded, y_pred_decoded)\n",
    "\n",
    "# Write metrics to file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "    file.write(f\"Precision weighted: {precision_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"Recall weighted: {recall_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score weighted: {f1_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"Precision micro: {precision_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"Recall micro: {recall_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score micro: {f1_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"Precision macro: {precision_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"Recall macro: {recall_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score macro: {f1_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"roc auc weighted: {roc_auc_score(y_true_decoded, y_proba, multi_class='ovr', average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"roc auc macro: {roc_auc_score(y_true_decoded, y_proba, multi_class='ovr', average='macro'):.4f}\\n\")\n",
    "    file.write(f\"roc auc micro: {roc_auc_score(y_true_decoded, y_proba, multi_class='ovr', average='micro'):.4f}\\n\")\n",
    "\n",
    "with open(f\"results/epoch_time_gtd{partition}.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(str(x) for x in epoch_timer.epoch_times))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "                          Abu Sayyaf Group (ASG)       0.90      0.99      0.94       144\n",
      "        African National Congress (South Africa)       0.99      1.00      1.00       144\n",
      "                                Al-Qaida in Iraq       0.74      0.78      0.76       144\n",
      "        Al-Qaida in the Arabian Peninsula (AQAP)       0.87      0.94      0.90       144\n",
      "                                      Al-Shabaab       0.99      1.00      0.99       144\n",
      "             Basque Fatherland and Freedom (ETA)       1.00      0.98      0.99       144\n",
      "                                      Boko Haram       0.93      0.99      0.96       144\n",
      "  Communist Party of India - Maoist (CPI-Maoist)       0.88      0.88      0.88       144\n",
      "       Corsican National Liberation Front (FLNC)       0.99      1.00      0.99       144\n",
      "                       Donetsk People's Republic       0.99      1.00      1.00       144\n",
      "Farabundo Marti National Liberation Front (FMLN)       0.97      0.99      0.98       144\n",
      "                               Fulani extremists       0.95      0.92      0.93       144\n",
      "                 Houthi extremists (Ansar Allah)       0.91      0.87      0.89       144\n",
      "                     Irish Republican Army (IRA)       0.98      1.00      0.99       144\n",
      "     Islamic State of Iraq and the Levant (ISIL)       0.66      0.69      0.67       144\n",
      "                  Kurdistan Workers' Party (PKK)       0.95      0.90      0.92       144\n",
      "         Liberation Tigers of Tamil Eelam (LTTE)       0.99      1.00      1.00       144\n",
      "         Manuel Rodriguez Patriotic Front (FPMR)       0.99      1.00      1.00       144\n",
      "                                         Maoists       0.85      0.85      0.85       144\n",
      "                               Muslim extremists       0.82      0.73      0.77       144\n",
      "      National Liberation Army of Colombia (ELN)       0.86      0.89      0.88       144\n",
      "                         New People's Army (NPA)       0.98      0.90      0.94       144\n",
      "               Nicaraguan Democratic Force (FDN)       0.99      0.96      0.98       144\n",
      "                                    Palestinians       0.95      0.93      0.94       144\n",
      "   Revolutionary Armed Forces of Colombia (FARC)       0.88      0.86      0.87       144\n",
      "                               Shining Path (SL)       0.74      0.96      0.83       144\n",
      "                                 Sikh Extremists       0.98      0.95      0.96       144\n",
      "                                         Taliban       0.91      0.97      0.94       144\n",
      "                 Tehrik-i-Taliban Pakistan (TTP)       0.93      0.88      0.90       144\n",
      "       Tupac Amaru Revolutionary Movement (MRTA)       0.95      0.65      0.77       144\n",
      "\n",
      "                                        accuracy                           0.91      4320\n",
      "                                       macro avg       0.92      0.91      0.91      4320\n",
      "                                    weighted avg       0.92      0.91      0.91      4320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Ytest_decoded, y_pred_decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(cm_normalized,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels,\n",
    "                cmap=\"viridis\",\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "    plt.title(f\"Normalized Confusion Matrix\", fontsize=18)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "    plt.ylabel(\"True Label\", fontsize=14)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = f\"results/confusion_matrix_partition_{partition}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved confusion matrix for partition {partition} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved confusion matrix for partition 478 to results/confusion_matrix_partition_478.png\n"
     ]
    }
   ],
   "source": [
    "# Get all unique class labels from the truths\n",
    "class_labels = np.unique(Ytest_decoded)\n",
    "\n",
    "plot_confusion_matrix(Ytest_decoded, y_pred_decoded, labels=class_labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TabNet)",
   "language": "python",
   "name": "tabnet-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
