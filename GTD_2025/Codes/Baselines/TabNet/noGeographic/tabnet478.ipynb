{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-tabnet in /opt/conda/lib/python3.11/site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (1.5.0)\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (2.3.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in /opt/conda/lib/python3.11/site-packages (from pytorch-tabnet) (4.66.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.3->pytorch-tabnet) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->pytorch-tabnet) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install pytorch-tabnet --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = f'../../../../data/top30groups/noGeographic/train1/train{partition}.csv'\n",
    "testpath = f'../../../../data/top30groups/noGeographic/test1/test{partition}.csv'\n",
    "\n",
    "traindata = pd.read_csv(trainpath, encoding='ISO-8859-1')\n",
    "testdata = pd.read_csv(testpath, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def split_data(dftrain, dftest):\n",
    "    Xtrain = dftrain.drop(columns=['gname']).values.astype(float)\n",
    "    Ytrain = dftrain['gname'].values\n",
    "    Xtest = dftest.drop(columns=['gname']).values.astype(float)\n",
    "    Ytest = dftest['gname'].values\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    Ytrain = le.fit_transform(Ytrain)\n",
    "    Ytest = le.transform(Ytest)\n",
    "\n",
    "    #y_pred_decoded = model.label_encoder.inverse_transform(y_pred)\n",
    "    y_true_decoded = le.inverse_transform(Ytest)\n",
    "\n",
    "    return Xtrain, Ytrain, Xtest, Ytest, y_true_decoded, le\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "class TabNetClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, lambda_sparse=1e-3, optimizer_params=None):\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.optimizer_params = optimizer_params or {'lr': 0.01}\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = TabNetClassifier(\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            lambda_sparse=self.lambda_sparse,\n",
    "            optimizer_params=self.optimizer_params,\n",
    "            seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        self.model.fit(\n",
    "            X, y,\n",
    "            eval_set=[(X, y)],\n",
    "            max_epochs=500,\n",
    "            patience=120,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            eval_metric=['accuracy']\n",
    "        )\n",
    "\n",
    "        self.classes_ = np.unique(y)  \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return (preds == y).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_tabnet.sklearn import TabNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "\n",
    "def find_best_tabnet(Xtrain, Ytrain, n_iter=20):\n",
    "    print(\"Starting TabNet grid search\")\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "    param_dist = {\n",
    "        'n_d': [8, 16, 24],\n",
    "        'n_a': [8, 16, 24],\n",
    "        'n_steps': [3, 4, 5],\n",
    "        'gamma': [1.0, 1.3, 1.5],\n",
    "        'lambda_sparse': [1e-4, 1e-3, 1e-2],\n",
    "        'optimizer_params': [{'lr': 0.01}]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=TabNetClassifierWrapper(),\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='accuracy',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(Xtrain, Ytrain)\n",
    "    print(\"Best parameters:\", random_search.best_params_)\n",
    "    print(\"Best accuracy:\", random_search.best_score_)\n",
    "\n",
    "    return random_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.callbacks import Callback\n",
    "import time\n",
    "\n",
    "class EpochTimer(Callback):\n",
    "    def __init__(self):\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch_idx, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch_idx, logs=None):\n",
    "        duration = time.time() - self.start_time\n",
    "        self.epoch_times.append(duration)\n",
    "        print(f\"⏱️ Epoch {epoch_idx + 1} took {duration:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TabNet grid search\n",
      "CUDA available: True\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 499 and best_val_0_accuracy = 0.76161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 493 and best_val_0_accuracy = 0.77708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 482 and best_val_0_accuracy = 0.7875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 477 and best_val_0_accuracy = 0.76339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 498 and best_val_0_accuracy = 0.78036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 494 and best_val_0_accuracy = 0.78185\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 482 and best_val_0_accuracy = 0.75833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 495 and best_val_0_accuracy = 0.75089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 498 and best_val_0_accuracy = 0.76577\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 499 and best_val_0_accuracy = 0.79048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 368 with best_epoch = 248 and best_val_0_accuracy = 0.66786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 493 and best_val_0_accuracy = 0.70238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 483 and best_val_0_accuracy = 0.68601\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 408 and best_val_0_accuracy = 0.68542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 462 and best_val_0_accuracy = 0.73333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 460 and best_val_0_accuracy = 0.74345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 470 and best_val_0_accuracy = 0.68631\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 457 and best_val_0_accuracy = 0.69732\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 469 and best_val_0_accuracy = 0.72321\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 461 and best_val_0_accuracy = 0.69435\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 439 and best_val_0_accuracy = 0.69554\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 464 and best_val_0_accuracy = 0.68155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 460 and best_val_0_accuracy = 0.68155\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 485 and best_val_0_accuracy = 0.71488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 493 and best_val_0_accuracy = 0.71935\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 492 and best_val_0_accuracy = 0.73571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 481 and best_val_0_accuracy = 0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 492 and best_val_0_accuracy = 0.7372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 433 and best_val_0_accuracy = 0.74821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 451 and best_val_0_accuracy = 0.72113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 476 and best_val_0_accuracy = 0.67738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 392 and best_val_0_accuracy = 0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 452 and best_val_0_accuracy = 0.7372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 497 and best_val_0_accuracy = 0.77411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 487 and best_val_0_accuracy = 0.74494\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 473 and best_val_0_accuracy = 0.72292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 482 and best_val_0_accuracy = 0.74821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 476 and best_val_0_accuracy = 0.72143\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 493 and best_val_0_accuracy = 0.69315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 495 and best_val_0_accuracy = 0.70952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 455 and best_val_0_accuracy = 0.70714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 480 and best_val_0_accuracy = 0.72292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 469 and best_val_0_accuracy = 0.73542\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 496 and best_val_0_accuracy = 0.71042\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 438 and best_val_0_accuracy = 0.70149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 499 and best_val_0_accuracy = 0.71935\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 489 and best_val_0_accuracy = 0.71548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 474 and best_val_0_accuracy = 0.7\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 455 and best_val_0_accuracy = 0.70268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 494 and best_val_0_accuracy = 0.74821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 476 and best_val_0_accuracy = 0.70923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 472 and best_val_0_accuracy = 0.72798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 486 and best_val_0_accuracy = 0.67738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 496 and best_val_0_accuracy = 0.69732\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 496 and best_val_0_accuracy = 0.69137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 455 and best_val_0_accuracy = 0.64048\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 489 and best_val_0_accuracy = 0.62768\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 482 and best_val_0_accuracy = 0.63988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 495 and best_val_0_accuracy = 0.66935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 482 and best_val_0_accuracy = 0.62976\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 496 and best_val_0_accuracy = 0.65327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 405 and best_val_0_accuracy = 0.61488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 484 and best_val_0_accuracy = 0.63452\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 451 and best_val_0_accuracy = 0.63869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 466 and best_val_0_accuracy = 0.62679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 489 and best_val_0_accuracy = 0.64018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 499 and best_val_0_accuracy = 0.60595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 474 and best_val_0_accuracy = 0.72143\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 485 and best_val_0_accuracy = 0.6994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 450 and best_val_0_accuracy = 0.72649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 465 and best_val_0_accuracy = 0.70863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 475 and best_val_0_accuracy = 0.7122\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 458 and best_val_0_accuracy = 0.65685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 481 and best_val_0_accuracy = 0.68452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 475 and best_val_0_accuracy = 0.65298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 444 and best_val_0_accuracy = 0.69375\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 490 and best_val_0_accuracy = 0.72113\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 470 and best_val_0_accuracy = 0.7994\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 480 and best_val_0_accuracy = 0.71935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 463 and best_val_0_accuracy = 0.775\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 488 and best_val_0_accuracy = 0.75268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 495 and best_val_0_accuracy = 0.7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 496 and best_val_0_accuracy = 0.78065\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 482 and best_val_0_accuracy = 0.77262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 451 and best_val_0_accuracy = 0.75387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 488 and best_val_0_accuracy = 0.60804\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 489 and best_val_0_accuracy = 0.63155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 494 and best_val_0_accuracy = 0.70417\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 478 and best_val_0_accuracy = 0.72292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 489 and best_val_0_accuracy = 0.75327\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 487 and best_val_0_accuracy = 0.72887\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 489 and best_val_0_accuracy = 0.74077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 483 and best_val_0_accuracy = 0.65149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 497 and best_val_0_accuracy = 0.64673\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 497 and best_val_0_accuracy = 0.68661\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 475 and best_val_0_accuracy = 0.63839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 490 and best_val_0_accuracy = 0.66399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 480 and best_val_0_accuracy = 0.66429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 495 and best_val_0_accuracy = 0.69762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 489 and best_val_0_accuracy = 0.68095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 500 with best_epoch = 499 and best_val_0_accuracy = 0.76667\n",
      "Best parameters: {'optimizer_params': {'lr': 0.01}, 'n_steps': 3, 'n_d': 24, 'n_a': 16, 'lambda_sparse': 0.01, 'gamma': 1.3}\n",
      "Best accuracy: 0.6323809523809524\n",
      "epoch 0  | loss: 4.41748 | val_0_accuracy: 0.02857 |  0:00:00s\n",
      "⏱️ Epoch 1 took 0.08 seconds\n",
      "epoch 1  | loss: 3.88126 | val_0_accuracy: 0.02738 |  0:00:00s\n",
      "⏱️ Epoch 2 took 0.08 seconds\n",
      "epoch 2  | loss: 3.64948 | val_0_accuracy: 0.03929 |  0:00:00s\n",
      "⏱️ Epoch 3 took 0.09 seconds\n",
      "epoch 3  | loss: 3.51091 | val_0_accuracy: 0.05    |  0:00:00s\n",
      "⏱️ Epoch 4 took 0.09 seconds\n",
      "epoch 4  | loss: 3.39774 | val_0_accuracy: 0.03214 |  0:00:00s\n",
      "⏱️ Epoch 5 took 0.08 seconds\n",
      "epoch 5  | loss: 3.31327 | val_0_accuracy: 0.04881 |  0:00:00s\n",
      "⏱️ Epoch 6 took 0.08 seconds\n",
      "epoch 6  | loss: 3.21937 | val_0_accuracy: 0.05119 |  0:00:00s\n",
      "⏱️ Epoch 7 took 0.08 seconds\n",
      "epoch 7  | loss: 3.1559  | val_0_accuracy: 0.06071 |  0:00:00s\n",
      "⏱️ Epoch 8 took 0.08 seconds\n",
      "epoch 8  | loss: 3.07255 | val_0_accuracy: 0.04524 |  0:00:00s\n",
      "⏱️ Epoch 9 took 0.08 seconds\n",
      "epoch 9  | loss: 2.99363 | val_0_accuracy: 0.02976 |  0:00:00s\n",
      "⏱️ Epoch 10 took 0.08 seconds\n",
      "epoch 10 | loss: 2.93148 | val_0_accuracy: 0.02619 |  0:00:00s\n",
      "⏱️ Epoch 11 took 0.08 seconds\n",
      "epoch 11 | loss: 2.87071 | val_0_accuracy: 0.04286 |  0:00:00s\n",
      "⏱️ Epoch 12 took 0.08 seconds\n",
      "epoch 12 | loss: 2.81517 | val_0_accuracy: 0.0369  |  0:00:01s\n",
      "⏱️ Epoch 13 took 0.08 seconds\n",
      "epoch 13 | loss: 2.77093 | val_0_accuracy: 0.05595 |  0:00:01s\n",
      "⏱️ Epoch 14 took 0.08 seconds\n",
      "epoch 14 | loss: 2.69914 | val_0_accuracy: 0.05952 |  0:00:01s\n",
      "⏱️ Epoch 15 took 0.08 seconds\n",
      "epoch 15 | loss: 2.66933 | val_0_accuracy: 0.05357 |  0:00:01s\n",
      "⏱️ Epoch 16 took 0.08 seconds\n",
      "epoch 16 | loss: 2.60536 | val_0_accuracy: 0.06071 |  0:00:01s\n",
      "⏱️ Epoch 17 took 0.08 seconds\n",
      "epoch 17 | loss: 2.54078 | val_0_accuracy: 0.06905 |  0:00:01s\n",
      "⏱️ Epoch 18 took 0.08 seconds\n",
      "epoch 18 | loss: 2.50875 | val_0_accuracy: 0.06548 |  0:00:01s\n",
      "⏱️ Epoch 19 took 0.08 seconds\n",
      "epoch 19 | loss: 2.47196 | val_0_accuracy: 0.05952 |  0:00:01s\n",
      "⏱️ Epoch 20 took 0.08 seconds\n",
      "epoch 20 | loss: 2.41297 | val_0_accuracy: 0.0619  |  0:00:01s\n",
      "⏱️ Epoch 21 took 0.08 seconds\n",
      "epoch 21 | loss: 2.39663 | val_0_accuracy: 0.06905 |  0:00:01s\n",
      "⏱️ Epoch 22 took 0.08 seconds\n",
      "epoch 22 | loss: 2.36723 | val_0_accuracy: 0.075   |  0:00:01s\n",
      "⏱️ Epoch 23 took 0.09 seconds\n",
      "epoch 23 | loss: 2.36318 | val_0_accuracy: 0.07024 |  0:00:01s\n",
      "⏱️ Epoch 24 took 0.08 seconds\n",
      "epoch 24 | loss: 2.32024 | val_0_accuracy: 0.07738 |  0:00:02s\n",
      "⏱️ Epoch 25 took 0.08 seconds\n",
      "epoch 25 | loss: 2.29166 | val_0_accuracy: 0.08095 |  0:00:02s\n",
      "⏱️ Epoch 26 took 0.09 seconds\n",
      "epoch 26 | loss: 2.26886 | val_0_accuracy: 0.07381 |  0:00:02s\n",
      "⏱️ Epoch 27 took 0.08 seconds\n",
      "epoch 27 | loss: 2.27759 | val_0_accuracy: 0.07857 |  0:00:02s\n",
      "⏱️ Epoch 28 took 0.08 seconds\n",
      "epoch 28 | loss: 2.25128 | val_0_accuracy: 0.07976 |  0:00:02s\n",
      "⏱️ Epoch 29 took 0.09 seconds\n",
      "epoch 29 | loss: 2.22865 | val_0_accuracy: 0.08214 |  0:00:02s\n",
      "⏱️ Epoch 30 took 0.08 seconds\n",
      "epoch 30 | loss: 2.21085 | val_0_accuracy: 0.08095 |  0:00:02s\n",
      "⏱️ Epoch 31 took 0.08 seconds\n",
      "epoch 31 | loss: 2.14469 | val_0_accuracy: 0.09167 |  0:00:02s\n",
      "⏱️ Epoch 32 took 0.09 seconds\n",
      "epoch 32 | loss: 2.13471 | val_0_accuracy: 0.09286 |  0:00:02s\n",
      "⏱️ Epoch 33 took 0.09 seconds\n",
      "epoch 33 | loss: 2.10236 | val_0_accuracy: 0.09286 |  0:00:02s\n",
      "⏱️ Epoch 34 took 0.08 seconds\n",
      "epoch 34 | loss: 2.14743 | val_0_accuracy: 0.11429 |  0:00:02s\n",
      "⏱️ Epoch 35 took 0.09 seconds\n",
      "epoch 35 | loss: 2.08088 | val_0_accuracy: 0.11667 |  0:00:02s\n",
      "⏱️ Epoch 36 took 0.08 seconds\n",
      "epoch 36 | loss: 2.08067 | val_0_accuracy: 0.10952 |  0:00:03s\n",
      "⏱️ Epoch 37 took 0.08 seconds\n",
      "epoch 37 | loss: 2.04007 | val_0_accuracy: 0.1     |  0:00:03s\n",
      "⏱️ Epoch 38 took 0.08 seconds\n",
      "epoch 38 | loss: 2.05753 | val_0_accuracy: 0.10595 |  0:00:03s\n",
      "⏱️ Epoch 39 took 0.08 seconds\n",
      "epoch 39 | loss: 2.03784 | val_0_accuracy: 0.1119  |  0:00:03s\n",
      "⏱️ Epoch 40 took 0.08 seconds\n",
      "epoch 40 | loss: 2.00598 | val_0_accuracy: 0.11429 |  0:00:03s\n",
      "⏱️ Epoch 41 took 0.08 seconds\n",
      "epoch 41 | loss: 2.02989 | val_0_accuracy: 0.1131  |  0:00:03s\n",
      "⏱️ Epoch 42 took 0.08 seconds\n",
      "epoch 42 | loss: 1.99621 | val_0_accuracy: 0.12143 |  0:00:03s\n",
      "⏱️ Epoch 43 took 0.09 seconds\n",
      "epoch 43 | loss: 2.03657 | val_0_accuracy: 0.13452 |  0:00:03s\n",
      "⏱️ Epoch 44 took 0.09 seconds\n",
      "epoch 44 | loss: 1.94704 | val_0_accuracy: 0.12619 |  0:00:03s\n",
      "⏱️ Epoch 45 took 0.08 seconds\n",
      "epoch 45 | loss: 1.95179 | val_0_accuracy: 0.14048 |  0:00:03s\n",
      "⏱️ Epoch 46 took 0.09 seconds\n",
      "epoch 46 | loss: 1.97354 | val_0_accuracy: 0.14881 |  0:00:03s\n",
      "⏱️ Epoch 47 took 0.09 seconds\n",
      "epoch 47 | loss: 1.93206 | val_0_accuracy: 0.11905 |  0:00:03s\n",
      "⏱️ Epoch 48 took 0.08 seconds\n",
      "epoch 48 | loss: 1.92089 | val_0_accuracy: 0.1119  |  0:00:04s\n",
      "⏱️ Epoch 49 took 0.08 seconds\n",
      "epoch 49 | loss: 1.93902 | val_0_accuracy: 0.12262 |  0:00:04s\n",
      "⏱️ Epoch 50 took 0.09 seconds\n",
      "epoch 50 | loss: 1.90764 | val_0_accuracy: 0.12857 |  0:00:04s\n",
      "⏱️ Epoch 51 took 0.09 seconds\n",
      "epoch 51 | loss: 1.91266 | val_0_accuracy: 0.11429 |  0:00:04s\n",
      "⏱️ Epoch 52 took 0.08 seconds\n",
      "epoch 52 | loss: 1.90062 | val_0_accuracy: 0.12738 |  0:00:04s\n",
      "⏱️ Epoch 53 took 0.08 seconds\n",
      "epoch 53 | loss: 1.86259 | val_0_accuracy: 0.1369  |  0:00:04s\n",
      "⏱️ Epoch 54 took 0.08 seconds\n",
      "epoch 54 | loss: 1.8837  | val_0_accuracy: 0.13095 |  0:00:04s\n",
      "⏱️ Epoch 55 took 0.08 seconds\n",
      "epoch 55 | loss: 1.89647 | val_0_accuracy: 0.14167 |  0:00:04s\n",
      "⏱️ Epoch 56 took 0.08 seconds\n",
      "epoch 56 | loss: 1.86909 | val_0_accuracy: 0.15119 |  0:00:04s\n",
      "⏱️ Epoch 57 took 0.08 seconds\n",
      "epoch 57 | loss: 1.90835 | val_0_accuracy: 0.14048 |  0:00:04s\n",
      "⏱️ Epoch 58 took 0.08 seconds\n",
      "epoch 58 | loss: 1.852   | val_0_accuracy: 0.14762 |  0:00:04s\n",
      "⏱️ Epoch 59 took 0.08 seconds\n",
      "epoch 59 | loss: 1.88196 | val_0_accuracy: 0.15119 |  0:00:04s\n",
      "⏱️ Epoch 60 took 0.08 seconds\n",
      "epoch 60 | loss: 1.84404 | val_0_accuracy: 0.1619  |  0:00:05s\n",
      "⏱️ Epoch 61 took 0.08 seconds\n",
      "epoch 61 | loss: 1.85637 | val_0_accuracy: 0.17976 |  0:00:05s\n",
      "⏱️ Epoch 62 took 0.09 seconds\n",
      "epoch 62 | loss: 1.79425 | val_0_accuracy: 0.22143 |  0:00:05s\n",
      "⏱️ Epoch 63 took 0.09 seconds\n",
      "epoch 63 | loss: 1.85999 | val_0_accuracy: 0.2119  |  0:00:05s\n",
      "⏱️ Epoch 64 took 0.08 seconds\n",
      "epoch 64 | loss: 1.77798 | val_0_accuracy: 0.2131  |  0:00:05s\n",
      "⏱️ Epoch 65 took 0.09 seconds\n",
      "epoch 65 | loss: 1.81448 | val_0_accuracy: 0.19762 |  0:00:05s\n",
      "⏱️ Epoch 66 took 0.11 seconds\n",
      "epoch 66 | loss: 1.84097 | val_0_accuracy: 0.2131  |  0:00:05s\n",
      "⏱️ Epoch 67 took 0.11 seconds\n",
      "epoch 67 | loss: 1.82354 | val_0_accuracy: 0.24048 |  0:00:05s\n",
      "⏱️ Epoch 68 took 0.08 seconds\n",
      "epoch 68 | loss: 1.79384 | val_0_accuracy: 0.24643 |  0:00:05s\n",
      "⏱️ Epoch 69 took 0.09 seconds\n",
      "epoch 69 | loss: 1.81734 | val_0_accuracy: 0.24048 |  0:00:05s\n",
      "⏱️ Epoch 70 took 0.08 seconds\n",
      "epoch 70 | loss: 1.79671 | val_0_accuracy: 0.25476 |  0:00:05s\n",
      "⏱️ Epoch 71 took 0.08 seconds\n",
      "epoch 71 | loss: 1.79784 | val_0_accuracy: 0.27738 |  0:00:06s\n",
      "⏱️ Epoch 72 took 0.09 seconds\n",
      "epoch 72 | loss: 1.80499 | val_0_accuracy: 0.26429 |  0:00:06s\n",
      "⏱️ Epoch 73 took 0.09 seconds\n",
      "epoch 73 | loss: 1.79311 | val_0_accuracy: 0.30952 |  0:00:06s\n",
      "⏱️ Epoch 74 took 0.09 seconds\n",
      "epoch 74 | loss: 1.7695  | val_0_accuracy: 0.25595 |  0:00:06s\n",
      "⏱️ Epoch 75 took 0.08 seconds\n",
      "epoch 75 | loss: 1.7623  | val_0_accuracy: 0.25238 |  0:00:06s\n",
      "⏱️ Epoch 76 took 0.08 seconds\n",
      "epoch 76 | loss: 1.74776 | val_0_accuracy: 0.28929 |  0:00:06s\n",
      "⏱️ Epoch 77 took 0.08 seconds\n",
      "epoch 77 | loss: 1.77476 | val_0_accuracy: 0.31667 |  0:00:06s\n",
      "⏱️ Epoch 78 took 0.09 seconds\n",
      "epoch 78 | loss: 1.72069 | val_0_accuracy: 0.32976 |  0:00:06s\n",
      "⏱️ Epoch 79 took 0.09 seconds\n",
      "epoch 79 | loss: 1.73181 | val_0_accuracy: 0.35476 |  0:00:06s\n",
      "⏱️ Epoch 80 took 0.09 seconds\n",
      "epoch 80 | loss: 1.73634 | val_0_accuracy: 0.34405 |  0:00:06s\n",
      "⏱️ Epoch 81 took 0.08 seconds\n",
      "epoch 81 | loss: 1.7236  | val_0_accuracy: 0.34286 |  0:00:07s\n",
      "⏱️ Epoch 82 took 0.21 seconds\n",
      "epoch 82 | loss: 1.71099 | val_0_accuracy: 0.38452 |  0:00:07s\n",
      "⏱️ Epoch 83 took 0.09 seconds\n",
      "epoch 83 | loss: 1.73739 | val_0_accuracy: 0.3369  |  0:00:07s\n",
      "⏱️ Epoch 84 took 0.08 seconds\n",
      "epoch 84 | loss: 1.71965 | val_0_accuracy: 0.32857 |  0:00:07s\n",
      "⏱️ Epoch 85 took 0.08 seconds\n",
      "epoch 85 | loss: 1.69291 | val_0_accuracy: 0.34762 |  0:00:07s\n",
      "⏱️ Epoch 86 took 0.08 seconds\n",
      "epoch 86 | loss: 1.71409 | val_0_accuracy: 0.37024 |  0:00:07s\n",
      "⏱️ Epoch 87 took 0.08 seconds\n",
      "epoch 87 | loss: 1.75536 | val_0_accuracy: 0.42262 |  0:00:07s\n",
      "⏱️ Epoch 88 took 0.08 seconds\n",
      "epoch 88 | loss: 1.72023 | val_0_accuracy: 0.39048 |  0:00:07s\n",
      "⏱️ Epoch 89 took 0.08 seconds\n",
      "epoch 89 | loss: 1.67226 | val_0_accuracy: 0.40119 |  0:00:07s\n",
      "⏱️ Epoch 90 took 0.08 seconds\n",
      "epoch 90 | loss: 1.82629 | val_0_accuracy: 0.39643 |  0:00:07s\n",
      "⏱️ Epoch 91 took 0.09 seconds\n",
      "epoch 91 | loss: 1.72731 | val_0_accuracy: 0.40357 |  0:00:07s\n",
      "⏱️ Epoch 92 took 0.08 seconds\n",
      "epoch 92 | loss: 1.71204 | val_0_accuracy: 0.41429 |  0:00:07s\n",
      "⏱️ Epoch 93 took 0.08 seconds\n",
      "epoch 93 | loss: 1.67796 | val_0_accuracy: 0.42738 |  0:00:08s\n",
      "⏱️ Epoch 94 took 0.09 seconds\n",
      "epoch 94 | loss: 1.68512 | val_0_accuracy: 0.4131  |  0:00:08s\n",
      "⏱️ Epoch 95 took 0.08 seconds\n",
      "epoch 95 | loss: 1.68173 | val_0_accuracy: 0.43333 |  0:00:08s\n",
      "⏱️ Epoch 96 took 0.08 seconds\n",
      "epoch 96 | loss: 1.66953 | val_0_accuracy: 0.43929 |  0:00:08s\n",
      "⏱️ Epoch 97 took 0.09 seconds\n",
      "epoch 97 | loss: 1.63538 | val_0_accuracy: 0.46667 |  0:00:08s\n",
      "⏱️ Epoch 98 took 0.09 seconds\n",
      "epoch 98 | loss: 1.73568 | val_0_accuracy: 0.44405 |  0:00:08s\n",
      "⏱️ Epoch 99 took 0.08 seconds\n",
      "epoch 99 | loss: 1.6622  | val_0_accuracy: 0.46548 |  0:00:08s\n",
      "⏱️ Epoch 100 took 0.08 seconds\n",
      "epoch 100| loss: 1.65055 | val_0_accuracy: 0.44524 |  0:00:08s\n",
      "⏱️ Epoch 101 took 0.08 seconds\n",
      "epoch 101| loss: 1.62688 | val_0_accuracy: 0.45952 |  0:00:08s\n",
      "⏱️ Epoch 102 took 0.08 seconds\n",
      "epoch 102| loss: 1.64991 | val_0_accuracy: 0.42857 |  0:00:08s\n",
      "⏱️ Epoch 103 took 0.08 seconds\n",
      "epoch 103| loss: 1.61541 | val_0_accuracy: 0.43333 |  0:00:08s\n",
      "⏱️ Epoch 104 took 0.08 seconds\n",
      "epoch 104| loss: 1.67927 | val_0_accuracy: 0.42976 |  0:00:08s\n",
      "⏱️ Epoch 105 took 0.09 seconds\n",
      "epoch 105| loss: 1.65517 | val_0_accuracy: 0.47024 |  0:00:09s\n",
      "⏱️ Epoch 106 took 0.09 seconds\n",
      "epoch 106| loss: 1.66799 | val_0_accuracy: 0.45357 |  0:00:09s\n",
      "⏱️ Epoch 107 took 0.08 seconds\n",
      "epoch 107| loss: 1.64168 | val_0_accuracy: 0.47262 |  0:00:09s\n",
      "⏱️ Epoch 108 took 0.09 seconds\n",
      "epoch 108| loss: 1.67429 | val_0_accuracy: 0.44405 |  0:00:09s\n",
      "⏱️ Epoch 109 took 0.08 seconds\n",
      "epoch 109| loss: 1.61742 | val_0_accuracy: 0.44643 |  0:00:09s\n",
      "⏱️ Epoch 110 took 0.09 seconds\n",
      "epoch 110| loss: 1.59484 | val_0_accuracy: 0.47857 |  0:00:09s\n",
      "⏱️ Epoch 111 took 0.09 seconds\n",
      "epoch 111| loss: 1.67429 | val_0_accuracy: 0.46548 |  0:00:09s\n",
      "⏱️ Epoch 112 took 0.09 seconds\n",
      "epoch 112| loss: 1.62954 | val_0_accuracy: 0.44524 |  0:00:09s\n",
      "⏱️ Epoch 113 took 0.08 seconds\n",
      "epoch 113| loss: 1.6202  | val_0_accuracy: 0.45714 |  0:00:09s\n",
      "⏱️ Epoch 114 took 0.08 seconds\n",
      "epoch 114| loss: 1.62382 | val_0_accuracy: 0.47024 |  0:00:09s\n",
      "⏱️ Epoch 115 took 0.08 seconds\n",
      "epoch 115| loss: 1.58429 | val_0_accuracy: 0.46786 |  0:00:09s\n",
      "⏱️ Epoch 116 took 0.08 seconds\n",
      "epoch 116| loss: 1.58797 | val_0_accuracy: 0.48571 |  0:00:09s\n",
      "⏱️ Epoch 117 took 0.09 seconds\n",
      "epoch 117| loss: 1.57467 | val_0_accuracy: 0.475   |  0:00:09s\n",
      "⏱️ Epoch 118 took 0.08 seconds\n",
      "epoch 118| loss: 1.59953 | val_0_accuracy: 0.48095 |  0:00:10s\n",
      "⏱️ Epoch 119 took 0.09 seconds\n",
      "epoch 119| loss: 1.62212 | val_0_accuracy: 0.47738 |  0:00:10s\n",
      "⏱️ Epoch 120 took 0.08 seconds\n",
      "epoch 120| loss: 1.54448 | val_0_accuracy: 0.51071 |  0:00:10s\n",
      "⏱️ Epoch 121 took 0.09 seconds\n",
      "epoch 121| loss: 1.55598 | val_0_accuracy: 0.50476 |  0:00:10s\n",
      "⏱️ Epoch 122 took 0.09 seconds\n",
      "epoch 122| loss: 1.59928 | val_0_accuracy: 0.49643 |  0:00:10s\n",
      "⏱️ Epoch 123 took 0.08 seconds\n",
      "epoch 123| loss: 1.62943 | val_0_accuracy: 0.47857 |  0:00:10s\n",
      "⏱️ Epoch 124 took 0.09 seconds\n",
      "epoch 124| loss: 1.55795 | val_0_accuracy: 0.50833 |  0:00:10s\n",
      "⏱️ Epoch 125 took 0.08 seconds\n",
      "epoch 125| loss: 1.5917  | val_0_accuracy: 0.47619 |  0:00:10s\n",
      "⏱️ Epoch 126 took 0.08 seconds\n",
      "epoch 126| loss: 1.59714 | val_0_accuracy: 0.49643 |  0:00:10s\n",
      "⏱️ Epoch 127 took 0.08 seconds\n",
      "epoch 127| loss: 1.57213 | val_0_accuracy: 0.45476 |  0:00:10s\n",
      "⏱️ Epoch 128 took 0.08 seconds\n",
      "epoch 128| loss: 1.57749 | val_0_accuracy: 0.50357 |  0:00:10s\n",
      "⏱️ Epoch 129 took 0.08 seconds\n",
      "epoch 129| loss: 1.58356 | val_0_accuracy: 0.51429 |  0:00:10s\n",
      "⏱️ Epoch 130 took 0.08 seconds\n",
      "epoch 130| loss: 1.57009 | val_0_accuracy: 0.52381 |  0:00:11s\n",
      "⏱️ Epoch 131 took 0.08 seconds\n",
      "epoch 131| loss: 1.54197 | val_0_accuracy: 0.47143 |  0:00:11s\n",
      "⏱️ Epoch 132 took 0.08 seconds\n",
      "epoch 132| loss: 1.52237 | val_0_accuracy: 0.51667 |  0:00:11s\n",
      "⏱️ Epoch 133 took 0.09 seconds\n",
      "epoch 133| loss: 1.588   | val_0_accuracy: 0.51429 |  0:00:11s\n",
      "⏱️ Epoch 134 took 0.08 seconds\n",
      "epoch 134| loss: 1.52099 | val_0_accuracy: 0.50119 |  0:00:11s\n",
      "⏱️ Epoch 135 took 0.09 seconds\n",
      "epoch 135| loss: 1.55172 | val_0_accuracy: 0.52024 |  0:00:11s\n",
      "⏱️ Epoch 136 took 0.08 seconds\n",
      "epoch 136| loss: 1.51712 | val_0_accuracy: 0.49524 |  0:00:11s\n",
      "⏱️ Epoch 137 took 0.08 seconds\n",
      "epoch 137| loss: 1.49932 | val_0_accuracy: 0.51667 |  0:00:11s\n",
      "⏱️ Epoch 138 took 0.08 seconds\n",
      "epoch 138| loss: 1.52903 | val_0_accuracy: 0.53929 |  0:00:11s\n",
      "⏱️ Epoch 139 took 0.08 seconds\n",
      "epoch 139| loss: 1.58896 | val_0_accuracy: 0.52381 |  0:00:11s\n",
      "⏱️ Epoch 140 took 0.08 seconds\n",
      "epoch 140| loss: 1.56132 | val_0_accuracy: 0.50595 |  0:00:11s\n",
      "⏱️ Epoch 141 took 0.08 seconds\n",
      "epoch 141| loss: 1.54118 | val_0_accuracy: 0.49643 |  0:00:11s\n",
      "⏱️ Epoch 142 took 0.08 seconds\n",
      "epoch 142| loss: 1.51895 | val_0_accuracy: 0.50952 |  0:00:12s\n",
      "⏱️ Epoch 143 took 0.08 seconds\n",
      "epoch 143| loss: 1.5468  | val_0_accuracy: 0.53929 |  0:00:12s\n",
      "⏱️ Epoch 144 took 0.08 seconds\n",
      "epoch 144| loss: 1.49859 | val_0_accuracy: 0.52381 |  0:00:12s\n",
      "⏱️ Epoch 145 took 0.08 seconds\n",
      "epoch 145| loss: 1.50228 | val_0_accuracy: 0.50833 |  0:00:12s\n",
      "⏱️ Epoch 146 took 0.08 seconds\n",
      "epoch 146| loss: 1.51765 | val_0_accuracy: 0.47857 |  0:00:12s\n",
      "⏱️ Epoch 147 took 0.08 seconds\n",
      "epoch 147| loss: 1.53915 | val_0_accuracy: 0.49286 |  0:00:12s\n",
      "⏱️ Epoch 148 took 0.08 seconds\n",
      "epoch 148| loss: 1.51267 | val_0_accuracy: 0.51667 |  0:00:12s\n",
      "⏱️ Epoch 149 took 0.08 seconds\n",
      "epoch 149| loss: 1.49641 | val_0_accuracy: 0.54881 |  0:00:12s\n",
      "⏱️ Epoch 150 took 0.09 seconds\n",
      "epoch 150| loss: 1.48914 | val_0_accuracy: 0.52976 |  0:00:12s\n",
      "⏱️ Epoch 151 took 0.09 seconds\n",
      "epoch 151| loss: 1.50956 | val_0_accuracy: 0.51429 |  0:00:12s\n",
      "⏱️ Epoch 152 took 0.09 seconds\n",
      "epoch 152| loss: 1.52279 | val_0_accuracy: 0.50357 |  0:00:12s\n",
      "⏱️ Epoch 153 took 0.08 seconds\n",
      "epoch 153| loss: 1.47548 | val_0_accuracy: 0.53452 |  0:00:12s\n",
      "⏱️ Epoch 154 took 0.08 seconds\n",
      "epoch 154| loss: 1.55202 | val_0_accuracy: 0.50714 |  0:00:13s\n",
      "⏱️ Epoch 155 took 0.08 seconds\n",
      "epoch 155| loss: 1.48842 | val_0_accuracy: 0.54286 |  0:00:13s\n",
      "⏱️ Epoch 156 took 0.08 seconds\n",
      "epoch 156| loss: 1.49047 | val_0_accuracy: 0.52976 |  0:00:13s\n",
      "⏱️ Epoch 157 took 0.08 seconds\n",
      "epoch 157| loss: 1.51322 | val_0_accuracy: 0.52143 |  0:00:13s\n",
      "⏱️ Epoch 158 took 0.08 seconds\n",
      "epoch 158| loss: 1.46038 | val_0_accuracy: 0.52738 |  0:00:13s\n",
      "⏱️ Epoch 159 took 0.08 seconds\n",
      "epoch 159| loss: 1.53058 | val_0_accuracy: 0.51071 |  0:00:13s\n",
      "⏱️ Epoch 160 took 0.09 seconds\n",
      "epoch 160| loss: 1.49436 | val_0_accuracy: 0.50238 |  0:00:13s\n",
      "⏱️ Epoch 161 took 0.08 seconds\n",
      "epoch 161| loss: 1.52972 | val_0_accuracy: 0.53095 |  0:00:13s\n",
      "⏱️ Epoch 162 took 0.08 seconds\n",
      "epoch 162| loss: 1.51895 | val_0_accuracy: 0.53214 |  0:00:13s\n",
      "⏱️ Epoch 163 took 0.11 seconds\n",
      "epoch 163| loss: 1.48181 | val_0_accuracy: 0.55238 |  0:00:13s\n",
      "⏱️ Epoch 164 took 0.11 seconds\n",
      "epoch 164| loss: 1.51968 | val_0_accuracy: 0.54286 |  0:00:13s\n",
      "⏱️ Epoch 165 took 0.08 seconds\n",
      "epoch 165| loss: 1.45595 | val_0_accuracy: 0.50714 |  0:00:14s\n",
      "⏱️ Epoch 166 took 0.08 seconds\n",
      "epoch 166| loss: 1.45414 | val_0_accuracy: 0.50714 |  0:00:14s\n",
      "⏱️ Epoch 167 took 0.08 seconds\n",
      "epoch 167| loss: 1.45554 | val_0_accuracy: 0.52143 |  0:00:14s\n",
      "⏱️ Epoch 168 took 0.08 seconds\n",
      "epoch 168| loss: 1.4471  | val_0_accuracy: 0.52024 |  0:00:14s\n",
      "⏱️ Epoch 169 took 0.09 seconds\n",
      "epoch 169| loss: 1.49942 | val_0_accuracy: 0.53452 |  0:00:14s\n",
      "⏱️ Epoch 170 took 0.08 seconds\n",
      "epoch 170| loss: 1.51724 | val_0_accuracy: 0.53333 |  0:00:14s\n",
      "⏱️ Epoch 171 took 0.08 seconds\n",
      "epoch 171| loss: 1.47153 | val_0_accuracy: 0.49881 |  0:00:14s\n",
      "⏱️ Epoch 172 took 0.09 seconds\n",
      "epoch 172| loss: 1.4538  | val_0_accuracy: 0.51905 |  0:00:14s\n",
      "⏱️ Epoch 173 took 0.08 seconds\n",
      "epoch 173| loss: 1.50865 | val_0_accuracy: 0.52024 |  0:00:14s\n",
      "⏱️ Epoch 174 took 0.09 seconds\n",
      "epoch 174| loss: 1.51685 | val_0_accuracy: 0.55952 |  0:00:14s\n",
      "⏱️ Epoch 175 took 0.08 seconds\n",
      "epoch 175| loss: 1.5178  | val_0_accuracy: 0.51071 |  0:00:14s\n",
      "⏱️ Epoch 176 took 0.08 seconds\n",
      "epoch 176| loss: 1.56285 | val_0_accuracy: 0.52857 |  0:00:14s\n",
      "⏱️ Epoch 177 took 0.08 seconds\n",
      "epoch 177| loss: 1.47853 | val_0_accuracy: 0.52619 |  0:00:15s\n",
      "⏱️ Epoch 178 took 0.08 seconds\n",
      "epoch 178| loss: 1.42659 | val_0_accuracy: 0.54405 |  0:00:15s\n",
      "⏱️ Epoch 179 took 0.10 seconds\n",
      "epoch 179| loss: 1.42484 | val_0_accuracy: 0.52738 |  0:00:15s\n",
      "⏱️ Epoch 180 took 0.09 seconds\n",
      "epoch 180| loss: 1.45879 | val_0_accuracy: 0.51071 |  0:00:15s\n",
      "⏱️ Epoch 181 took 0.08 seconds\n",
      "epoch 181| loss: 1.43889 | val_0_accuracy: 0.55    |  0:00:15s\n",
      "⏱️ Epoch 182 took 0.08 seconds\n",
      "epoch 182| loss: 1.40297 | val_0_accuracy: 0.5619  |  0:00:15s\n",
      "⏱️ Epoch 183 took 0.09 seconds\n",
      "epoch 183| loss: 1.56718 | val_0_accuracy: 0.55476 |  0:00:15s\n",
      "⏱️ Epoch 184 took 0.08 seconds\n",
      "epoch 184| loss: 1.41297 | val_0_accuracy: 0.55595 |  0:00:15s\n",
      "⏱️ Epoch 185 took 0.08 seconds\n",
      "epoch 185| loss: 1.44834 | val_0_accuracy: 0.50952 |  0:00:15s\n",
      "⏱️ Epoch 186 took 0.08 seconds\n",
      "epoch 186| loss: 1.45417 | val_0_accuracy: 0.53333 |  0:00:15s\n",
      "⏱️ Epoch 187 took 0.08 seconds\n",
      "epoch 187| loss: 1.43242 | val_0_accuracy: 0.5381  |  0:00:15s\n",
      "⏱️ Epoch 188 took 0.08 seconds\n",
      "epoch 188| loss: 1.46284 | val_0_accuracy: 0.55238 |  0:00:15s\n",
      "⏱️ Epoch 189 took 0.08 seconds\n",
      "epoch 189| loss: 1.40805 | val_0_accuracy: 0.55952 |  0:00:16s\n",
      "⏱️ Epoch 190 took 0.09 seconds\n",
      "epoch 190| loss: 1.40544 | val_0_accuracy: 0.52143 |  0:00:16s\n",
      "⏱️ Epoch 191 took 0.09 seconds\n",
      "epoch 191| loss: 1.41283 | val_0_accuracy: 0.55714 |  0:00:16s\n",
      "⏱️ Epoch 192 took 0.09 seconds\n",
      "epoch 192| loss: 1.39628 | val_0_accuracy: 0.51548 |  0:00:16s\n",
      "⏱️ Epoch 193 took 0.08 seconds\n",
      "epoch 193| loss: 1.42477 | val_0_accuracy: 0.56071 |  0:00:16s\n",
      "⏱️ Epoch 194 took 0.08 seconds\n",
      "epoch 194| loss: 1.39649 | val_0_accuracy: 0.49405 |  0:00:16s\n",
      "⏱️ Epoch 195 took 0.08 seconds\n",
      "epoch 195| loss: 1.41908 | val_0_accuracy: 0.49762 |  0:00:16s\n",
      "⏱️ Epoch 196 took 0.08 seconds\n",
      "epoch 196| loss: 1.38973 | val_0_accuracy: 0.54405 |  0:00:16s\n",
      "⏱️ Epoch 197 took 0.08 seconds\n",
      "epoch 197| loss: 1.36834 | val_0_accuracy: 0.56071 |  0:00:16s\n",
      "⏱️ Epoch 198 took 0.09 seconds\n",
      "epoch 198| loss: 1.3591  | val_0_accuracy: 0.57143 |  0:00:16s\n",
      "⏱️ Epoch 199 took 0.08 seconds\n",
      "epoch 199| loss: 1.37233 | val_0_accuracy: 0.56548 |  0:00:16s\n",
      "⏱️ Epoch 200 took 0.08 seconds\n",
      "epoch 200| loss: 1.46246 | val_0_accuracy: 0.55714 |  0:00:16s\n",
      "⏱️ Epoch 201 took 0.08 seconds\n",
      "epoch 201| loss: 1.37091 | val_0_accuracy: 0.52024 |  0:00:17s\n",
      "⏱️ Epoch 202 took 0.09 seconds\n",
      "epoch 202| loss: 1.42956 | val_0_accuracy: 0.53095 |  0:00:17s\n",
      "⏱️ Epoch 203 took 0.08 seconds\n",
      "epoch 203| loss: 1.4732  | val_0_accuracy: 0.53214 |  0:00:17s\n",
      "⏱️ Epoch 204 took 0.08 seconds\n",
      "epoch 204| loss: 1.37562 | val_0_accuracy: 0.55595 |  0:00:17s\n",
      "⏱️ Epoch 205 took 0.08 seconds\n",
      "epoch 205| loss: 1.41434 | val_0_accuracy: 0.55714 |  0:00:17s\n",
      "⏱️ Epoch 206 took 0.08 seconds\n",
      "epoch 206| loss: 1.36897 | val_0_accuracy: 0.55119 |  0:00:17s\n",
      "⏱️ Epoch 207 took 0.08 seconds\n",
      "epoch 207| loss: 1.37733 | val_0_accuracy: 0.56667 |  0:00:17s\n",
      "⏱️ Epoch 208 took 0.08 seconds\n",
      "epoch 208| loss: 1.38355 | val_0_accuracy: 0.5381  |  0:00:17s\n",
      "⏱️ Epoch 209 took 0.08 seconds\n",
      "epoch 209| loss: 1.37608 | val_0_accuracy: 0.50833 |  0:00:17s\n",
      "⏱️ Epoch 210 took 0.08 seconds\n",
      "epoch 210| loss: 1.36115 | val_0_accuracy: 0.55714 |  0:00:17s\n",
      "⏱️ Epoch 211 took 0.08 seconds\n",
      "epoch 211| loss: 1.40428 | val_0_accuracy: 0.54167 |  0:00:17s\n",
      "⏱️ Epoch 212 took 0.09 seconds\n",
      "epoch 212| loss: 1.36137 | val_0_accuracy: 0.47024 |  0:00:17s\n",
      "⏱️ Epoch 213 took 0.08 seconds\n",
      "epoch 213| loss: 1.35194 | val_0_accuracy: 0.52857 |  0:00:18s\n",
      "⏱️ Epoch 214 took 0.08 seconds\n",
      "epoch 214| loss: 1.32571 | val_0_accuracy: 0.56071 |  0:00:18s\n",
      "⏱️ Epoch 215 took 0.08 seconds\n",
      "epoch 215| loss: 1.35165 | val_0_accuracy: 0.5619  |  0:00:18s\n",
      "⏱️ Epoch 216 took 0.08 seconds\n",
      "epoch 216| loss: 1.37344 | val_0_accuracy: 0.53929 |  0:00:18s\n",
      "⏱️ Epoch 217 took 0.08 seconds\n",
      "epoch 217| loss: 1.40429 | val_0_accuracy: 0.56429 |  0:00:18s\n",
      "⏱️ Epoch 218 took 0.08 seconds\n",
      "epoch 218| loss: 1.37813 | val_0_accuracy: 0.57619 |  0:00:18s\n",
      "⏱️ Epoch 219 took 0.09 seconds\n",
      "epoch 219| loss: 1.35209 | val_0_accuracy: 0.59643 |  0:00:18s\n",
      "⏱️ Epoch 220 took 0.09 seconds\n",
      "epoch 220| loss: 1.34477 | val_0_accuracy: 0.5631  |  0:00:18s\n",
      "⏱️ Epoch 221 took 0.08 seconds\n",
      "epoch 221| loss: 1.34687 | val_0_accuracy: 0.5881  |  0:00:18s\n",
      "⏱️ Epoch 222 took 0.08 seconds\n",
      "epoch 222| loss: 1.31082 | val_0_accuracy: 0.57976 |  0:00:18s\n",
      "⏱️ Epoch 223 took 0.08 seconds\n",
      "epoch 223| loss: 1.31314 | val_0_accuracy: 0.55238 |  0:00:18s\n",
      "⏱️ Epoch 224 took 0.08 seconds\n",
      "epoch 224| loss: 1.34313 | val_0_accuracy: 0.56905 |  0:00:18s\n",
      "⏱️ Epoch 225 took 0.08 seconds\n",
      "epoch 225| loss: 1.34117 | val_0_accuracy: 0.56905 |  0:00:19s\n",
      "⏱️ Epoch 226 took 0.08 seconds\n",
      "epoch 226| loss: 1.3475  | val_0_accuracy: 0.58452 |  0:00:19s\n",
      "⏱️ Epoch 227 took 0.09 seconds\n",
      "epoch 227| loss: 1.41037 | val_0_accuracy: 0.5369  |  0:00:19s\n",
      "⏱️ Epoch 228 took 0.08 seconds\n",
      "epoch 228| loss: 1.34461 | val_0_accuracy: 0.56548 |  0:00:19s\n",
      "⏱️ Epoch 229 took 0.08 seconds\n",
      "epoch 229| loss: 1.31031 | val_0_accuracy: 0.55595 |  0:00:19s\n",
      "⏱️ Epoch 230 took 0.09 seconds\n",
      "epoch 230| loss: 1.37859 | val_0_accuracy: 0.59405 |  0:00:19s\n",
      "⏱️ Epoch 231 took 0.08 seconds\n",
      "epoch 231| loss: 1.38619 | val_0_accuracy: 0.57619 |  0:00:19s\n",
      "⏱️ Epoch 232 took 0.08 seconds\n",
      "epoch 232| loss: 1.31689 | val_0_accuracy: 0.57262 |  0:00:19s\n",
      "⏱️ Epoch 233 took 0.08 seconds\n",
      "epoch 233| loss: 1.27019 | val_0_accuracy: 0.55357 |  0:00:19s\n",
      "⏱️ Epoch 234 took 0.10 seconds\n",
      "epoch 234| loss: 1.28405 | val_0_accuracy: 0.5619  |  0:00:19s\n",
      "⏱️ Epoch 235 took 0.08 seconds\n",
      "epoch 235| loss: 1.30577 | val_0_accuracy: 0.55119 |  0:00:19s\n",
      "⏱️ Epoch 236 took 0.08 seconds\n",
      "epoch 236| loss: 1.40746 | val_0_accuracy: 0.56429 |  0:00:19s\n",
      "⏱️ Epoch 237 took 0.09 seconds\n",
      "epoch 237| loss: 1.2981  | val_0_accuracy: 0.56429 |  0:00:20s\n",
      "⏱️ Epoch 238 took 0.08 seconds\n",
      "epoch 238| loss: 1.346   | val_0_accuracy: 0.55238 |  0:00:20s\n",
      "⏱️ Epoch 239 took 0.08 seconds\n",
      "epoch 239| loss: 1.27666 | val_0_accuracy: 0.55238 |  0:00:20s\n",
      "⏱️ Epoch 240 took 0.08 seconds\n",
      "epoch 240| loss: 1.32044 | val_0_accuracy: 0.56429 |  0:00:20s\n",
      "⏱️ Epoch 241 took 0.08 seconds\n",
      "epoch 241| loss: 1.32947 | val_0_accuracy: 0.58452 |  0:00:20s\n",
      "⏱️ Epoch 242 took 0.08 seconds\n",
      "epoch 242| loss: 1.31675 | val_0_accuracy: 0.55357 |  0:00:20s\n",
      "⏱️ Epoch 243 took 0.08 seconds\n",
      "epoch 243| loss: 1.38303 | val_0_accuracy: 0.57381 |  0:00:20s\n",
      "⏱️ Epoch 244 took 0.08 seconds\n",
      "epoch 244| loss: 1.38748 | val_0_accuracy: 0.55238 |  0:00:20s\n",
      "⏱️ Epoch 245 took 0.08 seconds\n",
      "epoch 245| loss: 1.28673 | val_0_accuracy: 0.58214 |  0:00:20s\n",
      "⏱️ Epoch 246 took 0.08 seconds\n",
      "epoch 246| loss: 1.39617 | val_0_accuracy: 0.54881 |  0:00:20s\n",
      "⏱️ Epoch 247 took 0.08 seconds\n",
      "epoch 247| loss: 1.32596 | val_0_accuracy: 0.57738 |  0:00:20s\n",
      "⏱️ Epoch 248 took 0.08 seconds\n",
      "epoch 248| loss: 1.31944 | val_0_accuracy: 0.58452 |  0:00:20s\n",
      "⏱️ Epoch 249 took 0.08 seconds\n",
      "epoch 249| loss: 1.33619 | val_0_accuracy: 0.53095 |  0:00:20s\n",
      "⏱️ Epoch 250 took 0.08 seconds\n",
      "epoch 250| loss: 1.29643 | val_0_accuracy: 0.5119  |  0:00:21s\n",
      "⏱️ Epoch 251 took 0.08 seconds\n",
      "epoch 251| loss: 1.27276 | val_0_accuracy: 0.52262 |  0:00:21s\n",
      "⏱️ Epoch 252 took 0.08 seconds\n",
      "epoch 252| loss: 1.29881 | val_0_accuracy: 0.51429 |  0:00:21s\n",
      "⏱️ Epoch 253 took 0.09 seconds\n",
      "epoch 253| loss: 1.35927 | val_0_accuracy: 0.49762 |  0:00:21s\n",
      "⏱️ Epoch 254 took 0.08 seconds\n",
      "epoch 254| loss: 1.33958 | val_0_accuracy: 0.59048 |  0:00:21s\n",
      "⏱️ Epoch 255 took 0.08 seconds\n",
      "epoch 255| loss: 1.28619 | val_0_accuracy: 0.54286 |  0:00:21s\n",
      "⏱️ Epoch 256 took 0.08 seconds\n",
      "epoch 256| loss: 1.36032 | val_0_accuracy: 0.56905 |  0:00:21s\n",
      "⏱️ Epoch 257 took 0.08 seconds\n",
      "epoch 257| loss: 1.34585 | val_0_accuracy: 0.57857 |  0:00:21s\n",
      "⏱️ Epoch 258 took 0.21 seconds\n",
      "epoch 258| loss: 1.28982 | val_0_accuracy: 0.51429 |  0:00:21s\n",
      "⏱️ Epoch 259 took 0.08 seconds\n",
      "epoch 259| loss: 1.30449 | val_0_accuracy: 0.55833 |  0:00:21s\n",
      "⏱️ Epoch 260 took 0.08 seconds\n",
      "epoch 260| loss: 1.27924 | val_0_accuracy: 0.54048 |  0:00:22s\n",
      "⏱️ Epoch 261 took 0.08 seconds\n",
      "epoch 261| loss: 1.34066 | val_0_accuracy: 0.53571 |  0:00:22s\n",
      "⏱️ Epoch 262 took 0.08 seconds\n",
      "epoch 262| loss: 1.3481  | val_0_accuracy: 0.57738 |  0:00:22s\n",
      "⏱️ Epoch 263 took 0.08 seconds\n",
      "epoch 263| loss: 1.27856 | val_0_accuracy: 0.57738 |  0:00:22s\n",
      "⏱️ Epoch 264 took 0.08 seconds\n",
      "epoch 264| loss: 1.31327 | val_0_accuracy: 0.55238 |  0:00:22s\n",
      "⏱️ Epoch 265 took 0.08 seconds\n",
      "epoch 265| loss: 1.30855 | val_0_accuracy: 0.54524 |  0:00:22s\n",
      "⏱️ Epoch 266 took 0.08 seconds\n",
      "epoch 266| loss: 1.32543 | val_0_accuracy: 0.54405 |  0:00:22s\n",
      "⏱️ Epoch 267 took 0.08 seconds\n",
      "epoch 267| loss: 1.28293 | val_0_accuracy: 0.56667 |  0:00:22s\n",
      "⏱️ Epoch 268 took 0.08 seconds\n",
      "epoch 268| loss: 1.37916 | val_0_accuracy: 0.58929 |  0:00:22s\n",
      "⏱️ Epoch 269 took 0.08 seconds\n",
      "epoch 269| loss: 1.26362 | val_0_accuracy: 0.58333 |  0:00:22s\n",
      "⏱️ Epoch 270 took 0.08 seconds\n",
      "epoch 270| loss: 1.29769 | val_0_accuracy: 0.57381 |  0:00:22s\n",
      "⏱️ Epoch 271 took 0.08 seconds\n",
      "epoch 271| loss: 1.26611 | val_0_accuracy: 0.54881 |  0:00:22s\n",
      "⏱️ Epoch 272 took 0.08 seconds\n",
      "epoch 272| loss: 1.2807  | val_0_accuracy: 0.53452 |  0:00:23s\n",
      "⏱️ Epoch 273 took 0.08 seconds\n",
      "epoch 273| loss: 1.4161  | val_0_accuracy: 0.52262 |  0:00:23s\n",
      "⏱️ Epoch 274 took 0.08 seconds\n",
      "epoch 274| loss: 1.32943 | val_0_accuracy: 0.50714 |  0:00:23s\n",
      "⏱️ Epoch 275 took 0.08 seconds\n",
      "epoch 275| loss: 1.35074 | val_0_accuracy: 0.51786 |  0:00:23s\n",
      "⏱️ Epoch 276 took 0.08 seconds\n",
      "epoch 276| loss: 1.29608 | val_0_accuracy: 0.54881 |  0:00:23s\n",
      "⏱️ Epoch 277 took 0.08 seconds\n",
      "epoch 277| loss: 1.30329 | val_0_accuracy: 0.59286 |  0:00:23s\n",
      "⏱️ Epoch 278 took 0.08 seconds\n",
      "epoch 278| loss: 1.29763 | val_0_accuracy: 0.56071 |  0:00:23s\n",
      "⏱️ Epoch 279 took 0.09 seconds\n",
      "epoch 279| loss: 1.32406 | val_0_accuracy: 0.57857 |  0:00:23s\n",
      "⏱️ Epoch 280 took 0.08 seconds\n",
      "epoch 280| loss: 1.32071 | val_0_accuracy: 0.55595 |  0:00:23s\n",
      "⏱️ Epoch 281 took 0.08 seconds\n",
      "epoch 281| loss: 1.28069 | val_0_accuracy: 0.52738 |  0:00:23s\n",
      "⏱️ Epoch 282 took 0.09 seconds\n",
      "epoch 282| loss: 1.31625 | val_0_accuracy: 0.54524 |  0:00:23s\n",
      "⏱️ Epoch 283 took 0.08 seconds\n",
      "epoch 283| loss: 1.28732 | val_0_accuracy: 0.55833 |  0:00:23s\n",
      "⏱️ Epoch 284 took 0.08 seconds\n",
      "epoch 284| loss: 1.29423 | val_0_accuracy: 0.56786 |  0:00:23s\n",
      "⏱️ Epoch 285 took 0.08 seconds\n",
      "epoch 285| loss: 1.22649 | val_0_accuracy: 0.59286 |  0:00:24s\n",
      "⏱️ Epoch 286 took 0.08 seconds\n",
      "epoch 286| loss: 1.30042 | val_0_accuracy: 0.6     |  0:00:24s\n",
      "⏱️ Epoch 287 took 0.08 seconds\n",
      "epoch 287| loss: 1.25523 | val_0_accuracy: 0.59524 |  0:00:24s\n",
      "⏱️ Epoch 288 took 0.08 seconds\n",
      "epoch 288| loss: 1.32993 | val_0_accuracy: 0.57381 |  0:00:24s\n",
      "⏱️ Epoch 289 took 0.08 seconds\n",
      "epoch 289| loss: 1.36783 | val_0_accuracy: 0.56905 |  0:00:24s\n",
      "⏱️ Epoch 290 took 0.08 seconds\n",
      "epoch 290| loss: 1.27489 | val_0_accuracy: 0.56429 |  0:00:24s\n",
      "⏱️ Epoch 291 took 0.09 seconds\n",
      "epoch 291| loss: 1.26963 | val_0_accuracy: 0.55714 |  0:00:24s\n",
      "⏱️ Epoch 292 took 0.08 seconds\n",
      "epoch 292| loss: 1.30555 | val_0_accuracy: 0.56667 |  0:00:24s\n",
      "⏱️ Epoch 293 took 0.08 seconds\n",
      "epoch 293| loss: 1.30207 | val_0_accuracy: 0.57738 |  0:00:24s\n",
      "⏱️ Epoch 294 took 0.08 seconds\n",
      "epoch 294| loss: 1.30855 | val_0_accuracy: 0.58214 |  0:00:24s\n",
      "⏱️ Epoch 295 took 0.08 seconds\n",
      "epoch 295| loss: 1.33805 | val_0_accuracy: 0.5369  |  0:00:24s\n",
      "⏱️ Epoch 296 took 0.08 seconds\n",
      "epoch 296| loss: 1.32411 | val_0_accuracy: 0.54048 |  0:00:24s\n",
      "⏱️ Epoch 297 took 0.08 seconds\n",
      "epoch 297| loss: 1.32696 | val_0_accuracy: 0.58571 |  0:00:25s\n",
      "⏱️ Epoch 298 took 0.08 seconds\n",
      "epoch 298| loss: 1.31615 | val_0_accuracy: 0.57381 |  0:00:25s\n",
      "⏱️ Epoch 299 took 0.08 seconds\n",
      "epoch 299| loss: 1.39415 | val_0_accuracy: 0.55714 |  0:00:25s\n",
      "⏱️ Epoch 300 took 0.08 seconds\n",
      "epoch 300| loss: 1.33994 | val_0_accuracy: 0.57381 |  0:00:25s\n",
      "⏱️ Epoch 301 took 0.08 seconds\n",
      "epoch 301| loss: 1.30103 | val_0_accuracy: 0.55    |  0:00:25s\n",
      "⏱️ Epoch 302 took 0.08 seconds\n",
      "epoch 302| loss: 1.38105 | val_0_accuracy: 0.56548 |  0:00:25s\n",
      "⏱️ Epoch 303 took 0.08 seconds\n",
      "epoch 303| loss: 1.3139  | val_0_accuracy: 0.55714 |  0:00:25s\n",
      "⏱️ Epoch 304 took 0.08 seconds\n",
      "epoch 304| loss: 1.30628 | val_0_accuracy: 0.5369  |  0:00:25s\n",
      "⏱️ Epoch 305 took 0.08 seconds\n",
      "epoch 305| loss: 1.33301 | val_0_accuracy: 0.55952 |  0:00:25s\n",
      "⏱️ Epoch 306 took 0.09 seconds\n",
      "epoch 306| loss: 1.28777 | val_0_accuracy: 0.55952 |  0:00:25s\n",
      "⏱️ Epoch 307 took 0.08 seconds\n",
      "epoch 307| loss: 1.3402  | val_0_accuracy: 0.58214 |  0:00:25s\n",
      "⏱️ Epoch 308 took 0.08 seconds\n",
      "epoch 308| loss: 1.27076 | val_0_accuracy: 0.56905 |  0:00:25s\n",
      "⏱️ Epoch 309 took 0.08 seconds\n",
      "epoch 309| loss: 1.28289 | val_0_accuracy: 0.57381 |  0:00:26s\n",
      "⏱️ Epoch 310 took 0.09 seconds\n",
      "epoch 310| loss: 1.25863 | val_0_accuracy: 0.54881 |  0:00:26s\n",
      "⏱️ Epoch 311 took 0.10 seconds\n",
      "epoch 311| loss: 1.31466 | val_0_accuracy: 0.51548 |  0:00:26s\n",
      "⏱️ Epoch 312 took 0.10 seconds\n",
      "epoch 312| loss: 1.25183 | val_0_accuracy: 0.49524 |  0:00:26s\n",
      "⏱️ Epoch 313 took 0.10 seconds\n",
      "epoch 313| loss: 1.25891 | val_0_accuracy: 0.4631  |  0:00:26s\n",
      "⏱️ Epoch 314 took 0.10 seconds\n",
      "epoch 314| loss: 1.2719  | val_0_accuracy: 0.55238 |  0:00:26s\n",
      "⏱️ Epoch 315 took 0.09 seconds\n",
      "epoch 315| loss: 1.26534 | val_0_accuracy: 0.53095 |  0:00:26s\n",
      "⏱️ Epoch 316 took 0.08 seconds\n",
      "epoch 316| loss: 1.22756 | val_0_accuracy: 0.56786 |  0:00:26s\n",
      "⏱️ Epoch 317 took 0.08 seconds\n",
      "epoch 317| loss: 1.27868 | val_0_accuracy: 0.59167 |  0:00:26s\n",
      "⏱️ Epoch 318 took 0.08 seconds\n",
      "epoch 318| loss: 1.27087 | val_0_accuracy: 0.55952 |  0:00:26s\n",
      "⏱️ Epoch 319 took 0.08 seconds\n",
      "epoch 319| loss: 1.25453 | val_0_accuracy: 0.59048 |  0:00:26s\n",
      "⏱️ Epoch 320 took 0.08 seconds\n",
      "epoch 320| loss: 1.30596 | val_0_accuracy: 0.55714 |  0:00:27s\n",
      "⏱️ Epoch 321 took 0.08 seconds\n",
      "epoch 321| loss: 1.23801 | val_0_accuracy: 0.60714 |  0:00:27s\n",
      "⏱️ Epoch 322 took 0.08 seconds\n",
      "epoch 322| loss: 1.29063 | val_0_accuracy: 0.58095 |  0:00:27s\n",
      "⏱️ Epoch 323 took 0.08 seconds\n",
      "epoch 323| loss: 1.35788 | val_0_accuracy: 0.55952 |  0:00:27s\n",
      "⏱️ Epoch 324 took 0.08 seconds\n",
      "epoch 324| loss: 1.25314 | val_0_accuracy: 0.60119 |  0:00:27s\n",
      "⏱️ Epoch 325 took 0.08 seconds\n",
      "epoch 325| loss: 1.2165  | val_0_accuracy: 0.55952 |  0:00:27s\n",
      "⏱️ Epoch 326 took 0.08 seconds\n",
      "epoch 326| loss: 1.21764 | val_0_accuracy: 0.54643 |  0:00:27s\n",
      "⏱️ Epoch 327 took 0.08 seconds\n",
      "epoch 327| loss: 1.26021 | val_0_accuracy: 0.57024 |  0:00:27s\n",
      "⏱️ Epoch 328 took 0.08 seconds\n",
      "epoch 328| loss: 1.21885 | val_0_accuracy: 0.5881  |  0:00:27s\n",
      "⏱️ Epoch 329 took 0.08 seconds\n",
      "epoch 329| loss: 1.21607 | val_0_accuracy: 0.6     |  0:00:27s\n",
      "⏱️ Epoch 330 took 0.08 seconds\n",
      "epoch 330| loss: 1.26703 | val_0_accuracy: 0.59405 |  0:00:27s\n",
      "⏱️ Epoch 331 took 0.08 seconds\n",
      "epoch 331| loss: 1.29205 | val_0_accuracy: 0.58214 |  0:00:27s\n",
      "⏱️ Epoch 332 took 0.08 seconds\n",
      "epoch 332| loss: 1.1997  | val_0_accuracy: 0.60357 |  0:00:28s\n",
      "⏱️ Epoch 333 took 0.08 seconds\n",
      "epoch 333| loss: 1.2005  | val_0_accuracy: 0.60833 |  0:00:28s\n",
      "⏱️ Epoch 334 took 0.08 seconds\n",
      "epoch 334| loss: 1.25777 | val_0_accuracy: 0.59048 |  0:00:28s\n",
      "⏱️ Epoch 335 took 0.08 seconds\n",
      "epoch 335| loss: 1.19219 | val_0_accuracy: 0.55833 |  0:00:28s\n",
      "⏱️ Epoch 336 took 0.08 seconds\n",
      "epoch 336| loss: 1.28817 | val_0_accuracy: 0.53452 |  0:00:28s\n",
      "⏱️ Epoch 337 took 0.08 seconds\n",
      "epoch 337| loss: 1.21411 | val_0_accuracy: 0.57619 |  0:00:28s\n",
      "⏱️ Epoch 338 took 0.08 seconds\n",
      "epoch 338| loss: 1.25111 | val_0_accuracy: 0.60357 |  0:00:28s\n",
      "⏱️ Epoch 339 took 0.08 seconds\n",
      "epoch 339| loss: 1.21655 | val_0_accuracy: 0.58333 |  0:00:28s\n",
      "⏱️ Epoch 340 took 0.08 seconds\n",
      "epoch 340| loss: 1.22078 | val_0_accuracy: 0.59167 |  0:00:28s\n",
      "⏱️ Epoch 341 took 0.08 seconds\n",
      "epoch 341| loss: 1.23253 | val_0_accuracy: 0.52024 |  0:00:28s\n",
      "⏱️ Epoch 342 took 0.08 seconds\n",
      "epoch 342| loss: 1.25153 | val_0_accuracy: 0.50238 |  0:00:28s\n",
      "⏱️ Epoch 343 took 0.08 seconds\n",
      "epoch 343| loss: 1.25665 | val_0_accuracy: 0.52738 |  0:00:28s\n",
      "⏱️ Epoch 344 took 0.08 seconds\n",
      "epoch 344| loss: 1.24167 | val_0_accuracy: 0.59048 |  0:00:28s\n",
      "⏱️ Epoch 345 took 0.08 seconds\n",
      "epoch 345| loss: 1.17829 | val_0_accuracy: 0.60833 |  0:00:29s\n",
      "⏱️ Epoch 346 took 0.08 seconds\n",
      "epoch 346| loss: 1.22984 | val_0_accuracy: 0.59762 |  0:00:29s\n",
      "⏱️ Epoch 347 took 0.08 seconds\n",
      "epoch 347| loss: 1.17255 | val_0_accuracy: 0.5619  |  0:00:29s\n",
      "⏱️ Epoch 348 took 0.09 seconds\n",
      "epoch 348| loss: 1.27658 | val_0_accuracy: 0.59762 |  0:00:29s\n",
      "⏱️ Epoch 349 took 0.08 seconds\n",
      "epoch 349| loss: 1.18722 | val_0_accuracy: 0.57381 |  0:00:29s\n",
      "⏱️ Epoch 350 took 0.08 seconds\n",
      "epoch 350| loss: 1.34083 | val_0_accuracy: 0.56548 |  0:00:29s\n",
      "⏱️ Epoch 351 took 0.08 seconds\n",
      "epoch 351| loss: 1.17914 | val_0_accuracy: 0.56071 |  0:00:29s\n",
      "⏱️ Epoch 352 took 0.08 seconds\n",
      "epoch 352| loss: 1.1909  | val_0_accuracy: 0.6     |  0:00:29s\n",
      "⏱️ Epoch 353 took 0.08 seconds\n",
      "epoch 353| loss: 1.16625 | val_0_accuracy: 0.59762 |  0:00:29s\n",
      "⏱️ Epoch 354 took 0.08 seconds\n",
      "epoch 354| loss: 1.18222 | val_0_accuracy: 0.55357 |  0:00:29s\n",
      "⏱️ Epoch 355 took 0.08 seconds\n",
      "epoch 355| loss: 1.29071 | val_0_accuracy: 0.58333 |  0:00:29s\n",
      "⏱️ Epoch 356 took 0.08 seconds\n",
      "epoch 356| loss: 1.15868 | val_0_accuracy: 0.5881  |  0:00:29s\n",
      "⏱️ Epoch 357 took 0.08 seconds\n",
      "epoch 357| loss: 1.16609 | val_0_accuracy: 0.5881  |  0:00:30s\n",
      "⏱️ Epoch 358 took 0.08 seconds\n",
      "epoch 358| loss: 1.17241 | val_0_accuracy: 0.58929 |  0:00:30s\n",
      "⏱️ Epoch 359 took 0.09 seconds\n",
      "epoch 359| loss: 1.18645 | val_0_accuracy: 0.60476 |  0:00:30s\n",
      "⏱️ Epoch 360 took 0.08 seconds\n",
      "epoch 360| loss: 1.19883 | val_0_accuracy: 0.5381  |  0:00:30s\n",
      "⏱️ Epoch 361 took 0.08 seconds\n",
      "epoch 361| loss: 1.19761 | val_0_accuracy: 0.51667 |  0:00:30s\n",
      "⏱️ Epoch 362 took 0.08 seconds\n",
      "epoch 362| loss: 1.24905 | val_0_accuracy: 0.55119 |  0:00:30s\n",
      "⏱️ Epoch 363 took 0.08 seconds\n",
      "epoch 363| loss: 1.29514 | val_0_accuracy: 0.61429 |  0:00:30s\n",
      "⏱️ Epoch 364 took 0.09 seconds\n",
      "epoch 364| loss: 1.21739 | val_0_accuracy: 0.59286 |  0:00:30s\n",
      "⏱️ Epoch 365 took 0.08 seconds\n",
      "epoch 365| loss: 1.19847 | val_0_accuracy: 0.59643 |  0:00:30s\n",
      "⏱️ Epoch 366 took 0.09 seconds\n",
      "epoch 366| loss: 1.18396 | val_0_accuracy: 0.57143 |  0:00:30s\n",
      "⏱️ Epoch 367 took 0.08 seconds\n",
      "epoch 367| loss: 1.22078 | val_0_accuracy: 0.60119 |  0:00:30s\n",
      "⏱️ Epoch 368 took 0.08 seconds\n",
      "epoch 368| loss: 1.14067 | val_0_accuracy: 0.61429 |  0:00:30s\n",
      "⏱️ Epoch 369 took 0.08 seconds\n",
      "epoch 369| loss: 1.17333 | val_0_accuracy: 0.60357 |  0:00:31s\n",
      "⏱️ Epoch 370 took 0.08 seconds\n",
      "epoch 370| loss: 1.21736 | val_0_accuracy: 0.57143 |  0:00:31s\n",
      "⏱️ Epoch 371 took 0.09 seconds\n",
      "epoch 371| loss: 1.17118 | val_0_accuracy: 0.54286 |  0:00:31s\n",
      "⏱️ Epoch 372 took 0.08 seconds\n",
      "epoch 372| loss: 1.1543  | val_0_accuracy: 0.52262 |  0:00:31s\n",
      "⏱️ Epoch 373 took 0.08 seconds\n",
      "epoch 373| loss: 1.3025  | val_0_accuracy: 0.56548 |  0:00:31s\n",
      "⏱️ Epoch 374 took 0.08 seconds\n",
      "epoch 374| loss: 1.21326 | val_0_accuracy: 0.55595 |  0:00:31s\n",
      "⏱️ Epoch 375 took 0.08 seconds\n",
      "epoch 375| loss: 1.19546 | val_0_accuracy: 0.59405 |  0:00:31s\n",
      "⏱️ Epoch 376 took 0.08 seconds\n",
      "epoch 376| loss: 1.18508 | val_0_accuracy: 0.58571 |  0:00:31s\n",
      "⏱️ Epoch 377 took 0.08 seconds\n",
      "epoch 377| loss: 1.18464 | val_0_accuracy: 0.60357 |  0:00:31s\n",
      "⏱️ Epoch 378 took 0.08 seconds\n",
      "epoch 378| loss: 1.14973 | val_0_accuracy: 0.59881 |  0:00:31s\n",
      "⏱️ Epoch 379 took 0.08 seconds\n",
      "epoch 379| loss: 1.21755 | val_0_accuracy: 0.5869  |  0:00:31s\n",
      "⏱️ Epoch 380 took 0.08 seconds\n",
      "epoch 380| loss: 1.1937  | val_0_accuracy: 0.54405 |  0:00:31s\n",
      "⏱️ Epoch 381 took 0.08 seconds\n",
      "epoch 381| loss: 1.21505 | val_0_accuracy: 0.575   |  0:00:32s\n",
      "⏱️ Epoch 382 took 0.08 seconds\n",
      "epoch 382| loss: 1.16815 | val_0_accuracy: 0.59048 |  0:00:32s\n",
      "⏱️ Epoch 383 took 0.08 seconds\n",
      "epoch 383| loss: 1.22356 | val_0_accuracy: 0.57262 |  0:00:32s\n",
      "⏱️ Epoch 384 took 0.08 seconds\n",
      "epoch 384| loss: 1.17584 | val_0_accuracy: 0.5881  |  0:00:32s\n",
      "⏱️ Epoch 385 took 0.08 seconds\n",
      "epoch 385| loss: 1.36288 | val_0_accuracy: 0.59048 |  0:00:32s\n",
      "⏱️ Epoch 386 took 0.08 seconds\n",
      "epoch 386| loss: 1.19453 | val_0_accuracy: 0.57262 |  0:00:32s\n",
      "⏱️ Epoch 387 took 0.08 seconds\n",
      "epoch 387| loss: 1.18574 | val_0_accuracy: 0.60238 |  0:00:32s\n",
      "⏱️ Epoch 388 took 0.08 seconds\n",
      "epoch 388| loss: 1.16107 | val_0_accuracy: 0.57381 |  0:00:32s\n",
      "⏱️ Epoch 389 took 0.08 seconds\n",
      "epoch 389| loss: 1.12958 | val_0_accuracy: 0.5869  |  0:00:32s\n",
      "⏱️ Epoch 390 took 0.08 seconds\n",
      "epoch 390| loss: 1.12193 | val_0_accuracy: 0.59048 |  0:00:32s\n",
      "⏱️ Epoch 391 took 0.08 seconds\n",
      "epoch 391| loss: 1.14693 | val_0_accuracy: 0.55119 |  0:00:32s\n",
      "⏱️ Epoch 392 took 0.08 seconds\n",
      "epoch 392| loss: 1.12526 | val_0_accuracy: 0.57619 |  0:00:32s\n",
      "⏱️ Epoch 393 took 0.08 seconds\n",
      "epoch 393| loss: 1.12983 | val_0_accuracy: 0.60595 |  0:00:32s\n",
      "⏱️ Epoch 394 took 0.09 seconds\n",
      "epoch 394| loss: 1.1181  | val_0_accuracy: 0.58214 |  0:00:33s\n",
      "⏱️ Epoch 395 took 0.08 seconds\n",
      "epoch 395| loss: 1.14818 | val_0_accuracy: 0.59405 |  0:00:33s\n",
      "⏱️ Epoch 396 took 0.08 seconds\n",
      "epoch 396| loss: 1.18844 | val_0_accuracy: 0.6131  |  0:00:33s\n",
      "⏱️ Epoch 397 took 0.08 seconds\n",
      "epoch 397| loss: 1.12248 | val_0_accuracy: 0.60476 |  0:00:33s\n",
      "⏱️ Epoch 398 took 0.08 seconds\n",
      "epoch 398| loss: 1.10888 | val_0_accuracy: 0.61071 |  0:00:33s\n",
      "⏱️ Epoch 399 took 0.08 seconds\n",
      "epoch 399| loss: 1.15437 | val_0_accuracy: 0.60357 |  0:00:33s\n",
      "⏱️ Epoch 400 took 0.08 seconds\n",
      "epoch 400| loss: 1.1474  | val_0_accuracy: 0.54643 |  0:00:33s\n",
      "⏱️ Epoch 401 took 0.08 seconds\n",
      "epoch 401| loss: 1.18544 | val_0_accuracy: 0.57143 |  0:00:33s\n",
      "⏱️ Epoch 402 took 0.08 seconds\n",
      "epoch 402| loss: 1.18975 | val_0_accuracy: 0.57619 |  0:00:33s\n",
      "⏱️ Epoch 403 took 0.08 seconds\n",
      "epoch 403| loss: 1.14903 | val_0_accuracy: 0.5381  |  0:00:33s\n",
      "⏱️ Epoch 404 took 0.09 seconds\n",
      "epoch 404| loss: 1.13792 | val_0_accuracy: 0.54762 |  0:00:33s\n",
      "⏱️ Epoch 405 took 0.09 seconds\n",
      "epoch 405| loss: 1.14209 | val_0_accuracy: 0.5869  |  0:00:33s\n",
      "⏱️ Epoch 406 took 0.08 seconds\n",
      "epoch 406| loss: 1.11189 | val_0_accuracy: 0.60833 |  0:00:34s\n",
      "⏱️ Epoch 407 took 0.08 seconds\n",
      "epoch 407| loss: 1.13805 | val_0_accuracy: 0.59524 |  0:00:34s\n",
      "⏱️ Epoch 408 took 0.08 seconds\n",
      "epoch 408| loss: 1.13992 | val_0_accuracy: 0.60476 |  0:00:34s\n",
      "⏱️ Epoch 409 took 0.08 seconds\n",
      "epoch 409| loss: 1.16954 | val_0_accuracy: 0.57738 |  0:00:34s\n",
      "⏱️ Epoch 410 took 0.08 seconds\n",
      "epoch 410| loss: 1.16738 | val_0_accuracy: 0.60952 |  0:00:34s\n",
      "⏱️ Epoch 411 took 0.08 seconds\n",
      "epoch 411| loss: 1.14295 | val_0_accuracy: 0.61786 |  0:00:34s\n",
      "⏱️ Epoch 412 took 0.08 seconds\n",
      "epoch 412| loss: 1.19331 | val_0_accuracy: 0.56429 |  0:00:34s\n",
      "⏱️ Epoch 413 took 0.08 seconds\n",
      "epoch 413| loss: 1.18805 | val_0_accuracy: 0.5869  |  0:00:34s\n",
      "⏱️ Epoch 414 took 0.08 seconds\n",
      "epoch 414| loss: 1.16926 | val_0_accuracy: 0.5881  |  0:00:34s\n",
      "⏱️ Epoch 415 took 0.08 seconds\n",
      "epoch 415| loss: 1.14251 | val_0_accuracy: 0.58571 |  0:00:34s\n",
      "⏱️ Epoch 416 took 0.08 seconds\n",
      "epoch 416| loss: 1.17634 | val_0_accuracy: 0.61667 |  0:00:34s\n",
      "⏱️ Epoch 417 took 0.09 seconds\n",
      "epoch 417| loss: 1.16042 | val_0_accuracy: 0.58333 |  0:00:34s\n",
      "⏱️ Epoch 418 took 0.08 seconds\n",
      "epoch 418| loss: 1.18785 | val_0_accuracy: 0.5881  |  0:00:35s\n",
      "⏱️ Epoch 419 took 0.08 seconds\n",
      "epoch 419| loss: 1.08231 | val_0_accuracy: 0.61667 |  0:00:35s\n",
      "⏱️ Epoch 420 took 0.09 seconds\n",
      "epoch 420| loss: 1.10238 | val_0_accuracy: 0.57143 |  0:00:35s\n",
      "⏱️ Epoch 421 took 0.09 seconds\n",
      "epoch 421| loss: 1.08685 | val_0_accuracy: 0.6119  |  0:00:35s\n",
      "⏱️ Epoch 422 took 0.08 seconds\n",
      "epoch 422| loss: 1.1005  | val_0_accuracy: 0.59762 |  0:00:35s\n",
      "⏱️ Epoch 423 took 0.08 seconds\n",
      "epoch 423| loss: 1.1197  | val_0_accuracy: 0.60833 |  0:00:35s\n",
      "⏱️ Epoch 424 took 0.08 seconds\n",
      "epoch 424| loss: 1.10482 | val_0_accuracy: 0.60952 |  0:00:35s\n",
      "⏱️ Epoch 425 took 0.08 seconds\n",
      "epoch 425| loss: 1.10696 | val_0_accuracy: 0.575   |  0:00:35s\n",
      "⏱️ Epoch 426 took 0.08 seconds\n",
      "epoch 426| loss: 1.19035 | val_0_accuracy: 0.57857 |  0:00:35s\n",
      "⏱️ Epoch 427 took 0.09 seconds\n",
      "epoch 427| loss: 1.10087 | val_0_accuracy: 0.60833 |  0:00:35s\n",
      "⏱️ Epoch 428 took 0.08 seconds\n",
      "epoch 428| loss: 1.13096 | val_0_accuracy: 0.60476 |  0:00:35s\n",
      "⏱️ Epoch 429 took 0.08 seconds\n",
      "epoch 429| loss: 1.10795 | val_0_accuracy: 0.59881 |  0:00:35s\n",
      "⏱️ Epoch 430 took 0.08 seconds\n",
      "epoch 430| loss: 1.12125 | val_0_accuracy: 0.58095 |  0:00:36s\n",
      "⏱️ Epoch 431 took 0.08 seconds\n",
      "epoch 431| loss: 1.15084 | val_0_accuracy: 0.59881 |  0:00:36s\n",
      "⏱️ Epoch 432 took 0.08 seconds\n",
      "epoch 432| loss: 1.14247 | val_0_accuracy: 0.6     |  0:00:36s\n",
      "⏱️ Epoch 433 took 0.08 seconds\n",
      "epoch 433| loss: 1.13651 | val_0_accuracy: 0.60238 |  0:00:36s\n",
      "⏱️ Epoch 434 took 0.08 seconds\n",
      "epoch 434| loss: 1.09828 | val_0_accuracy: 0.5869  |  0:00:36s\n",
      "⏱️ Epoch 435 took 0.08 seconds\n",
      "epoch 435| loss: 1.18024 | val_0_accuracy: 0.5381  |  0:00:36s\n",
      "⏱️ Epoch 436 took 0.08 seconds\n",
      "epoch 436| loss: 1.11001 | val_0_accuracy: 0.55952 |  0:00:36s\n",
      "⏱️ Epoch 437 took 0.08 seconds\n",
      "epoch 437| loss: 1.22317 | val_0_accuracy: 0.55357 |  0:00:36s\n",
      "⏱️ Epoch 438 took 0.08 seconds\n",
      "epoch 438| loss: 1.10751 | val_0_accuracy: 0.59405 |  0:00:36s\n",
      "⏱️ Epoch 439 took 0.19 seconds\n",
      "epoch 439| loss: 1.14008 | val_0_accuracy: 0.63095 |  0:00:36s\n",
      "⏱️ Epoch 440 took 0.10 seconds\n",
      "epoch 440| loss: 1.11406 | val_0_accuracy: 0.57976 |  0:00:36s\n",
      "⏱️ Epoch 441 took 0.08 seconds\n",
      "epoch 441| loss: 1.09215 | val_0_accuracy: 0.59048 |  0:00:37s\n",
      "⏱️ Epoch 442 took 0.08 seconds\n",
      "epoch 442| loss: 1.07619 | val_0_accuracy: 0.59643 |  0:00:37s\n",
      "⏱️ Epoch 443 took 0.08 seconds\n",
      "epoch 443| loss: 1.03306 | val_0_accuracy: 0.59643 |  0:00:37s\n",
      "⏱️ Epoch 444 took 0.08 seconds\n",
      "epoch 444| loss: 1.08903 | val_0_accuracy: 0.59643 |  0:00:37s\n",
      "⏱️ Epoch 445 took 0.09 seconds\n",
      "epoch 445| loss: 1.05532 | val_0_accuracy: 0.57857 |  0:00:37s\n",
      "⏱️ Epoch 446 took 0.10 seconds\n",
      "epoch 446| loss: 1.06417 | val_0_accuracy: 0.56786 |  0:00:37s\n",
      "⏱️ Epoch 447 took 0.10 seconds\n",
      "epoch 447| loss: 1.10682 | val_0_accuracy: 0.57024 |  0:00:37s\n",
      "⏱️ Epoch 448 took 0.09 seconds\n",
      "epoch 448| loss: 1.11747 | val_0_accuracy: 0.59405 |  0:00:37s\n",
      "⏱️ Epoch 449 took 0.10 seconds\n",
      "epoch 449| loss: 1.06769 | val_0_accuracy: 0.56429 |  0:00:37s\n",
      "⏱️ Epoch 450 took 0.08 seconds\n",
      "epoch 450| loss: 1.09565 | val_0_accuracy: 0.58095 |  0:00:37s\n",
      "⏱️ Epoch 451 took 0.08 seconds\n",
      "epoch 451| loss: 1.07708 | val_0_accuracy: 0.56548 |  0:00:37s\n",
      "⏱️ Epoch 452 took 0.08 seconds\n",
      "epoch 452| loss: 1.10993 | val_0_accuracy: 0.61905 |  0:00:38s\n",
      "⏱️ Epoch 453 took 0.08 seconds\n",
      "epoch 453| loss: 1.15898 | val_0_accuracy: 0.60595 |  0:00:38s\n",
      "⏱️ Epoch 454 took 0.08 seconds\n",
      "epoch 454| loss: 1.19932 | val_0_accuracy: 0.57976 |  0:00:38s\n",
      "⏱️ Epoch 455 took 0.09 seconds\n",
      "epoch 455| loss: 1.0839  | val_0_accuracy: 0.61667 |  0:00:38s\n",
      "⏱️ Epoch 456 took 0.08 seconds\n",
      "epoch 456| loss: 1.07527 | val_0_accuracy: 0.54286 |  0:00:38s\n",
      "⏱️ Epoch 457 took 0.08 seconds\n",
      "epoch 457| loss: 1.06287 | val_0_accuracy: 0.51429 |  0:00:38s\n",
      "⏱️ Epoch 458 took 0.08 seconds\n",
      "epoch 458| loss: 1.10584 | val_0_accuracy: 0.54167 |  0:00:38s\n",
      "⏱️ Epoch 459 took 0.08 seconds\n",
      "epoch 459| loss: 1.13374 | val_0_accuracy: 0.58571 |  0:00:38s\n",
      "⏱️ Epoch 460 took 0.08 seconds\n",
      "epoch 460| loss: 1.073   | val_0_accuracy: 0.575   |  0:00:38s\n",
      "⏱️ Epoch 461 took 0.09 seconds\n",
      "epoch 461| loss: 1.08671 | val_0_accuracy: 0.6     |  0:00:38s\n",
      "⏱️ Epoch 462 took 0.08 seconds\n",
      "epoch 462| loss: 1.08811 | val_0_accuracy: 0.62619 |  0:00:38s\n",
      "⏱️ Epoch 463 took 0.08 seconds\n",
      "epoch 463| loss: 1.06176 | val_0_accuracy: 0.59881 |  0:00:38s\n",
      "⏱️ Epoch 464 took 0.08 seconds\n",
      "epoch 464| loss: 1.08622 | val_0_accuracy: 0.6119  |  0:00:39s\n",
      "⏱️ Epoch 465 took 0.08 seconds\n",
      "epoch 465| loss: 1.06843 | val_0_accuracy: 0.62024 |  0:00:39s\n",
      "⏱️ Epoch 466 took 0.08 seconds\n",
      "epoch 466| loss: 1.06053 | val_0_accuracy: 0.62619 |  0:00:39s\n",
      "⏱️ Epoch 467 took 0.08 seconds\n",
      "epoch 467| loss: 1.18062 | val_0_accuracy: 0.60238 |  0:00:39s\n",
      "⏱️ Epoch 468 took 0.08 seconds\n",
      "epoch 468| loss: 1.04602 | val_0_accuracy: 0.62024 |  0:00:39s\n",
      "⏱️ Epoch 469 took 0.08 seconds\n",
      "epoch 469| loss: 1.0713  | val_0_accuracy: 0.60595 |  0:00:39s\n",
      "⏱️ Epoch 470 took 0.09 seconds\n",
      "epoch 470| loss: 1.07431 | val_0_accuracy: 0.61548 |  0:00:39s\n",
      "⏱️ Epoch 471 took 0.08 seconds\n",
      "epoch 471| loss: 1.04443 | val_0_accuracy: 0.57738 |  0:00:39s\n",
      "⏱️ Epoch 472 took 0.08 seconds\n",
      "epoch 472| loss: 1.06012 | val_0_accuracy: 0.60238 |  0:00:39s\n",
      "⏱️ Epoch 473 took 0.08 seconds\n",
      "epoch 473| loss: 1.08279 | val_0_accuracy: 0.58095 |  0:00:39s\n",
      "⏱️ Epoch 474 took 0.08 seconds\n",
      "epoch 474| loss: 1.13767 | val_0_accuracy: 0.56071 |  0:00:39s\n",
      "⏱️ Epoch 475 took 0.08 seconds\n",
      "epoch 475| loss: 1.14742 | val_0_accuracy: 0.52619 |  0:00:39s\n",
      "⏱️ Epoch 476 took 0.08 seconds\n",
      "epoch 476| loss: 1.11257 | val_0_accuracy: 0.59286 |  0:00:40s\n",
      "⏱️ Epoch 477 took 0.08 seconds\n",
      "epoch 477| loss: 1.14288 | val_0_accuracy: 0.58333 |  0:00:40s\n",
      "⏱️ Epoch 478 took 0.08 seconds\n",
      "epoch 478| loss: 1.05691 | val_0_accuracy: 0.60833 |  0:00:40s\n",
      "⏱️ Epoch 479 took 0.08 seconds\n",
      "epoch 479| loss: 1.09705 | val_0_accuracy: 0.54881 |  0:00:40s\n",
      "⏱️ Epoch 480 took 0.08 seconds\n",
      "epoch 480| loss: 1.11091 | val_0_accuracy: 0.57619 |  0:00:40s\n",
      "⏱️ Epoch 481 took 0.08 seconds\n",
      "epoch 481| loss: 1.07743 | val_0_accuracy: 0.62262 |  0:00:40s\n",
      "⏱️ Epoch 482 took 0.08 seconds\n",
      "epoch 482| loss: 1.11477 | val_0_accuracy: 0.60595 |  0:00:40s\n",
      "⏱️ Epoch 483 took 0.08 seconds\n",
      "epoch 483| loss: 1.06254 | val_0_accuracy: 0.60357 |  0:00:40s\n",
      "⏱️ Epoch 484 took 0.08 seconds\n",
      "epoch 484| loss: 1.08752 | val_0_accuracy: 0.62024 |  0:00:40s\n",
      "⏱️ Epoch 485 took 0.08 seconds\n",
      "epoch 485| loss: 1.08144 | val_0_accuracy: 0.60357 |  0:00:40s\n",
      "⏱️ Epoch 486 took 0.08 seconds\n",
      "epoch 486| loss: 1.06689 | val_0_accuracy: 0.62143 |  0:00:40s\n",
      "⏱️ Epoch 487 took 0.08 seconds\n",
      "epoch 487| loss: 1.10113 | val_0_accuracy: 0.59167 |  0:00:40s\n",
      "⏱️ Epoch 488 took 0.08 seconds\n",
      "epoch 488| loss: 1.14542 | val_0_accuracy: 0.5869  |  0:00:40s\n",
      "⏱️ Epoch 489 took 0.08 seconds\n",
      "epoch 489| loss: 1.1433  | val_0_accuracy: 0.56429 |  0:00:41s\n",
      "⏱️ Epoch 490 took 0.08 seconds\n",
      "epoch 490| loss: 1.08167 | val_0_accuracy: 0.55952 |  0:00:41s\n",
      "⏱️ Epoch 491 took 0.08 seconds\n",
      "epoch 491| loss: 1.08743 | val_0_accuracy: 0.61667 |  0:00:41s\n",
      "⏱️ Epoch 492 took 0.08 seconds\n",
      "epoch 492| loss: 1.06505 | val_0_accuracy: 0.61071 |  0:00:41s\n",
      "⏱️ Epoch 493 took 0.09 seconds\n",
      "epoch 493| loss: 1.15763 | val_0_accuracy: 0.56905 |  0:00:41s\n",
      "⏱️ Epoch 494 took 0.08 seconds\n",
      "epoch 494| loss: 1.0781  | val_0_accuracy: 0.61667 |  0:00:41s\n",
      "⏱️ Epoch 495 took 0.08 seconds\n",
      "epoch 495| loss: 1.13873 | val_0_accuracy: 0.58571 |  0:00:41s\n",
      "⏱️ Epoch 496 took 0.09 seconds\n",
      "epoch 496| loss: 1.04506 | val_0_accuracy: 0.57857 |  0:00:41s\n",
      "⏱️ Epoch 497 took 0.09 seconds\n",
      "epoch 497| loss: 1.02384 | val_0_accuracy: 0.61667 |  0:00:41s\n",
      "⏱️ Epoch 498 took 0.09 seconds\n",
      "epoch 498| loss: 1.0135  | val_0_accuracy: 0.57976 |  0:00:41s\n",
      "⏱️ Epoch 499 took 0.08 seconds\n",
      "epoch 499| loss: 1.05108 | val_0_accuracy: 0.58452 |  0:00:41s\n",
      "⏱️ Epoch 500 took 0.09 seconds\n",
      "epoch 500| loss: 1.07246 | val_0_accuracy: 0.58333 |  0:00:41s\n",
      "⏱️ Epoch 501 took 0.08 seconds\n",
      "epoch 501| loss: 1.09434 | val_0_accuracy: 0.55238 |  0:00:42s\n",
      "⏱️ Epoch 502 took 0.08 seconds\n",
      "epoch 502| loss: 1.1468  | val_0_accuracy: 0.55357 |  0:00:42s\n",
      "⏱️ Epoch 503 took 0.09 seconds\n",
      "epoch 503| loss: 1.07465 | val_0_accuracy: 0.5869  |  0:00:42s\n",
      "⏱️ Epoch 504 took 0.08 seconds\n",
      "epoch 504| loss: 1.06321 | val_0_accuracy: 0.60119 |  0:00:42s\n",
      "⏱️ Epoch 505 took 0.09 seconds\n",
      "epoch 505| loss: 1.03329 | val_0_accuracy: 0.61429 |  0:00:42s\n",
      "⏱️ Epoch 506 took 0.08 seconds\n",
      "epoch 506| loss: 1.04141 | val_0_accuracy: 0.61667 |  0:00:42s\n",
      "⏱️ Epoch 507 took 0.08 seconds\n",
      "epoch 507| loss: 1.01812 | val_0_accuracy: 0.55714 |  0:00:42s\n",
      "⏱️ Epoch 508 took 0.08 seconds\n",
      "epoch 508| loss: 1.02032 | val_0_accuracy: 0.61071 |  0:00:42s\n",
      "⏱️ Epoch 509 took 0.09 seconds\n",
      "epoch 509| loss: 1.04417 | val_0_accuracy: 0.55476 |  0:00:42s\n",
      "⏱️ Epoch 510 took 0.08 seconds\n",
      "epoch 510| loss: 1.09986 | val_0_accuracy: 0.52619 |  0:00:42s\n",
      "⏱️ Epoch 511 took 0.08 seconds\n",
      "epoch 511| loss: 1.1468  | val_0_accuracy: 0.59524 |  0:00:42s\n",
      "⏱️ Epoch 512 took 0.08 seconds\n",
      "epoch 512| loss: 1.0964  | val_0_accuracy: 0.59881 |  0:00:42s\n",
      "⏱️ Epoch 513 took 0.08 seconds\n",
      "epoch 513| loss: 1.12323 | val_0_accuracy: 0.58452 |  0:00:43s\n",
      "⏱️ Epoch 514 took 0.09 seconds\n",
      "epoch 514| loss: 1.04625 | val_0_accuracy: 0.60595 |  0:00:43s\n",
      "⏱️ Epoch 515 took 0.10 seconds\n",
      "epoch 515| loss: 1.08137 | val_0_accuracy: 0.60833 |  0:00:43s\n",
      "⏱️ Epoch 516 took 0.08 seconds\n",
      "epoch 516| loss: 1.07097 | val_0_accuracy: 0.61071 |  0:00:43s\n",
      "⏱️ Epoch 517 took 0.09 seconds\n",
      "epoch 517| loss: 1.03547 | val_0_accuracy: 0.63095 |  0:00:43s\n",
      "⏱️ Epoch 518 took 0.09 seconds\n",
      "epoch 518| loss: 1.00582 | val_0_accuracy: 0.63333 |  0:00:43s\n",
      "⏱️ Epoch 519 took 0.09 seconds\n",
      "epoch 519| loss: 1.0782  | val_0_accuracy: 0.59167 |  0:00:43s\n",
      "⏱️ Epoch 520 took 0.08 seconds\n",
      "epoch 520| loss: 1.10396 | val_0_accuracy: 0.51429 |  0:00:43s\n",
      "⏱️ Epoch 521 took 0.08 seconds\n",
      "epoch 521| loss: 1.12149 | val_0_accuracy: 0.57976 |  0:00:43s\n",
      "⏱️ Epoch 522 took 0.08 seconds\n",
      "epoch 522| loss: 1.02312 | val_0_accuracy: 0.59048 |  0:00:43s\n",
      "⏱️ Epoch 523 took 0.08 seconds\n",
      "epoch 523| loss: 1.0503  | val_0_accuracy: 0.57381 |  0:00:43s\n",
      "⏱️ Epoch 524 took 0.10 seconds\n",
      "epoch 524| loss: 1.03242 | val_0_accuracy: 0.61786 |  0:00:44s\n",
      "⏱️ Epoch 525 took 0.08 seconds\n",
      "epoch 525| loss: 1.05326 | val_0_accuracy: 0.60119 |  0:00:44s\n",
      "⏱️ Epoch 526 took 0.09 seconds\n",
      "epoch 526| loss: 1.02666 | val_0_accuracy: 0.61429 |  0:00:44s\n",
      "⏱️ Epoch 527 took 0.09 seconds\n",
      "epoch 527| loss: 1.00867 | val_0_accuracy: 0.59881 |  0:00:44s\n",
      "⏱️ Epoch 528 took 0.08 seconds\n",
      "epoch 528| loss: 1.01797 | val_0_accuracy: 0.6     |  0:00:44s\n",
      "⏱️ Epoch 529 took 0.08 seconds\n",
      "epoch 529| loss: 1.04952 | val_0_accuracy: 0.57262 |  0:00:44s\n",
      "⏱️ Epoch 530 took 0.08 seconds\n",
      "epoch 530| loss: 1.00494 | val_0_accuracy: 0.58095 |  0:00:44s\n",
      "⏱️ Epoch 531 took 0.09 seconds\n",
      "epoch 531| loss: 1.04745 | val_0_accuracy: 0.58095 |  0:00:44s\n",
      "⏱️ Epoch 532 took 0.09 seconds\n",
      "epoch 532| loss: 1.11072 | val_0_accuracy: 0.60357 |  0:00:44s\n",
      "⏱️ Epoch 533 took 0.09 seconds\n",
      "epoch 533| loss: 1.07894 | val_0_accuracy: 0.58452 |  0:00:44s\n",
      "⏱️ Epoch 534 took 0.08 seconds\n",
      "epoch 534| loss: 1.10031 | val_0_accuracy: 0.60595 |  0:00:44s\n",
      "⏱️ Epoch 535 took 0.08 seconds\n",
      "epoch 535| loss: 1.06348 | val_0_accuracy: 0.59881 |  0:00:44s\n",
      "⏱️ Epoch 536 took 0.09 seconds\n",
      "epoch 536| loss: 1.06148 | val_0_accuracy: 0.575   |  0:00:45s\n",
      "⏱️ Epoch 537 took 0.08 seconds\n",
      "epoch 537| loss: 1.02556 | val_0_accuracy: 0.60357 |  0:00:45s\n",
      "⏱️ Epoch 538 took 0.08 seconds\n",
      "epoch 538| loss: 1.04106 | val_0_accuracy: 0.58333 |  0:00:45s\n",
      "⏱️ Epoch 539 took 0.08 seconds\n",
      "epoch 539| loss: 1.06085 | val_0_accuracy: 0.61905 |  0:00:45s\n",
      "⏱️ Epoch 540 took 0.08 seconds\n",
      "epoch 540| loss: 1.08716 | val_0_accuracy: 0.59881 |  0:00:45s\n",
      "⏱️ Epoch 541 took 0.08 seconds\n",
      "epoch 541| loss: 1.10778 | val_0_accuracy: 0.59286 |  0:00:45s\n",
      "⏱️ Epoch 542 took 0.08 seconds\n",
      "epoch 542| loss: 1.02662 | val_0_accuracy: 0.60952 |  0:00:45s\n",
      "⏱️ Epoch 543 took 0.09 seconds\n",
      "epoch 543| loss: 0.99558 | val_0_accuracy: 0.5381  |  0:00:45s\n",
      "⏱️ Epoch 544 took 0.08 seconds\n",
      "epoch 544| loss: 1.02379 | val_0_accuracy: 0.57024 |  0:00:45s\n",
      "⏱️ Epoch 545 took 0.08 seconds\n",
      "epoch 545| loss: 1.041   | val_0_accuracy: 0.59167 |  0:00:45s\n",
      "⏱️ Epoch 546 took 0.08 seconds\n",
      "epoch 546| loss: 1.00732 | val_0_accuracy: 0.56786 |  0:00:45s\n",
      "⏱️ Epoch 547 took 0.08 seconds\n",
      "epoch 547| loss: 1.03994 | val_0_accuracy: 0.59881 |  0:00:45s\n",
      "⏱️ Epoch 548 took 0.09 seconds\n",
      "epoch 548| loss: 1.0606  | val_0_accuracy: 0.60595 |  0:00:46s\n",
      "⏱️ Epoch 549 took 0.08 seconds\n",
      "epoch 549| loss: 1.01702 | val_0_accuracy: 0.60714 |  0:00:46s\n",
      "⏱️ Epoch 550 took 0.08 seconds\n",
      "epoch 550| loss: 1.01553 | val_0_accuracy: 0.62024 |  0:00:46s\n",
      "⏱️ Epoch 551 took 0.08 seconds\n",
      "epoch 551| loss: 0.99105 | val_0_accuracy: 0.61548 |  0:00:46s\n",
      "⏱️ Epoch 552 took 0.08 seconds\n",
      "epoch 552| loss: 0.98962 | val_0_accuracy: 0.57143 |  0:00:46s\n",
      "⏱️ Epoch 553 took 0.08 seconds\n",
      "epoch 553| loss: 1.0556  | val_0_accuracy: 0.59643 |  0:00:46s\n",
      "⏱️ Epoch 554 took 0.08 seconds\n",
      "epoch 554| loss: 1.02759 | val_0_accuracy: 0.61548 |  0:00:46s\n",
      "⏱️ Epoch 555 took 0.09 seconds\n",
      "epoch 555| loss: 1.03254 | val_0_accuracy: 0.62738 |  0:00:46s\n",
      "⏱️ Epoch 556 took 0.08 seconds\n",
      "epoch 556| loss: 1.02265 | val_0_accuracy: 0.62619 |  0:00:46s\n",
      "⏱️ Epoch 557 took 0.08 seconds\n",
      "epoch 557| loss: 0.97915 | val_0_accuracy: 0.60833 |  0:00:46s\n",
      "⏱️ Epoch 558 took 0.08 seconds\n",
      "epoch 558| loss: 1.00427 | val_0_accuracy: 0.62143 |  0:00:46s\n",
      "⏱️ Epoch 559 took 0.08 seconds\n",
      "epoch 559| loss: 1.06098 | val_0_accuracy: 0.58333 |  0:00:46s\n",
      "⏱️ Epoch 560 took 0.08 seconds\n",
      "epoch 560| loss: 1.12357 | val_0_accuracy: 0.57381 |  0:00:46s\n",
      "⏱️ Epoch 561 took 0.08 seconds\n",
      "epoch 561| loss: 1.0963  | val_0_accuracy: 0.59286 |  0:00:47s\n",
      "⏱️ Epoch 562 took 0.08 seconds\n",
      "epoch 562| loss: 1.03862 | val_0_accuracy: 0.60952 |  0:00:47s\n",
      "⏱️ Epoch 563 took 0.09 seconds\n",
      "epoch 563| loss: 1.06053 | val_0_accuracy: 0.62024 |  0:00:47s\n",
      "⏱️ Epoch 564 took 0.08 seconds\n",
      "epoch 564| loss: 1.02542 | val_0_accuracy: 0.60119 |  0:00:47s\n",
      "⏱️ Epoch 565 took 0.09 seconds\n",
      "epoch 565| loss: 1.09879 | val_0_accuracy: 0.59643 |  0:00:47s\n",
      "⏱️ Epoch 566 took 0.08 seconds\n",
      "epoch 566| loss: 1.01301 | val_0_accuracy: 0.60952 |  0:00:47s\n",
      "⏱️ Epoch 567 took 0.08 seconds\n",
      "epoch 567| loss: 1.05089 | val_0_accuracy: 0.60833 |  0:00:47s\n",
      "⏱️ Epoch 568 took 0.08 seconds\n",
      "epoch 568| loss: 1.011   | val_0_accuracy: 0.58095 |  0:00:47s\n",
      "⏱️ Epoch 569 took 0.08 seconds\n",
      "epoch 569| loss: 0.98407 | val_0_accuracy: 0.5631  |  0:00:47s\n",
      "⏱️ Epoch 570 took 0.09 seconds\n",
      "epoch 570| loss: 1.00138 | val_0_accuracy: 0.58333 |  0:00:47s\n",
      "⏱️ Epoch 571 took 0.08 seconds\n",
      "epoch 571| loss: 1.04937 | val_0_accuracy: 0.55714 |  0:00:47s\n",
      "⏱️ Epoch 572 took 0.08 seconds\n",
      "epoch 572| loss: 1.03313 | val_0_accuracy: 0.55833 |  0:00:47s\n",
      "⏱️ Epoch 573 took 0.08 seconds\n",
      "epoch 573| loss: 1.04777 | val_0_accuracy: 0.60119 |  0:00:48s\n",
      "⏱️ Epoch 574 took 0.08 seconds\n",
      "epoch 574| loss: 1.01921 | val_0_accuracy: 0.6131  |  0:00:48s\n",
      "⏱️ Epoch 575 took 0.08 seconds\n",
      "epoch 575| loss: 1.01023 | val_0_accuracy: 0.59881 |  0:00:48s\n",
      "⏱️ Epoch 576 took 0.08 seconds\n",
      "epoch 576| loss: 0.98961 | val_0_accuracy: 0.59167 |  0:00:48s\n",
      "⏱️ Epoch 577 took 0.08 seconds\n",
      "epoch 577| loss: 1.13824 | val_0_accuracy: 0.59286 |  0:00:48s\n",
      "⏱️ Epoch 578 took 0.09 seconds\n",
      "epoch 578| loss: 0.98238 | val_0_accuracy: 0.59167 |  0:00:48s\n",
      "⏱️ Epoch 579 took 0.08 seconds\n",
      "epoch 579| loss: 1.09295 | val_0_accuracy: 0.55238 |  0:00:48s\n",
      "⏱️ Epoch 580 took 0.08 seconds\n",
      "epoch 580| loss: 1.02653 | val_0_accuracy: 0.52381 |  0:00:48s\n",
      "⏱️ Epoch 581 took 0.09 seconds\n",
      "epoch 581| loss: 1.00344 | val_0_accuracy: 0.59643 |  0:00:48s\n",
      "⏱️ Epoch 582 took 0.08 seconds\n",
      "epoch 582| loss: 1.00382 | val_0_accuracy: 0.58929 |  0:00:48s\n",
      "⏱️ Epoch 583 took 0.08 seconds\n",
      "epoch 583| loss: 0.99293 | val_0_accuracy: 0.58571 |  0:00:48s\n",
      "⏱️ Epoch 584 took 0.08 seconds\n",
      "epoch 584| loss: 0.97759 | val_0_accuracy: 0.56548 |  0:00:48s\n",
      "⏱️ Epoch 585 took 0.08 seconds\n",
      "epoch 585| loss: 1.0065  | val_0_accuracy: 0.54762 |  0:00:49s\n",
      "⏱️ Epoch 586 took 0.08 seconds\n",
      "epoch 586| loss: 1.01339 | val_0_accuracy: 0.59167 |  0:00:49s\n",
      "⏱️ Epoch 587 took 0.08 seconds\n",
      "epoch 587| loss: 0.97391 | val_0_accuracy: 0.63452 |  0:00:49s\n",
      "⏱️ Epoch 588 took 0.09 seconds\n",
      "epoch 588| loss: 1.02379 | val_0_accuracy: 0.59881 |  0:00:49s\n",
      "⏱️ Epoch 589 took 0.08 seconds\n",
      "epoch 589| loss: 1.013   | val_0_accuracy: 0.60595 |  0:00:49s\n",
      "⏱️ Epoch 590 took 0.08 seconds\n",
      "epoch 590| loss: 1.07403 | val_0_accuracy: 0.5869  |  0:00:49s\n",
      "⏱️ Epoch 591 took 0.08 seconds\n",
      "epoch 591| loss: 1.00859 | val_0_accuracy: 0.6131  |  0:00:49s\n",
      "⏱️ Epoch 592 took 0.08 seconds\n",
      "epoch 592| loss: 1.00387 | val_0_accuracy: 0.60714 |  0:00:49s\n",
      "⏱️ Epoch 593 took 0.08 seconds\n",
      "epoch 593| loss: 0.96531 | val_0_accuracy: 0.63095 |  0:00:49s\n",
      "⏱️ Epoch 594 took 0.08 seconds\n",
      "epoch 594| loss: 1.01269 | val_0_accuracy: 0.60595 |  0:00:49s\n",
      "⏱️ Epoch 595 took 0.08 seconds\n",
      "epoch 595| loss: 1.01668 | val_0_accuracy: 0.57262 |  0:00:49s\n",
      "⏱️ Epoch 596 took 0.08 seconds\n",
      "epoch 596| loss: 1.03511 | val_0_accuracy: 0.6131  |  0:00:49s\n",
      "⏱️ Epoch 597 took 0.08 seconds\n",
      "epoch 597| loss: 1.0652  | val_0_accuracy: 0.62143 |  0:00:50s\n",
      "⏱️ Epoch 598 took 0.08 seconds\n",
      "epoch 598| loss: 1.00246 | val_0_accuracy: 0.60714 |  0:00:50s\n",
      "⏱️ Epoch 599 took 0.08 seconds\n",
      "epoch 599| loss: 0.96175 | val_0_accuracy: 0.5881  |  0:00:50s\n",
      "⏱️ Epoch 600 took 0.08 seconds\n",
      "epoch 600| loss: 1.09768 | val_0_accuracy: 0.56905 |  0:00:50s\n",
      "⏱️ Epoch 601 took 0.08 seconds\n",
      "epoch 601| loss: 1.09623 | val_0_accuracy: 0.56071 |  0:00:50s\n",
      "⏱️ Epoch 602 took 0.08 seconds\n",
      "epoch 602| loss: 0.96825 | val_0_accuracy: 0.59405 |  0:00:50s\n",
      "⏱️ Epoch 603 took 0.08 seconds\n",
      "epoch 603| loss: 1.02486 | val_0_accuracy: 0.59048 |  0:00:50s\n",
      "⏱️ Epoch 604 took 0.08 seconds\n",
      "epoch 604| loss: 0.97125 | val_0_accuracy: 0.62024 |  0:00:50s\n",
      "⏱️ Epoch 605 took 0.08 seconds\n",
      "epoch 605| loss: 0.94877 | val_0_accuracy: 0.5631  |  0:00:50s\n",
      "⏱️ Epoch 606 took 0.08 seconds\n",
      "epoch 606| loss: 1.01113 | val_0_accuracy: 0.59524 |  0:00:50s\n",
      "⏱️ Epoch 607 took 0.08 seconds\n",
      "epoch 607| loss: 0.97433 | val_0_accuracy: 0.63333 |  0:00:50s\n",
      "⏱️ Epoch 608 took 0.08 seconds\n",
      "epoch 608| loss: 1.06449 | val_0_accuracy: 0.60119 |  0:00:50s\n",
      "⏱️ Epoch 609 took 0.08 seconds\n",
      "epoch 609| loss: 0.94567 | val_0_accuracy: 0.6     |  0:00:51s\n",
      "⏱️ Epoch 610 took 0.08 seconds\n",
      "epoch 610| loss: 0.94785 | val_0_accuracy: 0.60476 |  0:00:51s\n",
      "⏱️ Epoch 611 took 0.08 seconds\n",
      "epoch 611| loss: 0.97039 | val_0_accuracy: 0.6     |  0:00:51s\n",
      "⏱️ Epoch 612 took 0.09 seconds\n",
      "epoch 612| loss: 1.00294 | val_0_accuracy: 0.58929 |  0:00:51s\n",
      "⏱️ Epoch 613 took 0.08 seconds\n",
      "epoch 613| loss: 0.98259 | val_0_accuracy: 0.59048 |  0:00:51s\n",
      "⏱️ Epoch 614 took 0.08 seconds\n",
      "epoch 614| loss: 0.96707 | val_0_accuracy: 0.59643 |  0:00:51s\n",
      "⏱️ Epoch 615 took 0.08 seconds\n",
      "epoch 615| loss: 0.98535 | val_0_accuracy: 0.625   |  0:00:51s\n",
      "⏱️ Epoch 616 took 0.08 seconds\n",
      "epoch 616| loss: 0.93664 | val_0_accuracy: 0.62262 |  0:00:51s\n",
      "⏱️ Epoch 617 took 0.08 seconds\n",
      "epoch 617| loss: 1.01865 | val_0_accuracy: 0.6131  |  0:00:51s\n",
      "⏱️ Epoch 618 took 0.08 seconds\n",
      "epoch 618| loss: 1.02895 | val_0_accuracy: 0.59762 |  0:00:51s\n",
      "⏱️ Epoch 619 took 0.08 seconds\n",
      "epoch 619| loss: 0.98284 | val_0_accuracy: 0.60833 |  0:00:51s\n",
      "⏱️ Epoch 620 took 0.08 seconds\n",
      "epoch 620| loss: 1.00935 | val_0_accuracy: 0.58571 |  0:00:51s\n",
      "⏱️ Epoch 621 took 0.08 seconds\n",
      "epoch 621| loss: 1.01661 | val_0_accuracy: 0.60833 |  0:00:52s\n",
      "⏱️ Epoch 622 took 0.20 seconds\n",
      "epoch 622| loss: 1.05285 | val_0_accuracy: 0.61667 |  0:00:52s\n",
      "⏱️ Epoch 623 took 0.08 seconds\n",
      "epoch 623| loss: 0.97589 | val_0_accuracy: 0.59167 |  0:00:52s\n",
      "⏱️ Epoch 624 took 0.08 seconds\n",
      "epoch 624| loss: 0.97775 | val_0_accuracy: 0.60357 |  0:00:52s\n",
      "⏱️ Epoch 625 took 0.08 seconds\n",
      "epoch 625| loss: 0.95017 | val_0_accuracy: 0.62857 |  0:00:52s\n",
      "⏱️ Epoch 626 took 0.08 seconds\n",
      "epoch 626| loss: 0.96208 | val_0_accuracy: 0.62024 |  0:00:52s\n",
      "⏱️ Epoch 627 took 0.08 seconds\n",
      "epoch 627| loss: 0.9455  | val_0_accuracy: 0.61667 |  0:00:52s\n",
      "⏱️ Epoch 628 took 0.08 seconds\n",
      "epoch 628| loss: 0.93289 | val_0_accuracy: 0.62262 |  0:00:52s\n",
      "⏱️ Epoch 629 took 0.08 seconds\n",
      "epoch 629| loss: 1.03018 | val_0_accuracy: 0.61548 |  0:00:52s\n",
      "⏱️ Epoch 630 took 0.08 seconds\n",
      "epoch 630| loss: 0.9476  | val_0_accuracy: 0.59524 |  0:00:52s\n",
      "⏱️ Epoch 631 took 0.08 seconds\n",
      "epoch 631| loss: 0.97886 | val_0_accuracy: 0.59881 |  0:00:52s\n",
      "⏱️ Epoch 632 took 0.08 seconds\n",
      "epoch 632| loss: 0.96179 | val_0_accuracy: 0.61667 |  0:00:53s\n",
      "⏱️ Epoch 633 took 0.08 seconds\n",
      "epoch 633| loss: 0.93984 | val_0_accuracy: 0.60952 |  0:00:53s\n",
      "⏱️ Epoch 634 took 0.08 seconds\n",
      "epoch 634| loss: 0.94292 | val_0_accuracy: 0.63571 |  0:00:53s\n",
      "⏱️ Epoch 635 took 0.09 seconds\n",
      "epoch 635| loss: 0.91587 | val_0_accuracy: 0.62619 |  0:00:53s\n",
      "⏱️ Epoch 636 took 0.08 seconds\n",
      "epoch 636| loss: 0.98427 | val_0_accuracy: 0.60833 |  0:00:53s\n",
      "⏱️ Epoch 637 took 0.08 seconds\n",
      "epoch 637| loss: 0.97121 | val_0_accuracy: 0.61548 |  0:00:53s\n",
      "⏱️ Epoch 638 took 0.08 seconds\n",
      "epoch 638| loss: 0.96686 | val_0_accuracy: 0.57619 |  0:00:53s\n",
      "⏱️ Epoch 639 took 0.09 seconds\n",
      "epoch 639| loss: 0.96946 | val_0_accuracy: 0.61786 |  0:00:53s\n",
      "⏱️ Epoch 640 took 0.08 seconds\n",
      "epoch 640| loss: 1.08534 | val_0_accuracy: 0.62619 |  0:00:53s\n",
      "⏱️ Epoch 641 took 0.08 seconds\n",
      "epoch 641| loss: 0.95901 | val_0_accuracy: 0.60238 |  0:00:53s\n",
      "⏱️ Epoch 642 took 0.08 seconds\n",
      "epoch 642| loss: 0.94651 | val_0_accuracy: 0.60595 |  0:00:53s\n",
      "⏱️ Epoch 643 took 0.08 seconds\n",
      "epoch 643| loss: 0.96847 | val_0_accuracy: 0.63095 |  0:00:53s\n",
      "⏱️ Epoch 644 took 0.08 seconds\n",
      "epoch 644| loss: 1.10241 | val_0_accuracy: 0.61786 |  0:00:53s\n",
      "⏱️ Epoch 645 took 0.08 seconds\n",
      "epoch 645| loss: 0.97216 | val_0_accuracy: 0.63214 |  0:00:54s\n",
      "⏱️ Epoch 646 took 0.08 seconds\n",
      "epoch 646| loss: 0.95528 | val_0_accuracy: 0.62262 |  0:00:54s\n",
      "⏱️ Epoch 647 took 0.08 seconds\n",
      "epoch 647| loss: 0.93157 | val_0_accuracy: 0.6119  |  0:00:54s\n",
      "⏱️ Epoch 648 took 0.08 seconds\n",
      "epoch 648| loss: 0.95146 | val_0_accuracy: 0.62619 |  0:00:54s\n",
      "⏱️ Epoch 649 took 0.08 seconds\n",
      "epoch 649| loss: 0.95825 | val_0_accuracy: 0.61905 |  0:00:54s\n",
      "⏱️ Epoch 650 took 0.08 seconds\n",
      "epoch 650| loss: 0.98955 | val_0_accuracy: 0.59048 |  0:00:54s\n",
      "⏱️ Epoch 651 took 0.08 seconds\n",
      "epoch 651| loss: 0.95567 | val_0_accuracy: 0.61667 |  0:00:54s\n",
      "⏱️ Epoch 652 took 0.08 seconds\n",
      "epoch 652| loss: 1.02356 | val_0_accuracy: 0.58929 |  0:00:54s\n",
      "⏱️ Epoch 653 took 0.08 seconds\n",
      "epoch 653| loss: 0.97327 | val_0_accuracy: 0.57024 |  0:00:54s\n",
      "⏱️ Epoch 654 took 0.08 seconds\n",
      "epoch 654| loss: 0.95187 | val_0_accuracy: 0.57976 |  0:00:54s\n",
      "⏱️ Epoch 655 took 0.08 seconds\n",
      "epoch 655| loss: 0.94898 | val_0_accuracy: 0.57857 |  0:00:54s\n",
      "⏱️ Epoch 656 took 0.08 seconds\n",
      "epoch 656| loss: 0.92237 | val_0_accuracy: 0.57619 |  0:00:54s\n",
      "⏱️ Epoch 657 took 0.08 seconds\n",
      "epoch 657| loss: 0.93242 | val_0_accuracy: 0.57024 |  0:00:55s\n",
      "⏱️ Epoch 658 took 0.08 seconds\n",
      "epoch 658| loss: 0.90131 | val_0_accuracy: 0.6     |  0:00:55s\n",
      "⏱️ Epoch 659 took 0.09 seconds\n",
      "epoch 659| loss: 0.91699 | val_0_accuracy: 0.61786 |  0:00:55s\n",
      "⏱️ Epoch 660 took 0.09 seconds\n",
      "epoch 660| loss: 0.90964 | val_0_accuracy: 0.6119  |  0:00:55s\n",
      "⏱️ Epoch 661 took 0.09 seconds\n",
      "epoch 661| loss: 0.93444 | val_0_accuracy: 0.62619 |  0:00:55s\n",
      "⏱️ Epoch 662 took 0.09 seconds\n",
      "epoch 662| loss: 0.97328 | val_0_accuracy: 0.62857 |  0:00:55s\n",
      "⏱️ Epoch 663 took 0.08 seconds\n",
      "epoch 663| loss: 0.92113 | val_0_accuracy: 0.59524 |  0:00:55s\n",
      "⏱️ Epoch 664 took 0.08 seconds\n",
      "epoch 664| loss: 1.0279  | val_0_accuracy: 0.53929 |  0:00:55s\n",
      "⏱️ Epoch 665 took 0.08 seconds\n",
      "epoch 665| loss: 1.00399 | val_0_accuracy: 0.60238 |  0:00:55s\n",
      "⏱️ Epoch 666 took 0.08 seconds\n",
      "epoch 666| loss: 0.97551 | val_0_accuracy: 0.61429 |  0:00:55s\n",
      "⏱️ Epoch 667 took 0.08 seconds\n",
      "epoch 667| loss: 0.94535 | val_0_accuracy: 0.62024 |  0:00:55s\n",
      "⏱️ Epoch 668 took 0.08 seconds\n",
      "epoch 668| loss: 0.92862 | val_0_accuracy: 0.6131  |  0:00:55s\n",
      "⏱️ Epoch 669 took 0.08 seconds\n",
      "epoch 669| loss: 0.87464 | val_0_accuracy: 0.64048 |  0:00:56s\n",
      "⏱️ Epoch 670 took 0.09 seconds\n",
      "epoch 670| loss: 0.93154 | val_0_accuracy: 0.62619 |  0:00:56s\n",
      "⏱️ Epoch 671 took 0.08 seconds\n",
      "epoch 671| loss: 0.94042 | val_0_accuracy: 0.6     |  0:00:56s\n",
      "⏱️ Epoch 672 took 0.08 seconds\n",
      "epoch 672| loss: 0.95371 | val_0_accuracy: 0.59643 |  0:00:56s\n",
      "⏱️ Epoch 673 took 0.08 seconds\n",
      "epoch 673| loss: 0.9332  | val_0_accuracy: 0.63095 |  0:00:56s\n",
      "⏱️ Epoch 674 took 0.08 seconds\n",
      "epoch 674| loss: 0.99095 | val_0_accuracy: 0.62976 |  0:00:56s\n",
      "⏱️ Epoch 675 took 0.08 seconds\n",
      "epoch 675| loss: 0.9171  | val_0_accuracy: 0.60476 |  0:00:56s\n",
      "⏱️ Epoch 676 took 0.08 seconds\n",
      "epoch 676| loss: 0.89827 | val_0_accuracy: 0.61667 |  0:00:56s\n",
      "⏱️ Epoch 677 took 0.08 seconds\n",
      "epoch 677| loss: 0.90904 | val_0_accuracy: 0.61548 |  0:00:56s\n",
      "⏱️ Epoch 678 took 0.08 seconds\n",
      "epoch 678| loss: 0.98599 | val_0_accuracy: 0.61548 |  0:00:56s\n",
      "⏱️ Epoch 679 took 0.08 seconds\n",
      "epoch 679| loss: 0.9989  | val_0_accuracy: 0.61429 |  0:00:56s\n",
      "⏱️ Epoch 680 took 0.08 seconds\n",
      "epoch 680| loss: 0.90787 | val_0_accuracy: 0.62262 |  0:00:56s\n",
      "⏱️ Epoch 681 took 0.08 seconds\n",
      "epoch 681| loss: 0.92174 | val_0_accuracy: 0.58095 |  0:00:57s\n",
      "⏱️ Epoch 682 took 0.08 seconds\n",
      "epoch 682| loss: 0.90454 | val_0_accuracy: 0.55    |  0:00:57s\n",
      "⏱️ Epoch 683 took 0.08 seconds\n",
      "epoch 683| loss: 0.97107 | val_0_accuracy: 0.53214 |  0:00:57s\n",
      "⏱️ Epoch 684 took 0.08 seconds\n",
      "epoch 684| loss: 0.90638 | val_0_accuracy: 0.6     |  0:00:57s\n",
      "⏱️ Epoch 685 took 0.08 seconds\n",
      "epoch 685| loss: 0.99731 | val_0_accuracy: 0.61548 |  0:00:57s\n",
      "⏱️ Epoch 686 took 0.08 seconds\n",
      "epoch 686| loss: 0.9007  | val_0_accuracy: 0.61071 |  0:00:57s\n",
      "⏱️ Epoch 687 took 0.08 seconds\n",
      "epoch 687| loss: 0.95499 | val_0_accuracy: 0.62024 |  0:00:57s\n",
      "⏱️ Epoch 688 took 0.09 seconds\n",
      "epoch 688| loss: 0.97534 | val_0_accuracy: 0.64762 |  0:00:57s\n",
      "⏱️ Epoch 689 took 0.08 seconds\n",
      "epoch 689| loss: 0.92148 | val_0_accuracy: 0.62738 |  0:00:57s\n",
      "⏱️ Epoch 690 took 0.08 seconds\n",
      "epoch 690| loss: 0.87251 | val_0_accuracy: 0.59643 |  0:00:57s\n",
      "⏱️ Epoch 691 took 0.08 seconds\n",
      "epoch 691| loss: 0.88251 | val_0_accuracy: 0.57976 |  0:00:57s\n",
      "⏱️ Epoch 692 took 0.09 seconds\n",
      "epoch 692| loss: 0.90954 | val_0_accuracy: 0.57381 |  0:00:57s\n",
      "⏱️ Epoch 693 took 0.08 seconds\n",
      "epoch 693| loss: 0.95221 | val_0_accuracy: 0.62381 |  0:00:58s\n",
      "⏱️ Epoch 694 took 0.08 seconds\n",
      "epoch 694| loss: 0.95443 | val_0_accuracy: 0.575   |  0:00:58s\n",
      "⏱️ Epoch 695 took 0.09 seconds\n",
      "epoch 695| loss: 0.98253 | val_0_accuracy: 0.52738 |  0:00:58s\n",
      "⏱️ Epoch 696 took 0.08 seconds\n",
      "epoch 696| loss: 0.91816 | val_0_accuracy: 0.61667 |  0:00:58s\n",
      "⏱️ Epoch 697 took 0.08 seconds\n",
      "epoch 697| loss: 0.97008 | val_0_accuracy: 0.62857 |  0:00:58s\n",
      "⏱️ Epoch 698 took 0.08 seconds\n",
      "epoch 698| loss: 0.90393 | val_0_accuracy: 0.60714 |  0:00:58s\n",
      "⏱️ Epoch 699 took 0.08 seconds\n",
      "epoch 699| loss: 0.89735 | val_0_accuracy: 0.59167 |  0:00:58s\n",
      "⏱️ Epoch 700 took 0.08 seconds\n",
      "epoch 700| loss: 0.98573 | val_0_accuracy: 0.58571 |  0:00:58s\n",
      "⏱️ Epoch 701 took 0.09 seconds\n",
      "epoch 701| loss: 0.99716 | val_0_accuracy: 0.625   |  0:00:58s\n",
      "⏱️ Epoch 702 took 0.08 seconds\n",
      "epoch 702| loss: 0.92824 | val_0_accuracy: 0.59881 |  0:00:58s\n",
      "⏱️ Epoch 703 took 0.08 seconds\n",
      "epoch 703| loss: 0.93054 | val_0_accuracy: 0.57976 |  0:00:58s\n",
      "⏱️ Epoch 704 took 0.08 seconds\n",
      "epoch 704| loss: 0.94587 | val_0_accuracy: 0.62976 |  0:00:58s\n",
      "⏱️ Epoch 705 took 0.08 seconds\n",
      "epoch 705| loss: 0.8931  | val_0_accuracy: 0.64048 |  0:00:59s\n",
      "⏱️ Epoch 706 took 0.08 seconds\n",
      "epoch 706| loss: 0.91981 | val_0_accuracy: 0.63095 |  0:00:59s\n",
      "⏱️ Epoch 707 took 0.08 seconds\n",
      "epoch 707| loss: 0.89019 | val_0_accuracy: 0.60833 |  0:00:59s\n",
      "⏱️ Epoch 708 took 0.08 seconds\n",
      "epoch 708| loss: 0.92005 | val_0_accuracy: 0.6     |  0:00:59s\n",
      "⏱️ Epoch 709 took 0.08 seconds\n",
      "epoch 709| loss: 0.90038 | val_0_accuracy: 0.62024 |  0:00:59s\n",
      "⏱️ Epoch 710 took 0.08 seconds\n",
      "epoch 710| loss: 0.87567 | val_0_accuracy: 0.61429 |  0:00:59s\n",
      "⏱️ Epoch 711 took 0.08 seconds\n",
      "epoch 711| loss: 0.88759 | val_0_accuracy: 0.60357 |  0:00:59s\n",
      "⏱️ Epoch 712 took 0.08 seconds\n",
      "epoch 712| loss: 0.8621  | val_0_accuracy: 0.62857 |  0:00:59s\n",
      "⏱️ Epoch 713 took 0.08 seconds\n",
      "epoch 713| loss: 0.91728 | val_0_accuracy: 0.61786 |  0:00:59s\n",
      "⏱️ Epoch 714 took 0.08 seconds\n",
      "epoch 714| loss: 0.93271 | val_0_accuracy: 0.58929 |  0:00:59s\n",
      "⏱️ Epoch 715 took 0.08 seconds\n",
      "epoch 715| loss: 0.90469 | val_0_accuracy: 0.57381 |  0:00:59s\n",
      "⏱️ Epoch 716 took 0.08 seconds\n",
      "epoch 716| loss: 0.88563 | val_0_accuracy: 0.6     |  0:00:59s\n",
      "⏱️ Epoch 717 took 0.09 seconds\n",
      "epoch 717| loss: 0.91858 | val_0_accuracy: 0.63095 |  0:00:59s\n",
      "⏱️ Epoch 718 took 0.08 seconds\n",
      "epoch 718| loss: 0.91481 | val_0_accuracy: 0.6119  |  0:01:00s\n",
      "⏱️ Epoch 719 took 0.08 seconds\n",
      "epoch 719| loss: 0.89351 | val_0_accuracy: 0.5869  |  0:01:00s\n",
      "⏱️ Epoch 720 took 0.08 seconds\n",
      "epoch 720| loss: 0.89253 | val_0_accuracy: 0.58571 |  0:01:00s\n",
      "⏱️ Epoch 721 took 0.08 seconds\n",
      "epoch 721| loss: 0.95009 | val_0_accuracy: 0.60952 |  0:01:00s\n",
      "⏱️ Epoch 722 took 0.08 seconds\n",
      "epoch 722| loss: 0.91459 | val_0_accuracy: 0.61429 |  0:01:00s\n",
      "⏱️ Epoch 723 took 0.08 seconds\n",
      "epoch 723| loss: 0.93182 | val_0_accuracy: 0.60476 |  0:01:00s\n",
      "⏱️ Epoch 724 took 0.08 seconds\n",
      "epoch 724| loss: 0.88516 | val_0_accuracy: 0.54405 |  0:01:00s\n",
      "⏱️ Epoch 725 took 0.08 seconds\n",
      "epoch 725| loss: 0.91042 | val_0_accuracy: 0.54167 |  0:01:00s\n",
      "⏱️ Epoch 726 took 0.08 seconds\n",
      "epoch 726| loss: 0.92426 | val_0_accuracy: 0.6119  |  0:01:00s\n",
      "⏱️ Epoch 727 took 0.09 seconds\n",
      "epoch 727| loss: 0.92606 | val_0_accuracy: 0.59881 |  0:01:00s\n",
      "⏱️ Epoch 728 took 0.09 seconds\n",
      "epoch 728| loss: 0.8426  | val_0_accuracy: 0.58214 |  0:01:00s\n",
      "⏱️ Epoch 729 took 0.08 seconds\n",
      "epoch 729| loss: 0.88006 | val_0_accuracy: 0.59524 |  0:01:00s\n",
      "⏱️ Epoch 730 took 0.08 seconds\n",
      "epoch 730| loss: 0.89444 | val_0_accuracy: 0.62976 |  0:01:01s\n",
      "⏱️ Epoch 731 took 0.08 seconds\n",
      "epoch 731| loss: 0.8769  | val_0_accuracy: 0.60476 |  0:01:01s\n",
      "⏱️ Epoch 732 took 0.08 seconds\n",
      "epoch 732| loss: 0.95478 | val_0_accuracy: 0.62262 |  0:01:01s\n",
      "⏱️ Epoch 733 took 0.08 seconds\n",
      "epoch 733| loss: 0.95969 | val_0_accuracy: 0.60952 |  0:01:01s\n",
      "⏱️ Epoch 734 took 0.08 seconds\n",
      "epoch 734| loss: 0.85032 | val_0_accuracy: 0.60238 |  0:01:01s\n",
      "⏱️ Epoch 735 took 0.08 seconds\n",
      "epoch 735| loss: 0.97129 | val_0_accuracy: 0.57024 |  0:01:01s\n",
      "⏱️ Epoch 736 took 0.08 seconds\n",
      "epoch 736| loss: 0.90491 | val_0_accuracy: 0.57857 |  0:01:01s\n",
      "⏱️ Epoch 737 took 0.08 seconds\n",
      "epoch 737| loss: 0.90937 | val_0_accuracy: 0.60119 |  0:01:01s\n",
      "⏱️ Epoch 738 took 0.08 seconds\n",
      "epoch 738| loss: 0.92279 | val_0_accuracy: 0.5881  |  0:01:01s\n",
      "⏱️ Epoch 739 took 0.08 seconds\n",
      "epoch 739| loss: 0.86888 | val_0_accuracy: 0.58929 |  0:01:01s\n",
      "⏱️ Epoch 740 took 0.08 seconds\n",
      "epoch 740| loss: 0.87071 | val_0_accuracy: 0.59405 |  0:01:01s\n",
      "⏱️ Epoch 741 took 0.08 seconds\n",
      "epoch 741| loss: 0.91332 | val_0_accuracy: 0.60357 |  0:01:01s\n",
      "⏱️ Epoch 742 took 0.08 seconds\n",
      "epoch 742| loss: 0.94029 | val_0_accuracy: 0.6131  |  0:01:02s\n",
      "⏱️ Epoch 743 took 0.08 seconds\n",
      "epoch 743| loss: 0.86813 | val_0_accuracy: 0.61786 |  0:01:02s\n",
      "⏱️ Epoch 744 took 0.08 seconds\n",
      "epoch 744| loss: 0.92463 | val_0_accuracy: 0.5869  |  0:01:02s\n",
      "⏱️ Epoch 745 took 0.08 seconds\n",
      "epoch 745| loss: 0.88931 | val_0_accuracy: 0.60476 |  0:01:02s\n",
      "⏱️ Epoch 746 took 0.08 seconds\n",
      "epoch 746| loss: 0.99576 | val_0_accuracy: 0.59524 |  0:01:02s\n",
      "⏱️ Epoch 747 took 0.08 seconds\n",
      "epoch 747| loss: 0.89458 | val_0_accuracy: 0.59643 |  0:01:02s\n",
      "⏱️ Epoch 748 took 0.08 seconds\n",
      "epoch 748| loss: 0.90195 | val_0_accuracy: 0.61548 |  0:01:02s\n",
      "⏱️ Epoch 749 took 0.09 seconds\n",
      "epoch 749| loss: 0.95807 | val_0_accuracy: 0.62024 |  0:01:02s\n",
      "⏱️ Epoch 750 took 0.08 seconds\n",
      "epoch 750| loss: 0.869   | val_0_accuracy: 0.60595 |  0:01:02s\n",
      "⏱️ Epoch 751 took 0.08 seconds\n",
      "epoch 751| loss: 0.8689  | val_0_accuracy: 0.59881 |  0:01:02s\n",
      "⏱️ Epoch 752 took 0.08 seconds\n",
      "epoch 752| loss: 0.85068 | val_0_accuracy: 0.60119 |  0:01:02s\n",
      "⏱️ Epoch 753 took 0.09 seconds\n",
      "epoch 753| loss: 0.90106 | val_0_accuracy: 0.59524 |  0:01:02s\n",
      "⏱️ Epoch 754 took 0.08 seconds\n",
      "epoch 754| loss: 0.87544 | val_0_accuracy: 0.61667 |  0:01:03s\n",
      "⏱️ Epoch 755 took 0.08 seconds\n",
      "epoch 755| loss: 0.83577 | val_0_accuracy: 0.63214 |  0:01:03s\n",
      "⏱️ Epoch 756 took 0.09 seconds\n",
      "epoch 756| loss: 0.88056 | val_0_accuracy: 0.62381 |  0:01:03s\n",
      "⏱️ Epoch 757 took 0.11 seconds\n",
      "epoch 757| loss: 0.878   | val_0_accuracy: 0.60595 |  0:01:03s\n",
      "⏱️ Epoch 758 took 0.08 seconds\n",
      "epoch 758| loss: 0.8877  | val_0_accuracy: 0.6131  |  0:01:03s\n",
      "⏱️ Epoch 759 took 0.08 seconds\n",
      "epoch 759| loss: 0.88669 | val_0_accuracy: 0.60238 |  0:01:03s\n",
      "⏱️ Epoch 760 took 0.08 seconds\n",
      "epoch 760| loss: 0.88693 | val_0_accuracy: 0.59405 |  0:01:03s\n",
      "⏱️ Epoch 761 took 0.09 seconds\n",
      "epoch 761| loss: 0.85706 | val_0_accuracy: 0.54762 |  0:01:03s\n",
      "⏱️ Epoch 762 took 0.08 seconds\n",
      "epoch 762| loss: 0.89502 | val_0_accuracy: 0.55833 |  0:01:03s\n",
      "⏱️ Epoch 763 took 0.08 seconds\n",
      "epoch 763| loss: 0.88466 | val_0_accuracy: 0.62262 |  0:01:03s\n",
      "⏱️ Epoch 764 took 0.08 seconds\n",
      "epoch 764| loss: 0.83367 | val_0_accuracy: 0.63214 |  0:01:03s\n",
      "⏱️ Epoch 765 took 0.08 seconds\n",
      "epoch 765| loss: 0.84706 | val_0_accuracy: 0.59524 |  0:01:03s\n",
      "⏱️ Epoch 766 took 0.08 seconds\n",
      "epoch 766| loss: 0.8686  | val_0_accuracy: 0.5869  |  0:01:04s\n",
      "⏱️ Epoch 767 took 0.08 seconds\n",
      "epoch 767| loss: 0.85089 | val_0_accuracy: 0.5869  |  0:01:04s\n",
      "⏱️ Epoch 768 took 0.08 seconds\n",
      "epoch 768| loss: 0.85222 | val_0_accuracy: 0.60238 |  0:01:04s\n",
      "⏱️ Epoch 769 took 0.08 seconds\n",
      "epoch 769| loss: 0.85962 | val_0_accuracy: 0.61071 |  0:01:04s\n",
      "⏱️ Epoch 770 took 0.08 seconds\n",
      "epoch 770| loss: 0.8163  | val_0_accuracy: 0.61548 |  0:01:04s\n",
      "⏱️ Epoch 771 took 0.08 seconds\n",
      "epoch 771| loss: 0.90557 | val_0_accuracy: 0.59167 |  0:01:04s\n",
      "⏱️ Epoch 772 took 0.08 seconds\n",
      "epoch 772| loss: 0.85914 | val_0_accuracy: 0.62262 |  0:01:04s\n",
      "⏱️ Epoch 773 took 0.08 seconds\n",
      "epoch 773| loss: 0.82557 | val_0_accuracy: 0.59643 |  0:01:04s\n",
      "⏱️ Epoch 774 took 0.08 seconds\n",
      "epoch 774| loss: 0.83763 | val_0_accuracy: 0.59286 |  0:01:04s\n",
      "⏱️ Epoch 775 took 0.09 seconds\n",
      "epoch 775| loss: 0.87279 | val_0_accuracy: 0.59524 |  0:01:04s\n",
      "⏱️ Epoch 776 took 0.08 seconds\n",
      "epoch 776| loss: 0.85253 | val_0_accuracy: 0.59405 |  0:01:04s\n",
      "⏱️ Epoch 777 took 0.08 seconds\n",
      "epoch 777| loss: 0.97021 | val_0_accuracy: 0.6119  |  0:01:04s\n",
      "⏱️ Epoch 778 took 0.08 seconds\n",
      "epoch 778| loss: 0.85505 | val_0_accuracy: 0.60119 |  0:01:05s\n",
      "⏱️ Epoch 779 took 0.08 seconds\n",
      "epoch 779| loss: 0.87985 | val_0_accuracy: 0.57262 |  0:01:05s\n",
      "⏱️ Epoch 780 took 0.08 seconds\n",
      "epoch 780| loss: 0.85386 | val_0_accuracy: 0.62857 |  0:01:05s\n",
      "⏱️ Epoch 781 took 0.08 seconds\n",
      "epoch 781| loss: 0.84511 | val_0_accuracy: 0.63095 |  0:01:05s\n",
      "⏱️ Epoch 782 took 0.08 seconds\n",
      "epoch 782| loss: 1.11314 | val_0_accuracy: 0.60238 |  0:01:05s\n",
      "⏱️ Epoch 783 took 0.08 seconds\n",
      "epoch 783| loss: 0.9276  | val_0_accuracy: 0.61667 |  0:01:05s\n",
      "⏱️ Epoch 784 took 0.08 seconds\n",
      "epoch 784| loss: 0.8942  | val_0_accuracy: 0.61548 |  0:01:05s\n",
      "⏱️ Epoch 785 took 0.08 seconds\n",
      "epoch 785| loss: 0.85128 | val_0_accuracy: 0.5881  |  0:01:05s\n",
      "⏱️ Epoch 786 took 0.08 seconds\n",
      "epoch 786| loss: 0.80198 | val_0_accuracy: 0.59286 |  0:01:05s\n",
      "⏱️ Epoch 787 took 0.08 seconds\n",
      "epoch 787| loss: 0.82325 | val_0_accuracy: 0.60238 |  0:01:05s\n",
      "⏱️ Epoch 788 took 0.08 seconds\n",
      "epoch 788| loss: 0.82854 | val_0_accuracy: 0.61905 |  0:01:05s\n",
      "⏱️ Epoch 789 took 0.08 seconds\n",
      "epoch 789| loss: 0.8147  | val_0_accuracy: 0.60238 |  0:01:05s\n",
      "⏱️ Epoch 790 took 0.08 seconds\n",
      "epoch 790| loss: 0.84886 | val_0_accuracy: 0.55238 |  0:01:05s\n",
      "⏱️ Epoch 791 took 0.08 seconds\n",
      "epoch 791| loss: 0.8615  | val_0_accuracy: 0.55833 |  0:01:06s\n",
      "⏱️ Epoch 792 took 0.08 seconds\n",
      "epoch 792| loss: 0.87433 | val_0_accuracy: 0.61071 |  0:01:06s\n",
      "⏱️ Epoch 793 took 0.08 seconds\n",
      "epoch 793| loss: 0.90144 | val_0_accuracy: 0.60714 |  0:01:06s\n",
      "⏱️ Epoch 794 took 0.08 seconds\n",
      "epoch 794| loss: 0.85018 | val_0_accuracy: 0.59524 |  0:01:06s\n",
      "⏱️ Epoch 795 took 0.08 seconds\n",
      "epoch 795| loss: 0.86968 | val_0_accuracy: 0.60952 |  0:01:06s\n",
      "⏱️ Epoch 796 took 0.08 seconds\n",
      "epoch 796| loss: 0.85897 | val_0_accuracy: 0.62857 |  0:01:06s\n",
      "⏱️ Epoch 797 took 0.08 seconds\n",
      "epoch 797| loss: 0.85599 | val_0_accuracy: 0.60357 |  0:01:06s\n",
      "⏱️ Epoch 798 took 0.08 seconds\n",
      "epoch 798| loss: 0.90222 | val_0_accuracy: 0.60476 |  0:01:06s\n",
      "⏱️ Epoch 799 took 0.08 seconds\n",
      "epoch 799| loss: 0.83502 | val_0_accuracy: 0.61905 |  0:01:06s\n",
      "⏱️ Epoch 800 took 0.08 seconds\n",
      "epoch 800| loss: 0.81793 | val_0_accuracy: 0.61429 |  0:01:06s\n",
      "⏱️ Epoch 801 took 0.08 seconds\n",
      "epoch 801| loss: 0.85756 | val_0_accuracy: 0.6131  |  0:01:06s\n",
      "⏱️ Epoch 802 took 0.08 seconds\n",
      "epoch 802| loss: 0.80846 | val_0_accuracy: 0.60952 |  0:01:06s\n",
      "⏱️ Epoch 803 took 0.08 seconds\n",
      "epoch 803| loss: 0.89169 | val_0_accuracy: 0.57738 |  0:01:07s\n",
      "⏱️ Epoch 804 took 0.09 seconds\n",
      "epoch 804| loss: 0.79947 | val_0_accuracy: 0.60238 |  0:01:07s\n",
      "⏱️ Epoch 805 took 0.08 seconds\n",
      "epoch 805| loss: 0.88701 | val_0_accuracy: 0.5631  |  0:01:07s\n",
      "⏱️ Epoch 806 took 0.08 seconds\n",
      "epoch 806| loss: 0.80991 | val_0_accuracy: 0.59881 |  0:01:07s\n",
      "⏱️ Epoch 807 took 0.08 seconds\n",
      "epoch 807| loss: 0.79342 | val_0_accuracy: 0.57143 |  0:01:07s\n",
      "⏱️ Epoch 808 took 0.19 seconds\n",
      "epoch 808| loss: 0.89259 | val_0_accuracy: 0.57857 |  0:01:07s\n",
      "⏱️ Epoch 809 took 0.08 seconds\n",
      "epoch 809| loss: 0.86106 | val_0_accuracy: 0.59167 |  0:01:07s\n",
      "⏱️ Epoch 810 took 0.08 seconds\n",
      "epoch 810| loss: 0.80357 | val_0_accuracy: 0.60119 |  0:01:07s\n",
      "⏱️ Epoch 811 took 0.08 seconds\n",
      "epoch 811| loss: 0.83721 | val_0_accuracy: 0.58214 |  0:01:07s\n",
      "⏱️ Epoch 812 took 0.08 seconds\n",
      "epoch 812| loss: 0.83215 | val_0_accuracy: 0.6     |  0:01:07s\n",
      "⏱️ Epoch 813 took 0.08 seconds\n",
      "epoch 813| loss: 0.83655 | val_0_accuracy: 0.62024 |  0:01:07s\n",
      "⏱️ Epoch 814 took 0.08 seconds\n",
      "epoch 814| loss: 0.86763 | val_0_accuracy: 0.59048 |  0:01:08s\n",
      "⏱️ Epoch 815 took 0.08 seconds\n",
      "epoch 815| loss: 0.87776 | val_0_accuracy: 0.6     |  0:01:08s\n",
      "⏱️ Epoch 816 took 0.08 seconds\n",
      "epoch 816| loss: 0.83113 | val_0_accuracy: 0.60595 |  0:01:08s\n",
      "⏱️ Epoch 817 took 0.08 seconds\n",
      "epoch 817| loss: 0.85708 | val_0_accuracy: 0.62619 |  0:01:08s\n",
      "⏱️ Epoch 818 took 0.08 seconds\n",
      "epoch 818| loss: 0.87276 | val_0_accuracy: 0.6369  |  0:01:08s\n",
      "⏱️ Epoch 819 took 0.08 seconds\n",
      "epoch 819| loss: 0.86709 | val_0_accuracy: 0.60357 |  0:01:08s\n",
      "⏱️ Epoch 820 took 0.08 seconds\n",
      "epoch 820| loss: 0.85483 | val_0_accuracy: 0.62024 |  0:01:08s\n",
      "⏱️ Epoch 821 took 0.08 seconds\n",
      "epoch 821| loss: 0.83287 | val_0_accuracy: 0.62143 |  0:01:08s\n",
      "⏱️ Epoch 822 took 0.08 seconds\n",
      "epoch 822| loss: 0.86743 | val_0_accuracy: 0.58571 |  0:01:08s\n",
      "⏱️ Epoch 823 took 0.08 seconds\n",
      "epoch 823| loss: 0.8544  | val_0_accuracy: 0.59286 |  0:01:08s\n",
      "⏱️ Epoch 824 took 0.08 seconds\n",
      "epoch 824| loss: 1.00424 | val_0_accuracy: 0.55238 |  0:01:08s\n",
      "⏱️ Epoch 825 took 0.08 seconds\n",
      "epoch 825| loss: 0.91133 | val_0_accuracy: 0.61071 |  0:01:08s\n",
      "⏱️ Epoch 826 took 0.08 seconds\n",
      "epoch 826| loss: 0.90458 | val_0_accuracy: 0.62143 |  0:01:09s\n",
      "⏱️ Epoch 827 took 0.09 seconds\n",
      "epoch 827| loss: 0.90895 | val_0_accuracy: 0.62738 |  0:01:09s\n",
      "⏱️ Epoch 828 took 0.08 seconds\n",
      "epoch 828| loss: 0.89316 | val_0_accuracy: 0.64048 |  0:01:09s\n",
      "⏱️ Epoch 829 took 0.08 seconds\n",
      "epoch 829| loss: 0.94668 | val_0_accuracy: 0.61548 |  0:01:09s\n",
      "⏱️ Epoch 830 took 0.08 seconds\n",
      "epoch 830| loss: 1.0709  | val_0_accuracy: 0.61786 |  0:01:09s\n",
      "⏱️ Epoch 831 took 0.08 seconds\n",
      "epoch 831| loss: 0.84566 | val_0_accuracy: 0.63214 |  0:01:09s\n",
      "⏱️ Epoch 832 took 0.09 seconds\n",
      "epoch 832| loss: 0.84002 | val_0_accuracy: 0.6131  |  0:01:09s\n",
      "⏱️ Epoch 833 took 0.09 seconds\n",
      "epoch 833| loss: 0.92475 | val_0_accuracy: 0.6     |  0:01:09s\n",
      "⏱️ Epoch 834 took 0.08 seconds\n",
      "epoch 834| loss: 0.79771 | val_0_accuracy: 0.63571 |  0:01:09s\n",
      "⏱️ Epoch 835 took 0.08 seconds\n",
      "epoch 835| loss: 0.84094 | val_0_accuracy: 0.6     |  0:01:09s\n",
      "⏱️ Epoch 836 took 0.08 seconds\n",
      "epoch 836| loss: 0.85179 | val_0_accuracy: 0.6     |  0:01:09s\n",
      "⏱️ Epoch 837 took 0.09 seconds\n",
      "epoch 837| loss: 0.8468  | val_0_accuracy: 0.59048 |  0:01:09s\n",
      "⏱️ Epoch 838 took 0.10 seconds\n",
      "epoch 838| loss: 0.83312 | val_0_accuracy: 0.5631  |  0:01:10s\n",
      "⏱️ Epoch 839 took 0.09 seconds\n",
      "epoch 839| loss: 0.84785 | val_0_accuracy: 0.59286 |  0:01:10s\n",
      "⏱️ Epoch 840 took 0.09 seconds\n",
      "epoch 840| loss: 0.82785 | val_0_accuracy: 0.6131  |  0:01:10s\n",
      "⏱️ Epoch 841 took 0.09 seconds\n",
      "epoch 841| loss: 0.82245 | val_0_accuracy: 0.60238 |  0:01:10s\n",
      "⏱️ Epoch 842 took 0.08 seconds\n",
      "epoch 842| loss: 0.85468 | val_0_accuracy: 0.59762 |  0:01:10s\n",
      "⏱️ Epoch 843 took 0.08 seconds\n",
      "epoch 843| loss: 0.81441 | val_0_accuracy: 0.60952 |  0:01:10s\n",
      "⏱️ Epoch 844 took 0.08 seconds\n",
      "epoch 844| loss: 0.82909 | val_0_accuracy: 0.62024 |  0:01:10s\n",
      "⏱️ Epoch 845 took 0.08 seconds\n",
      "epoch 845| loss: 0.87721 | val_0_accuracy: 0.59524 |  0:01:10s\n",
      "⏱️ Epoch 846 took 0.08 seconds\n",
      "epoch 846| loss: 0.82443 | val_0_accuracy: 0.61667 |  0:01:10s\n",
      "⏱️ Epoch 847 took 0.09 seconds\n",
      "epoch 847| loss: 0.84095 | val_0_accuracy: 0.6     |  0:01:10s\n",
      "⏱️ Epoch 848 took 0.08 seconds\n",
      "epoch 848| loss: 0.8051  | val_0_accuracy: 0.61667 |  0:01:10s\n",
      "⏱️ Epoch 849 took 0.08 seconds\n",
      "epoch 849| loss: 0.89508 | val_0_accuracy: 0.55714 |  0:01:10s\n",
      "⏱️ Epoch 850 took 0.09 seconds\n",
      "epoch 850| loss: 0.90086 | val_0_accuracy: 0.5619  |  0:01:11s\n",
      "⏱️ Epoch 851 took 0.08 seconds\n",
      "epoch 851| loss: 0.88124 | val_0_accuracy: 0.5881  |  0:01:11s\n",
      "⏱️ Epoch 852 took 0.08 seconds\n",
      "epoch 852| loss: 0.81089 | val_0_accuracy: 0.625   |  0:01:11s\n",
      "⏱️ Epoch 853 took 0.08 seconds\n",
      "epoch 853| loss: 0.78264 | val_0_accuracy: 0.61786 |  0:01:11s\n",
      "⏱️ Epoch 854 took 0.08 seconds\n",
      "epoch 854| loss: 0.77787 | val_0_accuracy: 0.59286 |  0:01:11s\n",
      "⏱️ Epoch 855 took 0.08 seconds\n",
      "epoch 855| loss: 0.79756 | val_0_accuracy: 0.61786 |  0:01:11s\n",
      "⏱️ Epoch 856 took 0.09 seconds\n",
      "epoch 856| loss: 0.80325 | val_0_accuracy: 0.61905 |  0:01:11s\n",
      "⏱️ Epoch 857 took 0.09 seconds\n",
      "epoch 857| loss: 0.79682 | val_0_accuracy: 0.59286 |  0:01:11s\n",
      "⏱️ Epoch 858 took 0.09 seconds\n",
      "epoch 858| loss: 0.89371 | val_0_accuracy: 0.59286 |  0:01:11s\n",
      "⏱️ Epoch 859 took 0.08 seconds\n",
      "epoch 859| loss: 0.80914 | val_0_accuracy: 0.61667 |  0:01:11s\n",
      "⏱️ Epoch 860 took 0.08 seconds\n",
      "epoch 860| loss: 0.81846 | val_0_accuracy: 0.64405 |  0:01:11s\n",
      "⏱️ Epoch 861 took 0.08 seconds\n",
      "epoch 861| loss: 0.84218 | val_0_accuracy: 0.62143 |  0:01:11s\n",
      "⏱️ Epoch 862 took 0.09 seconds\n",
      "epoch 862| loss: 0.82104 | val_0_accuracy: 0.62024 |  0:01:12s\n",
      "⏱️ Epoch 863 took 0.08 seconds\n",
      "epoch 863| loss: 0.86647 | val_0_accuracy: 0.62857 |  0:01:12s\n",
      "⏱️ Epoch 864 took 0.08 seconds\n",
      "epoch 864| loss: 0.8773  | val_0_accuracy: 0.6     |  0:01:12s\n",
      "⏱️ Epoch 865 took 0.08 seconds\n",
      "epoch 865| loss: 0.82396 | val_0_accuracy: 0.62857 |  0:01:12s\n",
      "⏱️ Epoch 866 took 0.08 seconds\n",
      "epoch 866| loss: 0.75978 | val_0_accuracy: 0.65714 |  0:01:12s\n",
      "⏱️ Epoch 867 took 0.09 seconds\n",
      "epoch 867| loss: 0.7824  | val_0_accuracy: 0.62262 |  0:01:12s\n",
      "⏱️ Epoch 868 took 0.08 seconds\n",
      "epoch 868| loss: 0.79045 | val_0_accuracy: 0.60119 |  0:01:12s\n",
      "⏱️ Epoch 869 took 0.08 seconds\n",
      "epoch 869| loss: 0.82722 | val_0_accuracy: 0.62143 |  0:01:12s\n",
      "⏱️ Epoch 870 took 0.08 seconds\n",
      "epoch 870| loss: 0.77545 | val_0_accuracy: 0.62262 |  0:01:12s\n",
      "⏱️ Epoch 871 took 0.09 seconds\n",
      "epoch 871| loss: 0.76994 | val_0_accuracy: 0.60238 |  0:01:12s\n",
      "⏱️ Epoch 872 took 0.09 seconds\n",
      "epoch 872| loss: 0.75906 | val_0_accuracy: 0.6     |  0:01:12s\n",
      "⏱️ Epoch 873 took 0.08 seconds\n",
      "epoch 873| loss: 0.805   | val_0_accuracy: 0.60119 |  0:01:12s\n",
      "⏱️ Epoch 874 took 0.08 seconds\n",
      "epoch 874| loss: 0.80144 | val_0_accuracy: 0.58571 |  0:01:13s\n",
      "⏱️ Epoch 875 took 0.08 seconds\n",
      "epoch 875| loss: 0.83361 | val_0_accuracy: 0.58929 |  0:01:13s\n",
      "⏱️ Epoch 876 took 0.08 seconds\n",
      "epoch 876| loss: 0.81354 | val_0_accuracy: 0.57976 |  0:01:13s\n",
      "⏱️ Epoch 877 took 0.08 seconds\n",
      "epoch 877| loss: 0.82314 | val_0_accuracy: 0.57857 |  0:01:13s\n",
      "⏱️ Epoch 878 took 0.08 seconds\n",
      "epoch 878| loss: 0.75217 | val_0_accuracy: 0.62381 |  0:01:13s\n",
      "⏱️ Epoch 879 took 0.08 seconds\n",
      "epoch 879| loss: 0.80294 | val_0_accuracy: 0.6131  |  0:01:13s\n",
      "⏱️ Epoch 880 took 0.08 seconds\n",
      "epoch 880| loss: 0.85025 | val_0_accuracy: 0.62143 |  0:01:13s\n",
      "⏱️ Epoch 881 took 0.08 seconds\n",
      "epoch 881| loss: 0.80215 | val_0_accuracy: 0.59286 |  0:01:13s\n",
      "⏱️ Epoch 882 took 0.08 seconds\n",
      "epoch 882| loss: 0.77843 | val_0_accuracy: 0.62262 |  0:01:13s\n",
      "⏱️ Epoch 883 took 0.08 seconds\n",
      "epoch 883| loss: 0.76341 | val_0_accuracy: 0.5881  |  0:01:13s\n",
      "⏱️ Epoch 884 took 0.08 seconds\n",
      "epoch 884| loss: 0.80567 | val_0_accuracy: 0.60952 |  0:01:13s\n",
      "⏱️ Epoch 885 took 0.09 seconds\n",
      "epoch 885| loss: 0.82191 | val_0_accuracy: 0.61429 |  0:01:13s\n",
      "⏱️ Epoch 886 took 0.08 seconds\n",
      "epoch 886| loss: 0.90269 | val_0_accuracy: 0.58452 |  0:01:13s\n",
      "⏱️ Epoch 887 took 0.08 seconds\n",
      "epoch 887| loss: 0.81681 | val_0_accuracy: 0.60595 |  0:01:14s\n",
      "⏱️ Epoch 888 took 0.08 seconds\n",
      "epoch 888| loss: 0.85106 | val_0_accuracy: 0.62262 |  0:01:14s\n",
      "⏱️ Epoch 889 took 0.08 seconds\n",
      "epoch 889| loss: 0.98379 | val_0_accuracy: 0.61071 |  0:01:14s\n",
      "⏱️ Epoch 890 took 0.08 seconds\n",
      "epoch 890| loss: 0.86651 | val_0_accuracy: 0.62976 |  0:01:14s\n",
      "⏱️ Epoch 891 took 0.08 seconds\n",
      "epoch 891| loss: 0.83511 | val_0_accuracy: 0.60595 |  0:01:14s\n",
      "⏱️ Epoch 892 took 0.08 seconds\n",
      "epoch 892| loss: 0.7798  | val_0_accuracy: 0.625   |  0:01:14s\n",
      "⏱️ Epoch 893 took 0.08 seconds\n",
      "epoch 893| loss: 1.03517 | val_0_accuracy: 0.62143 |  0:01:14s\n",
      "⏱️ Epoch 894 took 0.08 seconds\n",
      "epoch 894| loss: 0.88222 | val_0_accuracy: 0.60952 |  0:01:14s\n",
      "⏱️ Epoch 895 took 0.09 seconds\n",
      "epoch 895| loss: 0.90383 | val_0_accuracy: 0.57381 |  0:01:14s\n",
      "⏱️ Epoch 896 took 0.09 seconds\n",
      "epoch 896| loss: 0.88141 | val_0_accuracy: 0.58333 |  0:01:14s\n",
      "⏱️ Epoch 897 took 0.09 seconds\n",
      "epoch 897| loss: 0.82246 | val_0_accuracy: 0.60357 |  0:01:14s\n",
      "⏱️ Epoch 898 took 0.08 seconds\n",
      "epoch 898| loss: 0.81896 | val_0_accuracy: 0.61071 |  0:01:14s\n",
      "⏱️ Epoch 899 took 0.08 seconds\n",
      "epoch 899| loss: 0.82572 | val_0_accuracy: 0.59881 |  0:01:15s\n",
      "⏱️ Epoch 900 took 0.08 seconds\n",
      "epoch 900| loss: 0.8572  | val_0_accuracy: 0.5869  |  0:01:15s\n",
      "⏱️ Epoch 901 took 0.08 seconds\n",
      "epoch 901| loss: 0.83722 | val_0_accuracy: 0.6     |  0:01:15s\n",
      "⏱️ Epoch 902 took 0.08 seconds\n",
      "epoch 902| loss: 0.86322 | val_0_accuracy: 0.61429 |  0:01:15s\n",
      "⏱️ Epoch 903 took 0.08 seconds\n",
      "epoch 903| loss: 0.7721  | val_0_accuracy: 0.59524 |  0:01:15s\n",
      "⏱️ Epoch 904 took 0.08 seconds\n",
      "epoch 904| loss: 0.80835 | val_0_accuracy: 0.62143 |  0:01:15s\n",
      "⏱️ Epoch 905 took 0.08 seconds\n",
      "epoch 905| loss: 0.78395 | val_0_accuracy: 0.6131  |  0:01:15s\n",
      "⏱️ Epoch 906 took 0.08 seconds\n",
      "epoch 906| loss: 0.77338 | val_0_accuracy: 0.59286 |  0:01:15s\n",
      "⏱️ Epoch 907 took 0.08 seconds\n",
      "epoch 907| loss: 0.80193 | val_0_accuracy: 0.59524 |  0:01:15s\n",
      "⏱️ Epoch 908 took 0.08 seconds\n",
      "epoch 908| loss: 0.89341 | val_0_accuracy: 0.60595 |  0:01:15s\n",
      "⏱️ Epoch 909 took 0.08 seconds\n",
      "epoch 909| loss: 0.88327 | val_0_accuracy: 0.58452 |  0:01:15s\n",
      "⏱️ Epoch 910 took 0.08 seconds\n",
      "epoch 910| loss: 0.81245 | val_0_accuracy: 0.58333 |  0:01:15s\n",
      "⏱️ Epoch 911 took 0.08 seconds\n",
      "epoch 911| loss: 0.84435 | val_0_accuracy: 0.60952 |  0:01:16s\n",
      "⏱️ Epoch 912 took 0.10 seconds\n",
      "epoch 912| loss: 0.83591 | val_0_accuracy: 0.62143 |  0:01:16s\n",
      "⏱️ Epoch 913 took 0.09 seconds\n",
      "epoch 913| loss: 0.80849 | val_0_accuracy: 0.60833 |  0:01:16s\n",
      "⏱️ Epoch 914 took 0.08 seconds\n",
      "epoch 914| loss: 0.76686 | val_0_accuracy: 0.57738 |  0:01:16s\n",
      "⏱️ Epoch 915 took 0.08 seconds\n",
      "epoch 915| loss: 0.83382 | val_0_accuracy: 0.62976 |  0:01:16s\n",
      "⏱️ Epoch 916 took 0.08 seconds\n",
      "epoch 916| loss: 0.80928 | val_0_accuracy: 0.62143 |  0:01:16s\n",
      "⏱️ Epoch 917 took 0.08 seconds\n",
      "epoch 917| loss: 0.82467 | val_0_accuracy: 0.5869  |  0:01:16s\n",
      "⏱️ Epoch 918 took 0.08 seconds\n",
      "epoch 918| loss: 0.85925 | val_0_accuracy: 0.54881 |  0:01:16s\n",
      "⏱️ Epoch 919 took 0.09 seconds\n",
      "epoch 919| loss: 0.8407  | val_0_accuracy: 0.61548 |  0:01:16s\n",
      "⏱️ Epoch 920 took 0.08 seconds\n",
      "epoch 920| loss: 0.76747 | val_0_accuracy: 0.62619 |  0:01:16s\n",
      "⏱️ Epoch 921 took 0.09 seconds\n",
      "epoch 921| loss: 0.75433 | val_0_accuracy: 0.62262 |  0:01:16s\n",
      "⏱️ Epoch 922 took 0.08 seconds\n",
      "epoch 922| loss: 0.74059 | val_0_accuracy: 0.61071 |  0:01:16s\n",
      "⏱️ Epoch 923 took 0.08 seconds\n",
      "epoch 923| loss: 0.74286 | val_0_accuracy: 0.57857 |  0:01:17s\n",
      "⏱️ Epoch 924 took 0.08 seconds\n",
      "epoch 924| loss: 0.75364 | val_0_accuracy: 0.63214 |  0:01:17s\n",
      "⏱️ Epoch 925 took 0.08 seconds\n",
      "epoch 925| loss: 0.84739 | val_0_accuracy: 0.61905 |  0:01:17s\n",
      "⏱️ Epoch 926 took 0.08 seconds\n",
      "epoch 926| loss: 0.8324  | val_0_accuracy: 0.58452 |  0:01:17s\n",
      "⏱️ Epoch 927 took 0.08 seconds\n",
      "epoch 927| loss: 0.77459 | val_0_accuracy: 0.61071 |  0:01:17s\n",
      "⏱️ Epoch 928 took 0.08 seconds\n",
      "epoch 928| loss: 0.85266 | val_0_accuracy: 0.60238 |  0:01:17s\n",
      "⏱️ Epoch 929 took 0.08 seconds\n",
      "epoch 929| loss: 0.78345 | val_0_accuracy: 0.61071 |  0:01:17s\n",
      "⏱️ Epoch 930 took 0.08 seconds\n",
      "epoch 930| loss: 0.85026 | val_0_accuracy: 0.63333 |  0:01:17s\n",
      "⏱️ Epoch 931 took 0.08 seconds\n",
      "epoch 931| loss: 0.8808  | val_0_accuracy: 0.60119 |  0:01:17s\n",
      "⏱️ Epoch 932 took 0.09 seconds\n",
      "epoch 932| loss: 0.91935 | val_0_accuracy: 0.59881 |  0:01:17s\n",
      "⏱️ Epoch 933 took 0.08 seconds\n",
      "epoch 933| loss: 0.76946 | val_0_accuracy: 0.6119  |  0:01:17s\n",
      "⏱️ Epoch 934 took 0.08 seconds\n",
      "epoch 934| loss: 0.80558 | val_0_accuracy: 0.57381 |  0:01:17s\n",
      "⏱️ Epoch 935 took 0.08 seconds\n",
      "epoch 935| loss: 0.82663 | val_0_accuracy: 0.59286 |  0:01:18s\n",
      "⏱️ Epoch 936 took 0.08 seconds\n",
      "epoch 936| loss: 0.78251 | val_0_accuracy: 0.61429 |  0:01:18s\n",
      "⏱️ Epoch 937 took 0.08 seconds\n",
      "epoch 937| loss: 0.90202 | val_0_accuracy: 0.60595 |  0:01:18s\n",
      "⏱️ Epoch 938 took 0.08 seconds\n",
      "epoch 938| loss: 0.84464 | val_0_accuracy: 0.6119  |  0:01:18s\n",
      "⏱️ Epoch 939 took 0.08 seconds\n",
      "epoch 939| loss: 0.78779 | val_0_accuracy: 0.58214 |  0:01:18s\n",
      "⏱️ Epoch 940 took 0.08 seconds\n",
      "epoch 940| loss: 0.79381 | val_0_accuracy: 0.60238 |  0:01:18s\n",
      "⏱️ Epoch 941 took 0.08 seconds\n",
      "epoch 941| loss: 0.74296 | val_0_accuracy: 0.59881 |  0:01:18s\n",
      "⏱️ Epoch 942 took 0.08 seconds\n",
      "epoch 942| loss: 0.75772 | val_0_accuracy: 0.6131  |  0:01:18s\n",
      "⏱️ Epoch 943 took 0.08 seconds\n",
      "epoch 943| loss: 0.73173 | val_0_accuracy: 0.57857 |  0:01:18s\n",
      "⏱️ Epoch 944 took 0.08 seconds\n",
      "epoch 944| loss: 0.74535 | val_0_accuracy: 0.63214 |  0:01:18s\n",
      "⏱️ Epoch 945 took 0.08 seconds\n",
      "epoch 945| loss: 0.81749 | val_0_accuracy: 0.63214 |  0:01:18s\n",
      "⏱️ Epoch 946 took 0.08 seconds\n",
      "epoch 946| loss: 0.75689 | val_0_accuracy: 0.61429 |  0:01:18s\n",
      "⏱️ Epoch 947 took 0.08 seconds\n",
      "epoch 947| loss: 0.77843 | val_0_accuracy: 0.56667 |  0:01:19s\n",
      "⏱️ Epoch 948 took 0.08 seconds\n",
      "epoch 948| loss: 0.79138 | val_0_accuracy: 0.61071 |  0:01:19s\n",
      "⏱️ Epoch 949 took 0.08 seconds\n",
      "epoch 949| loss: 0.85335 | val_0_accuracy: 0.60238 |  0:01:19s\n",
      "⏱️ Epoch 950 took 0.08 seconds\n",
      "epoch 950| loss: 0.88149 | val_0_accuracy: 0.5881  |  0:01:19s\n",
      "⏱️ Epoch 951 took 0.08 seconds\n",
      "epoch 951| loss: 0.78988 | val_0_accuracy: 0.60357 |  0:01:19s\n",
      "⏱️ Epoch 952 took 0.08 seconds\n",
      "epoch 952| loss: 0.83738 | val_0_accuracy: 0.59881 |  0:01:19s\n",
      "⏱️ Epoch 953 took 0.08 seconds\n",
      "epoch 953| loss: 0.80678 | val_0_accuracy: 0.60119 |  0:01:19s\n",
      "⏱️ Epoch 954 took 0.09 seconds\n",
      "epoch 954| loss: 0.77352 | val_0_accuracy: 0.61786 |  0:01:19s\n",
      "⏱️ Epoch 955 took 0.09 seconds\n",
      "epoch 955| loss: 0.76185 | val_0_accuracy: 0.62738 |  0:01:19s\n",
      "⏱️ Epoch 956 took 0.08 seconds\n",
      "epoch 956| loss: 0.78631 | val_0_accuracy: 0.59762 |  0:01:19s\n",
      "⏱️ Epoch 957 took 0.08 seconds\n",
      "epoch 957| loss: 0.77575 | val_0_accuracy: 0.59405 |  0:01:19s\n",
      "⏱️ Epoch 958 took 0.08 seconds\n",
      "epoch 958| loss: 0.86301 | val_0_accuracy: 0.59405 |  0:01:19s\n",
      "⏱️ Epoch 959 took 0.08 seconds\n",
      "epoch 959| loss: 0.79166 | val_0_accuracy: 0.60357 |  0:01:20s\n",
      "⏱️ Epoch 960 took 0.08 seconds\n",
      "epoch 960| loss: 0.77038 | val_0_accuracy: 0.59881 |  0:01:20s\n",
      "⏱️ Epoch 961 took 0.08 seconds\n",
      "epoch 961| loss: 0.84005 | val_0_accuracy: 0.59643 |  0:01:20s\n",
      "⏱️ Epoch 962 took 0.08 seconds\n",
      "epoch 962| loss: 0.83314 | val_0_accuracy: 0.62024 |  0:01:20s\n",
      "⏱️ Epoch 963 took 0.08 seconds\n",
      "epoch 963| loss: 0.90938 | val_0_accuracy: 0.60714 |  0:01:20s\n",
      "⏱️ Epoch 964 took 0.08 seconds\n",
      "epoch 964| loss: 0.77785 | val_0_accuracy: 0.60357 |  0:01:20s\n",
      "⏱️ Epoch 965 took 0.09 seconds\n",
      "epoch 965| loss: 0.85865 | val_0_accuracy: 0.6     |  0:01:20s\n",
      "⏱️ Epoch 966 took 0.08 seconds\n",
      "epoch 966| loss: 0.82725 | val_0_accuracy: 0.60238 |  0:01:20s\n",
      "⏱️ Epoch 967 took 0.08 seconds\n",
      "epoch 967| loss: 0.80028 | val_0_accuracy: 0.62619 |  0:01:20s\n",
      "⏱️ Epoch 968 took 0.08 seconds\n",
      "epoch 968| loss: 0.78328 | val_0_accuracy: 0.62619 |  0:01:20s\n",
      "⏱️ Epoch 969 took 0.09 seconds\n",
      "epoch 969| loss: 0.78454 | val_0_accuracy: 0.6131  |  0:01:20s\n",
      "⏱️ Epoch 970 took 0.09 seconds\n",
      "epoch 970| loss: 0.82232 | val_0_accuracy: 0.60119 |  0:01:20s\n",
      "⏱️ Epoch 971 took 0.08 seconds\n",
      "epoch 971| loss: 0.75798 | val_0_accuracy: 0.59286 |  0:01:20s\n",
      "⏱️ Epoch 972 took 0.08 seconds\n",
      "epoch 972| loss: 0.7611  | val_0_accuracy: 0.59643 |  0:01:21s\n",
      "⏱️ Epoch 973 took 0.08 seconds\n",
      "epoch 973| loss: 0.78789 | val_0_accuracy: 0.58095 |  0:01:21s\n",
      "⏱️ Epoch 974 took 0.10 seconds\n",
      "epoch 974| loss: 0.77639 | val_0_accuracy: 0.57024 |  0:01:21s\n",
      "⏱️ Epoch 975 took 0.09 seconds\n",
      "epoch 975| loss: 0.84345 | val_0_accuracy: 0.5631  |  0:01:21s\n",
      "⏱️ Epoch 976 took 0.10 seconds\n",
      "epoch 976| loss: 0.87288 | val_0_accuracy: 0.60476 |  0:01:21s\n",
      "⏱️ Epoch 977 took 0.10 seconds\n",
      "epoch 977| loss: 0.78714 | val_0_accuracy: 0.61786 |  0:01:21s\n",
      "⏱️ Epoch 978 took 0.12 seconds\n",
      "epoch 978| loss: 0.7463  | val_0_accuracy: 0.62024 |  0:01:21s\n",
      "⏱️ Epoch 979 took 0.10 seconds\n",
      "epoch 979| loss: 0.77176 | val_0_accuracy: 0.60952 |  0:01:21s\n",
      "⏱️ Epoch 980 took 0.10 seconds\n",
      "epoch 980| loss: 0.82265 | val_0_accuracy: 0.61667 |  0:01:21s\n",
      "⏱️ Epoch 981 took 0.08 seconds\n",
      "epoch 981| loss: 0.78581 | val_0_accuracy: 0.60714 |  0:01:21s\n",
      "⏱️ Epoch 982 took 0.09 seconds\n",
      "epoch 982| loss: 0.76447 | val_0_accuracy: 0.5881  |  0:01:22s\n",
      "⏱️ Epoch 983 took 0.10 seconds\n",
      "epoch 983| loss: 0.74302 | val_0_accuracy: 0.58929 |  0:01:22s\n",
      "⏱️ Epoch 984 took 0.10 seconds\n",
      "epoch 984| loss: 0.78287 | val_0_accuracy: 0.59048 |  0:01:22s\n",
      "⏱️ Epoch 985 took 0.10 seconds\n",
      "epoch 985| loss: 0.81928 | val_0_accuracy: 0.58452 |  0:01:22s\n",
      "⏱️ Epoch 986 took 0.09 seconds\n",
      "epoch 986| loss: 0.84987 | val_0_accuracy: 0.59643 |  0:01:22s\n",
      "⏱️ Epoch 987 took 0.10 seconds\n",
      "epoch 987| loss: 0.74536 | val_0_accuracy: 0.59762 |  0:01:22s\n",
      "⏱️ Epoch 988 took 0.08 seconds\n",
      "epoch 988| loss: 0.70779 | val_0_accuracy: 0.62143 |  0:01:22s\n",
      "⏱️ Epoch 989 took 0.08 seconds\n",
      "epoch 989| loss: 0.72785 | val_0_accuracy: 0.60238 |  0:01:22s\n",
      "⏱️ Epoch 990 took 0.08 seconds\n",
      "epoch 990| loss: 0.74106 | val_0_accuracy: 0.57381 |  0:01:22s\n",
      "⏱️ Epoch 991 took 0.08 seconds\n",
      "epoch 991| loss: 0.77193 | val_0_accuracy: 0.60119 |  0:01:22s\n",
      "⏱️ Epoch 992 took 0.08 seconds\n",
      "epoch 992| loss: 0.76923 | val_0_accuracy: 0.61786 |  0:01:22s\n",
      "⏱️ Epoch 993 took 0.08 seconds\n",
      "epoch 993| loss: 0.79305 | val_0_accuracy: 0.6119  |  0:01:23s\n",
      "⏱️ Epoch 994 took 0.21 seconds\n",
      "epoch 994| loss: 0.83392 | val_0_accuracy: 0.6131  |  0:01:23s\n",
      "⏱️ Epoch 995 took 0.08 seconds\n",
      "epoch 995| loss: 0.77077 | val_0_accuracy: 0.59881 |  0:01:23s\n",
      "⏱️ Epoch 996 took 0.08 seconds\n",
      "epoch 996| loss: 0.80998 | val_0_accuracy: 0.60357 |  0:01:23s\n",
      "⏱️ Epoch 997 took 0.08 seconds\n",
      "epoch 997| loss: 0.8348  | val_0_accuracy: 0.59048 |  0:01:23s\n",
      "⏱️ Epoch 998 took 0.08 seconds\n",
      "epoch 998| loss: 0.80755 | val_0_accuracy: 0.58214 |  0:01:23s\n",
      "⏱️ Epoch 999 took 0.08 seconds\n",
      "epoch 999| loss: 0.76279 | val_0_accuracy: 0.55833 |  0:01:23s\n",
      "⏱️ Epoch 1000 took 0.08 seconds\n",
      "Stop training because you reached max_epochs = 1000 with best_epoch = 866 and best_val_0_accuracy = 0.65714\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Ytrain, Xtest, Ytest, Ytest_decoded, le = split_data(traindata, testdata)\n",
    "best_tabnet_params = find_best_tabnet(Xtrain, Ytrain)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split 80% train, 20% validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    Xtrain, Ytrain,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=Ytrain\n",
    ")\n",
    "\n",
    "# Re-initialize TabNet with best params\n",
    "final_model = TabNetClassifier(\n",
    "    **{k: v for k, v in best_tabnet_params.items()},\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "epoch_timer = EpochTimer()\n",
    "\n",
    "# Retrain on full training data\n",
    "final_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    max_epochs=1000,\n",
    "    patience=200,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    callbacks=[epoch_timer]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, _, le = split_data(traindata, testdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "file_path = os.path.join(\"results\", f\"gtd{partition}.txt\")\n",
    "\n",
    "# Predict class indices for test set\n",
    "y_pred = final_model.predict(Xtest)\n",
    "y_proba = final_model.predict_proba(Xtest)\n",
    "y_pred_decoded = le.inverse_transform(y_pred)\n",
    "y_true_decoded = le.inverse_transform(Ytest)\n",
    "\n",
    "# Make sure the directory exists\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Compute accuracy from decoded labels\n",
    "acc = accuracy_score(y_true_decoded, y_pred_decoded)\n",
    "\n",
    "# Write metrics to file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "    file.write(f\"Precision weighted: {precision_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"Recall weighted: {recall_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score weighted: {f1_score(y_true_decoded, y_pred_decoded, average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"Precision micro: {precision_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"Recall micro: {recall_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score micro: {f1_score(y_true_decoded, y_pred_decoded, average='micro'):.4f}\\n\")\n",
    "    file.write(f\"Precision macro: {precision_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"Recall macro: {recall_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"F1 Score macro: {f1_score(y_true_decoded, y_pred_decoded, average='macro'):.4f}\\n\")\n",
    "    file.write(f\"roc auc weighted: {roc_auc_score(y_true_decoded, y_proba, multi_class='ovr', average='weighted'):.4f}\\n\")\n",
    "    file.write(f\"roc auc macro: {roc_auc_score(y_true_decoded, y_proba, multi_class='ovr', average='macro'):.4f}\\n\")\n",
    "    file.write(f\"roc auc micro: {roc_auc_score(y_true_decoded, y_proba, multi_class='ovr', average='micro'):.4f}\\n\")\n",
    "\n",
    "with open(f\"results/epoch_time_gtd{partition}.txt\", \"w\") as f:\n",
    "    f.write('\\n'.join(str(x) for x in epoch_timer.epoch_times))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "                          Abu Sayyaf Group (ASG)       0.32      0.35      0.34        60\n",
      "        African National Congress (South Africa)       0.63      0.78      0.70        60\n",
      "                                Al-Qaida in Iraq       0.42      0.47      0.44        60\n",
      "        Al-Qaida in the Arabian Peninsula (AQAP)       0.38      0.38      0.38        60\n",
      "                                      Al-Shabaab       0.57      0.50      0.53        60\n",
      "             Basque Fatherland and Freedom (ETA)       0.60      0.63      0.62        60\n",
      "                                      Boko Haram       0.38      0.33      0.36        60\n",
      "  Communist Party of India - Maoist (CPI-Maoist)       0.71      0.60      0.65        60\n",
      "       Corsican National Liberation Front (FLNC)       0.59      0.75      0.66        60\n",
      "                       Donetsk People's Republic       0.47      0.67      0.55        60\n",
      "Farabundo Marti National Liberation Front (FMLN)       0.48      0.70      0.57        60\n",
      "                               Fulani extremists       0.57      0.78      0.66        60\n",
      "                 Houthi extremists (Ansar Allah)       0.56      0.45      0.50        60\n",
      "                     Irish Republican Army (IRA)       0.80      0.65      0.72        60\n",
      "     Islamic State of Iraq and the Levant (ISIL)       0.36      0.28      0.32        60\n",
      "                  Kurdistan Workers' Party (PKK)       0.30      0.18      0.23        60\n",
      "         Liberation Tigers of Tamil Eelam (LTTE)       0.56      0.57      0.56        60\n",
      "         Manuel Rodriguez Patriotic Front (FPMR)       0.70      0.77      0.73        60\n",
      "                                         Maoists       0.42      0.52      0.46        60\n",
      "                               Muslim extremists       0.36      0.33      0.35        60\n",
      "      National Liberation Army of Colombia (ELN)       0.62      0.43      0.51        60\n",
      "                         New People's Army (NPA)       0.37      0.37      0.37        60\n",
      "               Nicaraguan Democratic Force (FDN)       0.57      0.73      0.64        60\n",
      "                                    Palestinians       0.88      0.62      0.73        60\n",
      "   Revolutionary Armed Forces of Colombia (FARC)       0.50      0.43      0.46        60\n",
      "                               Shining Path (SL)       0.21      0.08      0.12        60\n",
      "                                 Sikh Extremists       0.39      0.50      0.44        60\n",
      "                                         Taliban       0.21      0.17      0.19        60\n",
      "                 Tehrik-i-Taliban Pakistan (TTP)       0.39      0.33      0.36        60\n",
      "       Tupac Amaru Revolutionary Movement (MRTA)       0.55      0.70      0.61        60\n",
      "\n",
      "                                        accuracy                           0.50      1800\n",
      "                                       macro avg       0.50      0.50      0.49      1800\n",
      "                                    weighted avg       0.50      0.50      0.49      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Ytest_decoded, y_pred_decoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    plt.figure(figsize=(18, 16))\n",
    "    sns.heatmap(cm_normalized,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels,\n",
    "                cmap=\"viridis\",\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "    plt.title(f\"Normalized Confusion Matrix\", fontsize=18)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "    plt.ylabel(\"True Label\", fontsize=14)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = f\"results/confusion_matrix_partition_{partition}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved confusion matrix for partition {partition} to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved confusion matrix for partition 200 to results/confusion_matrix_partition_200.png\n"
     ]
    }
   ],
   "source": [
    "# Get all unique class labels from the truths\n",
    "class_labels = np.unique(Ytest_decoded)\n",
    "\n",
    "plot_confusion_matrix(Ytest_decoded, y_pred_decoded, labels=class_labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TabNet)",
   "language": "python",
   "name": "tabnet-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
