{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from SeqFM import SeqFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = f'../../data/top30groups/LongLatCombined/train1/train{partition}.csv'\n",
    "testpath = f'../../data/top30groups/LongLatCombined/test1/test{partition}.csv'\n",
    "traindata = pd.read_csv(trainpath, encoding='ISO-8859-1')\n",
    "testdata = pd.read_csv(testpath, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_for_seqfm(train, test, device=\"cuda\"):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    # 1. Combine to ensure consistent encoding of location_id across splits\n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    full = pd.concat([train, test], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # 2. Create longlat feature and encode it\n",
    "    full['longlat'] = list(zip(full['longitude'], full['latitude']))\n",
    "    full['location_id'], _ = pd.factorize(full['longlat'])\n",
    "    full = full.drop(columns=['longitude', 'latitude', 'longlat'])\n",
    "\n",
    "    # 3. Split back\n",
    "    train = full.iloc[:len(train)].reset_index(drop=True)\n",
    "    test = full.iloc[len(train):].reset_index(drop=True)\n",
    "\n",
    "    # 4. Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(train['gname']).astype(np.int64)\n",
    "    y_test = le.transform(test['gname']).astype(np.int64)\n",
    "\n",
    "    # 5. Drop label column to form features\n",
    "    X_train = train.drop(columns=['gname']).to_numpy(dtype=np.float32)\n",
    "    X_test = test.drop(columns=['gname']).to_numpy(dtype=np.float32)\n",
    "\n",
    "    # 6. Format for SeqFM (N, T=1, F)\n",
    "    X_train_seq = torch.tensor(X_train[:, np.newaxis, :], device=device)\n",
    "    X_test_seq = torch.tensor(X_test[:, np.newaxis, :], device=device)\n",
    "    y_train = torch.tensor(y_train, device=device)\n",
    "    y_test = torch.tensor(y_test, device=device)\n",
    "\n",
    "    # 7. Lengths (since T=1 for all sequences)\n",
    "    lengths_train = torch.ones(X_train_seq.shape[0], dtype=torch.long, device=device)\n",
    "    lengths_test = torch.ones(X_test_seq.shape[0], dtype=torch.long, device=device)\n",
    "\n",
    "    return X_train_seq, X_test_seq, y_train, y_test, lengths_train, lengths_test, le, train, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "static_U_X: torch.Size([2100])\n",
      "static_E_X: torch.Size([2100, 12])\n",
      "dynamic_X: torch.Size([2100, 1, 1])\n",
      "num_static_u: 1790\n",
      "num_static_e: 1855\n",
      "num_dynamic: 9\n",
      "Max static_U_X: 1303\n",
      "Max static_E_X: 1835\n",
      "Max dynamic_X: 8\n",
      "Min static_U_X: 0\n",
      "Min static_E_X: 0\n",
      "Min dynamic_X: 1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):\n\u001b[1;32m     90\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 91\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstatic_U_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_E_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_X_dummy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, y_train)\n\u001b[1;32m     94\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/MEX0512/GTD_2025/Codes/SeqFM/SeqFM.py:70\u001b[0m, in \u001b[0;36mSeqFM.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     67\u001b[0m static_U \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_U(static_U_X)  \u001b[38;5;66;03m# (N, emb_dim)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m static_U \u001b[38;5;241m=\u001b[39m static_U\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)      \u001b[38;5;66;03m# (N, 1, emb_dim)\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m static_E \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_E\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatic_E_X\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (N, S, emb_dim)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature:\n\u001b[1;32m     73\u001b[0m     feature_E \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_E(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Device\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 1. Load data\n",
    "X_train, X_test, y_train, y_test, lengths_train, lengths_test, le, traindata_processed, testdata_processed = splitting_for_seqfm(traindata, testdata, device)\n",
    "\n",
    "# 2. Choose features\n",
    "dynamic_feature = 'attacktype1'     # or 'weaptype1'\n",
    "static_u_feature = 'location_id'    # already integer-encoded\n",
    "\n",
    "# 3. Extract features from training & test\n",
    "def get_feature_tensors(df, dynamic_feature, static_u_feature):\n",
    "    static_U = torch.tensor(df[static_u_feature].values, device=device).long()\n",
    "    static_E = torch.tensor(df.drop(columns=['gname', dynamic_feature, static_u_feature]).values, device=device).long()\n",
    "    dynamic = torch.tensor(df[dynamic_feature].values[:, None, None], device=device).long()\n",
    "    return static_U, static_E, dynamic\n",
    "\n",
    "static_U_X, static_E_X, dynamic_X = get_feature_tensors(traindata_processed, dynamic_feature, static_u_feature)\n",
    "static_U_X_test, static_E_X_test, dynamic_X_test = get_feature_tensors(testdata_processed, dynamic_feature, static_u_feature)\n",
    "\n",
    "# 4. Compute vocab sizes from raw data (important!)\n",
    "num_static_u = max(static_U_X.max().item(), static_U_X_test.max().item()) + 1\n",
    "num_static_e = max(static_E_X.max().item(), static_E_X_test.max().item()) + 1\n",
    "num_dynamic  = max(dynamic_X.max().item(), dynamic_X_test.max().item()) + 1\n",
    "\n",
    "# Final sanity clamp after vocab computation\n",
    "static_U_X = static_U_X.clamp(0, num_static_u - 1)\n",
    "static_E_X = static_E_X.clamp(0, num_static_e - 1)\n",
    "dynamic_X  = dynamic_X.clamp(0, num_dynamic - 1)\n",
    "\n",
    "static_U_X_test = static_U_X_test.clamp(0, num_static_u - 1)\n",
    "static_E_X_test = static_E_X_test.clamp(0, num_static_e - 1)\n",
    "dynamic_X_test  = dynamic_X_test.clamp(0, num_dynamic - 1)\n",
    "\n",
    "embedding_dim = 32\n",
    "\n",
    "# 5. Clamp indices to avoid out-of-range errors\n",
    "static_U_X = torch.clamp(static_U_X, 0, num_static_u - 1)\n",
    "static_E_X = torch.clamp(static_E_X, 0, num_static_e - 1)\n",
    "dynamic_X  = torch.clamp(dynamic_X,  0, num_dynamic - 1)\n",
    "\n",
    "static_U_X_test = torch.clamp(static_U_X_test, 0, num_static_u - 1)\n",
    "static_E_X_test = torch.clamp(static_E_X_test, 0, num_static_e - 1)\n",
    "dynamic_X_test  = torch.clamp(dynamic_X_test,  0, num_dynamic - 1)\n",
    "\n",
    "assert static_U_X.max().item() < num_static_u, \"static_U_X has out-of-bound index!\"\n",
    "assert static_E_X.max().item() < num_static_e, \"static_E_X has out-of-bound index!\"\n",
    "assert dynamic_X.max().item() < num_dynamic, \"dynamic_X has out-of-bound index!\"\n",
    "print(\"static_U_X:\", static_U_X.shape)\n",
    "print(\"static_E_X:\", static_E_X.shape)\n",
    "print(\"dynamic_X:\", dynamic_X.shape)\n",
    "\n",
    "feature_X_dummy = torch.empty((static_U_X.shape[0], 0), device=device).long()\n",
    "\n",
    "\n",
    "\n",
    "# 6. Initialize model\n",
    "model = SeqFM(\n",
    "    static_u_m=num_static_u,\n",
    "    feature_m=num_static_e,  \n",
    "    dynamic_m=num_dynamic,\n",
    "    emb_dim=embedding_dim,\n",
    "    dropout=0.2,\n",
    "    n_layer=2,\n",
    "    use_cuda=(device == \"cuda\"),\n",
    "    unshared=False,\n",
    "    pos_emb_dim=0\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 7. Print debug info\n",
    "print(\"num_static_u:\", num_static_u)\n",
    "print(\"num_static_e:\", num_static_e)\n",
    "print(\"num_dynamic:\", num_dynamic)\n",
    "print(\"Max static_U_X:\", static_U_X.max().item())\n",
    "print(\"Max static_E_X:\", static_E_X.max().item())\n",
    "print(\"Max dynamic_X:\", dynamic_X.max().item())\n",
    "print(\"Min static_U_X:\", static_U_X.min().item())\n",
    "print(\"Min static_E_X:\", static_E_X.min().item())\n",
    "print(\"Min dynamic_X:\", dynamic_X.min().item())\n",
    "\n",
    "# 8. Training loop\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    preds = model([static_U_X, static_E_X, feature_X_dummy, dynamic_X, lengths_train])\n",
    "    loss = loss_fn(preds, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | Train Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
